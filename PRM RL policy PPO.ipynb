{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1\"\n",
    "MODEL_PATH = f\"../Model/PRM_LORA{version}_merged_code_policy_01\"\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT_weight = 1\n",
    "clip_ratio = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# separate out question and solution and only train on solution\n",
    "patterns = [r\"Write your final answer as a single integer in the last line of your response, enclosed within \\\\boxed{}\",\\\n",
    "            r\"print the final result.\\nApproach:\",\\\n",
    "            r\"print the final output, as an integer not other python object such as list or tuple.\"]\n",
    "import re\n",
    "clean_text = lambda x:re.sub(r\"(<math>|<\\/math>|<cmath>|<\\/cmath>|\\\\begin\\{align\\*\\}|\\\\end\\{align\\*\\})\", \"\", x)\n",
    "def search_patterns(text, patterns):\n",
    "    for pattern in patterns:\n",
    "        # Compile the pattern\n",
    "        regex = re.compile(pattern)\n",
    "        # Find all matches of the pattern in the text\n",
    "        matches = list(regex.finditer(text))\n",
    "        # If there is one match, get the end position\n",
    "        if matches:\n",
    "            return matches[0].end()\n",
    "    raise Exception(\"no match\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0,1}\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "data = []\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([clean_text(text),y])\n",
    "texts,ys = zip(*data)\n",
    "\n",
    "ys = np.array(ys)\n",
    "ys = (ys-ys.mean())/ys.std()\n",
    "# ys[ys<0] *= neg_weight\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for text in texts:\n",
    "    idx = search_patterns(text,patterns)\n",
    "    if idx > 1100: continue\n",
    "    question = tokenizer.encode(text[21:idx]+\"\\n\\nAssistant:\",add_special_tokens=True)\n",
    "    answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi\n",
    "with open(f\"../llmOutputs/PRM/data_pi1_code{version}.pickle\", \"rb\") as f:\n",
    "    data_pi = pickle.load(f)\n",
    "texts2,ys2,lengths_raw = zip(*data_pi)\n",
    "ys2 = np.array(ys2)\n",
    "ys2 = ys2/ys2.std() #* len(texts) / len(texts2)\n",
    "\n",
    "# combined\n",
    "ys = ys.tolist() + ys2.tolist()\n",
    "for text,idx in zip(texts2,lengths_raw):\n",
    "    if idx > 1100: continue\n",
    "    question = tokenizer.encode(clean_text(text[21:idx]),add_special_tokens=True)\n",
    "    answer = tokenizer.encode(clean_text(text[idx:]),add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle outside from_gen, as we only have one epoch\n",
    "data = list(zip(input_ids,ys,lengths))\n",
    "random.shuffle(data)\n",
    "input_ids,ys,lengths = list(zip(*data))\n",
    "\n",
    "def from_gen(*data):\n",
    "    for da in zip(*data,strict=True):\n",
    "        if len(da) == 4:\n",
    "            text = torch.tensor(da[0][:MAX_LEN],device='cuda')[None]\n",
    "            logp_old = torch.tensor(da[1],device='cuda')\n",
    "            yield text,logp_old,*da[2:]\n",
    "        else:\n",
    "            text = torch.tensor(da[0][:MAX_LEN],device='cuda')[None]\n",
    "            yield text,*da[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SFT - Math\n",
    "# def gen_prompt_codeIn1(problem):\n",
    "#     return f\"\"\"User: {problem}\\n\n",
    "# Determine a sympy-based approach for solving the problem. When defining symbol, incorporate all constraints mentioned in the problem statement, e.g. real, integer, even, odd, positive, prime. If a variable represents a positive integer, Symbol('n', integer=True, positive=True). Your final answer should be integer, not expression, list, tuple or dictionary!\n",
    "# Write the entire script covering all the steps (use comments and document it well) and print the final result.\\n\\nAssistant:\n",
    "# \"\"\"\n",
    "\n",
    "# def gen_prompt_codeIn2(problem):\n",
    "#     return f\"\"\"User: {problem}\\n\n",
    "# You are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\\n\\nAssistant:\"\"\"\n",
    "\n",
    "# def gen_prompt3(problem):\n",
    "#     return \"User: \"+problem+'''\\n\n",
    "# Carefully read and understand the problem and use all information in problem statement. No Python code. Show your work step-by-step, explain your reasoning, calculations, mathematical concepts and formulas in detail.\n",
    "# Write your final answer as a single integer in the last line of your response, enclosed within \\\\boxed{}.\\n\\nAssistant:\n",
    "# '''\n",
    "\n",
    "# def add_prompt(problem):\n",
    "#     if np.random.rand()<0.5:\n",
    "#         return gen_prompt_codeIn1(problem)\n",
    "#     else:\n",
    "#         return gen_prompt_codeIn2(problem)\n",
    "    \n",
    "# sft = pd.read_csv(\"../Data/MATH/math.csv\")\n",
    "# # sft = sft.loc[sft.boxed_number == sft.parsed]\n",
    "# sft = sft.loc[(sft.boxed_number == sft.parsed) & (sft.level == 'Level 5')]\n",
    "# sft['code_wPrompt'] = sft.problem.apply(add_prompt)\n",
    "# for q,a in zip(sft.code_wPrompt.tolist(),sft.code_solution.tolist()):\n",
    "#     question = tokenizer.encode(clean_text(q),add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)\n",
    "# ys = ys + [SFT_weight] * sft.shape[0]\n",
    "\n",
    "# sft['pure_wPrompt'] = sft.problem.apply(gen_prompt3)\n",
    "# for q,a in zip(sft.pure_wPrompt.tolist(),sft.solution.tolist()):\n",
    "#     question = tokenizer.encode(clean_text(q),add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)\n",
    "# ys = ys + [SFT_weight] * sft.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SFT - AIME (prompt included). [9:] remove \"Problem:\"\n",
    "# with open(f\"../Data/ai-mathematical-olympiad-prize/10prob.pickle\", \"rb\") as f:\n",
    "#     outs = pickle.load(f)\n",
    "# with open(f\"../Data/AMC/aime_final.pickle\", \"rb\") as f:\n",
    "#     outs2 = pickle.load(f)\n",
    "# for q,a in outs:\n",
    "#     question = tokenizer.encode(\"User: \"+clean_text(q[9:])+\"\\n\\nAssistant:\",add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)\n",
    "# ys = ys + [SFT_weight] * len(outs)\n",
    "\n",
    "# for q,a in outs2:\n",
    "#     question = tokenizer.encode(\"User: \"+clean_text(q[9:])+\"\\n\\nAssistant:\",add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)\n",
    "# ys = ys + [SFT_weight] * len(outs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb453fd3c3354eff92ad49a1bd538d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 2e-5\n",
    "clip = 2e-3\n",
    "from transformers import AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            trust_remote_code=True,\n",
    "                                            attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "model.gradient_checkpointing_enable()\n",
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ],\n",
    "                        #  use_dora=True,\n",
    "                        )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute logP_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def logP_from_logits(logits, text):\n",
    "    \"\"\"\n",
    "    Extracts log probabilities of the selected classes from logits.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits of shape (l, C).\n",
    "        text (torch.Tensor): Text of shape (l,), where each element is a class index.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of shape (l,).\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # Normalize to log probabilities\n",
    "    log_probs_of_text = log_probs.gather(1, text.unsqueeze(1)).squeeze(1) # Gather log probabilities using fancy indexing\n",
    "    return log_probs_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug\n",
    "logP_list = []\n",
    "for text,y,l in from_gen(input_ids,ys,lengths):\n",
    "    with torch.no_grad():\n",
    "        logits = model(text).logits[0,l:-1]\n",
    "        logP = logP_from_logits(logits, text[0,l+1:]).cpu().numpy()\n",
    "    assert (text.shape[1] - l - 1) == logP.shape[0]\n",
    "    logP_list.append(logP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logp,logp_old,adv,clip_ratio):\n",
    "    ratio = torch.exp(logp - logp_old)\n",
    "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "    # approx_kl = (logp_old - logp).mean().item()\n",
    "    return loss_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: -0.017889673230229164\n",
      "iter: 2047, \n",
      " train loss: -0.05055426644764793\n",
      "iter: 3071, \n",
      " train loss: 0.0020132079342611853\n",
      "iter: 4095, \n",
      " train loss: -0.020976918880933226\n",
      "iter: 5119, \n",
      " train loss: 0.017848220501150536\n",
      "iter: 6143, \n",
      " train loss: 0.02257631879936639\n",
      "iter: 7167, \n",
      " train loss: -0.004972934181296296\n",
      "iter: 8191, \n",
      " train loss: -0.010209559092800191\n",
      "iter: 9215, \n",
      " train loss: 0.057543962477666355\n",
      "iter: 10239, \n",
      " train loss: 0.01731377030864678\n",
      "iter: 11263, \n",
      " train loss: 0.09355451947908477\n",
      "iter: 12287, \n",
      " train loss: 0.008970301129011204\n",
      "iter: 13311, \n",
      " train loss: 0.04897883080820975\n",
      "iter: 14335, \n",
      " train loss: -0.0242333594030697\n",
      "iter: 15359, \n",
      " train loss: -0.0320332298144308\n",
      "iter: 16383, \n",
      " train loss: 0.04110037381997245\n",
      "iter: 17407, \n",
      " train loss: -0.06405134648343846\n",
      "iter: 18431, \n",
      " train loss: -0.006292864737588388\n",
      "iter: 19455, \n",
      " train loss: 0.014717422582180006\n",
      "iter: 20479, \n",
      " train loss: -0.025821928602908883\n",
      "iter: 21503, \n",
      " train loss: 0.020590642883927757\n",
      "iter: 22527, \n",
      " train loss: -0.014683105933499974\n",
      "iter: 23551, \n",
      " train loss: 0.00656366502767014\n",
      "iter: 24575, \n",
      " train loss: -0.00288087672470283\n",
      "iter: 25599, \n",
      " train loss: 0.010030102161202592\n"
     ]
    }
   ],
   "source": [
    "def empty_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import math\n",
    "import gc\n",
    "\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,logP_old,adv,l) in enumerate(from_gen(input_ids,logP_list,ys,lengths)):\n",
    "        if i > 0:\n",
    "            del logits,logP,loss\n",
    "            empty_cache()\n",
    "        logits = model(text).logits[0,l:-1] # 1,l,C\n",
    "        logP = logP_from_logits(logits, text[0,l+1:])\n",
    "        loss = loss_fn(logP,logP_old,adv,clip_ratio)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()): continue\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../Model/PRM_LORA1_merged_code_policy_01 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_version = str(int(version) + 1)\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_01\"\n",
    "# !mkdir peft_model_id\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9352a1cb284e6f95ba30fff11dcf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del logits,logP,loss\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_01\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "base_model2.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../Model/PRM_LORA2_merged_code_policy_01/tokenizer_config.json',\n",
       " '../Model/PRM_LORA2_merged_code_policy_01/special_tokens_map.json',\n",
       " '../Model/PRM_LORA2_merged_code_policy_01/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "tokenizer.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
