{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "from datasets import load_dataset,load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen data\n",
    "with open('../Data/PRM_data/gen_texts.pkl', 'rb') as file:\n",
    "    gen_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_targets.pkl', 'rb') as file:\n",
    "    gen_targets = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_starts_ends.pkl', 'rb') as file:\n",
    "    gen_starts_ends = pickle.load(file)\n",
    "\n",
    "# sol data\n",
    "with open('../Data/PRM_data/sol_texts.pkl', 'rb') as file:\n",
    "    sol_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/sol_starts_ends.pkl', 'rb') as file:\n",
    "    sol_starts_ends = pickle.load(file)\n",
    "\n",
    "# Math-Shepherd\n",
    "dataset = load_dataset('../Data/Math-Shepherd')\n",
    "\n",
    "# MMOS\n",
    "dataset2 = load_from_disk('../Data/MMOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1200\n",
    "def shuffle_lists(*args):\n",
    "    combined = list(zip(*args))\n",
    "    random.shuffle(combined)\n",
    "    return list(zip(*combined))\n",
    "\n",
    "def np2torch(input,addBatchDim=True):\n",
    "    if addBatchDim:\n",
    "        return torch.tensor(input,device='cuda')[None]\n",
    "    else:\n",
    "        return torch.tensor(input,device='cuda')\n",
    "\n",
    "def from_shepherd(dataset):\n",
    "    # yield token_id, index, target, data_source\n",
    "    dataset = dataset.shuffle()\n",
    "    for data in dataset['train']:\n",
    "        if len(data['index']) != len(data['targets']): continue\n",
    "        if max(data['index']) > MAX_LEN:\n",
    "            out = zip(*[(d,t) for d,t in zip(data['index'],data['targets']) if d<MAX_LEN])\n",
    "            if out:\n",
    "                index,targets = out\n",
    "            else:\n",
    "                continue # out is [] -> all index > MAX_LEN\n",
    "        else:\n",
    "            index,targets = data['index'],data['targets']\n",
    "        yield np2torch(data['input_id'][:max(index)+1]),np2torch(index,False),np2torch(targets).float(),0\n",
    "\n",
    "def from_mmos(dataset,num_of_points=5):\n",
    "    # yield token_id, index, target, data_source\n",
    "    dataset = dataset.shuffle()\n",
    "    for data in dataset['train']:\n",
    "        text = data['input_id']\n",
    "        start,end = data['starts_ends']\n",
    "        end = min(end,MAX_LEN)\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            continue\n",
    "        else: \n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # targets = np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text[:max(index)+1]),np2torch(index,False),\\\n",
    "                    torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),3\n",
    "\n",
    "def from_sol(texts,starts_ends,num_of_points=5):\n",
    "    texts,starts_ends = shuffle_lists(texts,starts_ends)\n",
    "    for text,(start,end) in zip(texts,starts_ends):\n",
    "        end = min(end,MAX_LEN)\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            continue\n",
    "        else: \n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # targets = np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text[:max(index)+1]),np2torch(index,False),\\\n",
    "                    torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),1\n",
    "        \n",
    "def from_genData(texts,targets,starts_ends,num_of_points=5):\n",
    "    texts,targets,starts_ends = shuffle_lists(texts,targets,starts_ends)\n",
    "    for text,y,(start,end) in zip(texts,targets,starts_ends):\n",
    "        end = min(end,MAX_LEN)\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            continue\n",
    "        else:\n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # target = y * np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text[:max(index)+1]),np2torch(index,False),\\\n",
    "                    y*torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e084e9a25f40e5bc06ba875a1aa58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-rl and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained('deepseek-ai/deepseek-math-7b-rl',\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       torch_dtype=\"auto\",\\\n",
    "                                                       attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "model.score = model.score.float()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.score.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, loss: [0.6983305301484475, 0.7101281848121598, 0.7084300920061352]\n",
      "iter: 2047, loss: [0.6851908006863288, 0.6916047665031075, 0.7124841071643437]\n",
      "iter: 3071, loss: [0.6872878611262593, 0.6627943027786344, 0.719804101500693]\n",
      "iter: 4095, loss: [0.6924264471202303, 0.6356581202175611, 0.7236687314440633]\n",
      "iter: 5119, loss: [0.6937833166958993, 0.6292938889820905, 0.7132304342261507]\n",
      "iter: 6143, loss: [0.6900924494888776, 0.6158084587918388, 0.707914371119916]\n",
      "iter: 7167, loss: [0.688401895009877, 0.6027550736655238, 0.709160310134553]\n",
      "iter: 8191, loss: [0.6910998361152515, 0.6003385509563681, 0.6963855191584556]\n",
      "iter: 9215, loss: [0.6799317163106633, 0.5954016733762116, 0.6894079299091943]\n",
      "iter: 10239, loss: [0.6862649718337744, 0.5795238888508414, 0.6891365681301084]\n",
      "iter: 11263, loss: [0.6809413582957976, 0.5622165842839351, 0.6912893049003791]\n",
      "iter: 12287, loss: [0.6772415956094467, 0.5540770841272253, 0.6917170327255103]\n",
      "iter: 13311, loss: [0.6718460233155583, 0.5399993615241344, 0.697696793671937]\n",
      "iter: 14335, loss: [0.67761338053391, 0.5347885759520041, 0.695259722446766]\n",
      "iter: 15359, loss: [0.6802033429796045, 0.5261828938597127, 0.6839521568366859]\n",
      "iter: 16383, loss: [0.6660483233914697, 0.5267328168989277, 0.6634416073037867]\n",
      "iter: 17407, loss: [0.6664484161556813, 0.5090130091237882, 0.6688284661588082]\n",
      "iter: 18431, loss: [0.6794882827490306, 0.48941919512567467, 0.6859997877102794]\n",
      "iter: 19455, loss: [0.6631345910585521, 0.5029350039371647, 0.6714248845451757]\n",
      "iter: 20479, loss: [0.6719226281196751, 0.48861458670358854, 0.6666884588356242]\n",
      "iter: 21503, loss: [0.6642173840677983, 0.48814652834022254, 0.6593430133159559]\n",
      "iter: 22527, loss: [0.6690172856329474, 0.4850848616043494, 0.6508324335367359]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in zip(from_shepherd(dataset),\\\n",
    "                    from_sol(sol_texts,sol_starts_ends),\\\n",
    "                    from_genData(gen_texts,gen_targets,gen_starts_ends)):\n",
    "        for d in data:\n",
    "            # if d is None: continue # zip_longest will return None for shorter iterable\n",
    "            text,index,target,source = d\n",
    "            hidden_states = model.model(text)[0].float()\n",
    "            logits = model.score(hidden_states)[:,index,0]\n",
    "            loss = loss_fn(logits,target)\n",
    "            loss.backward()\n",
    "            \n",
    "            train_loss[source] += loss.item()\n",
    "            count_loss[source] += 1\n",
    "            i += 1\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(model.score.parameters(),clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            if (i + 1) % verbose == 0:\n",
    "                print(f\"iter: {i}, loss: {[l/c if c!=0 else 'N/A' for l,c in zip(train_loss,count_loss)]}\")\n",
    "                train_loss = [0,0,0]\n",
    "                count_loss = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../Model/PRM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "alpha_factor = 4.0\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "topics_num = 4\n",
    "weights=[0.5,0.2,0.2,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification,BitsAndBytesConfig,AutoConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac159d84b0b4df5a5190e2c5e337702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = LlamaForSequenceClassification.from_pretrained('../Model/PRM',\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       torch_dtype=\"auto\",\n",
    "                                                       quantization_config=quantization_config,\n",
    "                                                       attn_implementation=\"flash_attention_2\"\n",
    "                                                       )\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,509,674,496 || trainable%: 0.287866928085493\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "base_model = get_peft_model(model.model, peft_config)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.print_trainable_parameters()\n",
    "model.score = model.score.float()\n",
    "model.score.weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def sample_from_iterables(weights,*iterables):\n",
    "    while True:\n",
    "        iterable = random.choices(iterables, weights=weights, k=1)[0]\n",
    "        try:\n",
    "            yield next(iterable)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha  # Store alpha in the context\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.alpha, None  # Use stored alpha, return None for alpha's grad\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def forward(self, x, alpha):\n",
    "        return GradientReversalFunction.apply(x, alpha)\n",
    "\n",
    "class revLinear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(revLinear, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(input_dim, input_dim),nn.CELU(),nn.Linear(input_dim, output_dim))\n",
    "        self.grad_rev = GradientReversalLayer()\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        return self.layers(self.grad_rev(x, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = revLinear(model.score.weight.shape[1],topics_num).to('cuda').float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params =  base_params + \\\n",
    "                    list(model.score.parameters()) + \\\n",
    "                    list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: [0.6586695021451122, 0.5011874470943795, 0.6152544320762837]\n",
      " topic loss: [0.27065960769400454, 1.1094748996254196, 1.022733565292431]\n",
      "iter: 2047, \n",
      " train loss: [0.6595108968341996, 0.5713091604411602, 0.6187274953468138]\n",
      " topic loss: [0.14879767537744107, 0.7093883109046146, 0.7141550876837237]\n",
      "iter: 3071, \n",
      " train loss: [0.6649498947095964, 0.5957006261274509, 0.6691991997649893]\n",
      " topic loss: [0.30601415178080593, 0.7701010363111236, 0.8107206465210766]\n",
      "iter: 4095, \n",
      " train loss: [0.6915157938049478, 0.5918818094575679, 0.7046043671698237]\n",
      " topic loss: [0.545760416915651, 0.8766326547124004, 0.8753725546622964]\n",
      "iter: 5119, \n",
      " train loss: [0.6797502887685124, 0.6271690009685045, 0.7075983670522582]\n",
      " topic loss: [0.9726178204504455, 1.1583269673786807, 1.0148750835994504]\n",
      "iter: 6143, \n",
      " train loss: [0.6827877082180459, 0.6134802477252215, 0.7526266221031249]\n",
      " topic loss: [1.6970238533363304, 1.2060306280627286, 1.5317563136735286]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m loss_topic(logits_topics,target_topics)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m train_loss[source] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss1\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m topic_loss[source] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss_topic = torch.nn.CrossEntropyLoss()\n",
    "asym =lambda x: x if x<0 else 2*x\n",
    "# sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "softplue = lambda x:np.log(1 + np.exp(x))\n",
    "\n",
    "train_loss = [0,0,0,0]\n",
    "topic_loss = [0,0,0,0]\n",
    "count_loss = [0,0,0,0]\n",
    "loss2_base = -np.log(1/topics_num)\n",
    "for epoch in range(epochs):\n",
    "    iterables = [from_shepherd(dataset),\\\n",
    "                 from_sol(sol_texts,sol_starts_ends),\n",
    "                 from_genData(gen_texts,gen_targets,gen_starts_ends),\\\n",
    "                 from_mmos(dataset2)]\n",
    "    for i,(text,index,target,source) in enumerate(sample_from_iterables(weights, *iterables)):\n",
    "        target_topics = source * torch.ones(target.shape[1],dtype=torch.long,device='cuda') # l\n",
    "        hidden_states = base_model(text)[0][:,index].float() # b,l,d\n",
    "        logits = model.score(hidden_states)[:,:,0] # b,l\n",
    "        \n",
    "        if sum(count_loss) < 10:\n",
    "            alpha = 0.5\n",
    "        else:\n",
    "            loss2_running_avg = sum(topic_loss)/sum(count_loss)\n",
    "            alpha = softplue(alpha_factor*asym(loss2_base-loss2_running_avg))\n",
    "        logits_topics = topic_model(hidden_states,alpha)[0] # l,C\n",
    "        loss1 = loss_fn(logits,target)\n",
    "        loss2 = loss_topic(logits_topics,target_topics)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss[source] += loss1.item()\n",
    "        topic_loss[source] += loss2.item()\n",
    "        count_loss[source] += 1\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(base_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {[l/c if c!=0 else 'N/A' for l,c in zip(train_loss,count_loss)]}\\n topic loss: {[l/c if c!=0 else 'N/A' for l,c in zip(topic_loss,count_loss)]}\")\n",
    "            train_loss = [0,0,0,0]\n",
    "            topic_loss = [0,0,0,0]\n",
    "            count_loss = [0,0,0,0]\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
