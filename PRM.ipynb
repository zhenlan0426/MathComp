{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen data\n",
    "with open('../Data/PRM_data/gen_texts.pkl', 'rb') as file:\n",
    "    gen_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_targets.pkl', 'rb') as file:\n",
    "    gen_targets = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_starts_ends.pkl', 'rb') as file:\n",
    "    gen_starts_ends = pickle.load(file)\n",
    "\n",
    "# sol data\n",
    "with open('../Data/PRM_data/sol_texts.pkl', 'rb') as file:\n",
    "    sol_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/sol_starts_ends.pkl', 'rb') as file:\n",
    "    sol_starts_ends = pickle.load(file)\n",
    "\n",
    "# Math-Shepherd\n",
    "dataset = load_dataset('../Data/Math-Shepherd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_lists(*args):\n",
    "    combined = list(zip(*args))\n",
    "    random.shuffle(combined)\n",
    "    return list(zip(*combined))\n",
    "\n",
    "def np2torch(input,addBatchDim=True):\n",
    "    if addBatchDim:\n",
    "        return torch.tensor(input,device='cuda')[None]\n",
    "    else:\n",
    "        return torch.tensor(input,device='cuda')\n",
    "\n",
    "def from_shepherd(dataset):\n",
    "    # yield token_id, index, target, data_source\n",
    "    dataset = dataset.shuffle()\n",
    "    for data in dataset['train']:\n",
    "        yield np2torch(data['input_id']),np2torch(data['index'],False),np2torch(data['targets']).float(),0\n",
    "\n",
    "def from_sol(texts,starts_ends,num_of_points=5):\n",
    "    texts,starts_ends = shuffle_lists(texts,starts_ends)\n",
    "    for text,(start,end) in zip(texts,starts_ends):\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            yield np2torch(text),end,torch.ones(1,device='cuda',dtype=torch.float32),1\n",
    "        else: \n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # targets = np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text),np2torch(index,False),\\\n",
    "                    torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),1\n",
    "        \n",
    "def from_genData(texts,targets,starts_ends,num_of_points=5):\n",
    "    texts,targets,starts_ends = shuffle_lists(texts,targets,starts_ends)\n",
    "    for text,y,(start,end) in zip(texts,targets,starts_ends):\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            yield np2torch(text),end,y*torch.ones(1,device='cuda',dtype=torch.float32),2\n",
    "        else:\n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # target = y * np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text),np2torch(index,False),\\\n",
    "                    y*torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e084e9a25f40e5bc06ba875a1aa58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-rl and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained('deepseek-ai/deepseek-math-7b-rl',\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       torch_dtype=\"auto\",\\\n",
    "                                                       attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "model.score = model.score.float()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.score.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, loss: [0.6983305301484475, 0.7101281848121598, 0.7084300920061352]\n",
      "iter: 2047, loss: [0.6851908006863288, 0.6916047665031075, 0.7124841071643437]\n",
      "iter: 3071, loss: [0.6872878611262593, 0.6627943027786344, 0.719804101500693]\n",
      "iter: 4095, loss: [0.6924264471202303, 0.6356581202175611, 0.7236687314440633]\n",
      "iter: 5119, loss: [0.6937833166958993, 0.6292938889820905, 0.7132304342261507]\n",
      "iter: 6143, loss: [0.6900924494888776, 0.6158084587918388, 0.707914371119916]\n",
      "iter: 7167, loss: [0.688401895009877, 0.6027550736655238, 0.709160310134553]\n",
      "iter: 8191, loss: [0.6910998361152515, 0.6003385509563681, 0.6963855191584556]\n",
      "iter: 9215, loss: [0.6799317163106633, 0.5954016733762116, 0.6894079299091943]\n",
      "iter: 10239, loss: [0.6862649718337744, 0.5795238888508414, 0.6891365681301084]\n",
      "iter: 11263, loss: [0.6809413582957976, 0.5622165842839351, 0.6912893049003791]\n",
      "iter: 12287, loss: [0.6772415956094467, 0.5540770841272253, 0.6917170327255103]\n",
      "iter: 13311, loss: [0.6718460233155583, 0.5399993615241344, 0.697696793671937]\n",
      "iter: 14335, loss: [0.67761338053391, 0.5347885759520041, 0.695259722446766]\n",
      "iter: 15359, loss: [0.6802033429796045, 0.5261828938597127, 0.6839521568366859]\n",
      "iter: 16383, loss: [0.6660483233914697, 0.5267328168989277, 0.6634416073037867]\n",
      "iter: 17407, loss: [0.6664484161556813, 0.5090130091237882, 0.6688284661588082]\n",
      "iter: 18431, loss: [0.6794882827490306, 0.48941919512567467, 0.6859997877102794]\n",
      "iter: 19455, loss: [0.6631345910585521, 0.5029350039371647, 0.6714248845451757]\n",
      "iter: 20479, loss: [0.6719226281196751, 0.48861458670358854, 0.6666884588356242]\n",
      "iter: 21503, loss: [0.6642173840677983, 0.48814652834022254, 0.6593430133159559]\n",
      "iter: 22527, loss: [0.6690172856329474, 0.4850848616043494, 0.6508324335367359]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_loss = [0,0,0]\n",
    "count_loss = [0,0,0]\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in zip(from_shepherd(dataset),\\\n",
    "                    from_sol(sol_texts,sol_starts_ends),\\\n",
    "                    from_genData(gen_texts,gen_targets,gen_starts_ends)):\n",
    "        for d in data:\n",
    "            # if d is None: continue # zip_longest will return None for shorter iterable\n",
    "            text,index,target,source = d\n",
    "            hidden_states = model.model(text)[0].float()\n",
    "            logits = model.score(hidden_states)[:,index,0]\n",
    "            loss = loss_fn(logits,target)\n",
    "            loss.backward()\n",
    "            \n",
    "            train_loss[source] += loss.item()\n",
    "            count_loss[source] += 1\n",
    "            i += 1\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(model.score.parameters(),clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            if (i + 1) % verbose == 0:\n",
    "                print(f\"iter: {i}, loss: {[l/c if c!=0 else 'N/A' for l,c in zip(train_loss,count_loss)]}\")\n",
    "                train_loss = [0,0,0]\n",
    "                count_loss = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../Model/PRM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
