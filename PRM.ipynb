{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen data\n",
    "with open('../Data/PRM_data/gen_texts.pkl', 'rb') as file:\n",
    "    gen_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_targets.pkl', 'rb') as file:\n",
    "    gen_targets = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_starts_ends.pkl', 'rb') as file:\n",
    "    gen_starts_ends = pickle.load(file)\n",
    "\n",
    "# sol data\n",
    "with open('../Data/PRM_data/sol_texts.pkl', 'rb') as file:\n",
    "    sol_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/sol_starts_ends.pkl', 'rb') as file:\n",
    "    sol_starts_ends = pickle.load(file)\n",
    "\n",
    "# Math-Shepherd\n",
    "dataset = load_dataset('../Data/Math-Shepherd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_lists(*args):\n",
    "    combined = list(zip(*args))\n",
    "    random.shuffle(combined)\n",
    "    return list(zip(*combined))\n",
    "\n",
    "def np2torch(input,addBatchDim=True):\n",
    "    if addBatchDim:\n",
    "        return torch.tensor(input,device='cuda')[None]\n",
    "    else:\n",
    "        return torch.tensor(input,device='cuda')\n",
    "\n",
    "def from_shepherd(dataset):\n",
    "    # yield token_id, index, target, data_source\n",
    "    dataset = dataset.shuffle()\n",
    "    for data in dataset['train']:\n",
    "        yield np2torch(data['input_id']),np2torch(data['index'],False),np2torch(data['targets']).float(),0\n",
    "\n",
    "def from_sol(texts,starts_ends,num_of_points=5):\n",
    "    texts,starts_ends = shuffle_lists(texts,starts_ends)\n",
    "    for text,(start,end) in zip(texts,starts_ends):\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            yield np2torch(text),end,torch.ones(1,device='cuda',dtype=torch.float32),1\n",
    "        else: \n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # targets = np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text),np2torch(index,False),\\\n",
    "                    torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),1\n",
    "        \n",
    "def from_genData(texts,targets,starts_ends,num_of_points=5):\n",
    "    texts,targets,starts_ends = shuffle_lists(texts,targets,starts_ends)\n",
    "    for text,y,(start,end) in zip(texts,targets,starts_ends):\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            yield np2torch(text),end,y*torch.ones(1,device='cuda',dtype=torch.float32),2\n",
    "        else:\n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # target = y * np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text),np2torch(index,False),\\\n",
    "                    y*torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e084e9a25f40e5bc06ba875a1aa58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-rl and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained('deepseek-ai/deepseek-math-7b-rl',\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       torch_dtype=\"auto\",\\\n",
    "                                                       attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "model.score = model.score.float()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.score.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = [0,0,0]\n",
    "count_loss = [0,0,0]\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in zip(from_shepherd(dataset),\\\n",
    "                    from_sol(sol_texts,sol_starts_ends),\\\n",
    "                    from_genData(gen_texts,gen_targets,gen_starts_ends)):\n",
    "        for d in data:\n",
    "            # if d is None: continue # zip_longest will return None for shorter iterable\n",
    "            text,index,target,source = d\n",
    "            hidden_states = model.model(text)[0].float()\n",
    "            logits = model.score(hidden_states)[:,index,0]\n",
    "            loss = loss_fn(logits,target)\n",
    "            loss.backward()\n",
    "            \n",
    "            train_loss[source] += loss.item()\n",
    "            count_loss[source] += 1\n",
    "            i += 1\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(model.score.parameters(),clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            if (i + 1) % verbose == 0:\n",
    "                print(f\"iter: {i}, loss: {[l/c if c!=0 else 'N/A' for l,c in zip(train_loss,count_loss)]}\")\n",
    "                train_loss = [0,0,0]\n",
    "                count_loss = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../Model/PRM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
