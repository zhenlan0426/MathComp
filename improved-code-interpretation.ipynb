{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4c46d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:09:32.182719Z",
     "iopub.status.busy": "2024-04-19T19:09:32.182296Z",
     "iopub.status.idle": "2024-04-19T19:09:32.187562Z",
     "shell.execute_reply": "2024-04-19T19:09:32.186496Z"
    },
    "papermill": {
     "duration": 0.02062,
     "end_time": "2024-04-19T19:09:32.190028",
     "exception": false,
     "start_time": "2024-04-19T19:09:32.169408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n",
    "\n",
    "\n",
    "# credits:\n",
    "# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n",
    "# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n",
    "# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d0a731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:09:32.214718Z",
     "iopub.status.busy": "2024-04-19T19:09:32.214087Z",
     "iopub.status.idle": "2024-04-19T19:09:32.224760Z",
     "shell.execute_reply": "2024-04-19T19:09:32.223711Z"
    },
    "papermill": {
     "duration": 0.025677,
     "end_time": "2024-04-19T19:09:32.227379",
     "exception": false,
     "start_time": "2024-04-19T19:09:32.201702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "NOTEBOOK_START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7b971",
   "metadata": {
    "papermill": {
     "duration": 0.012458,
     "end_time": "2024-04-19T19:09:32.252861",
     "exception": false,
     "start_time": "2024-04-19T19:09:32.240403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TO-DO\n",
    "\n",
    "Change temperature as the question goes longer\n",
    "Change temperature based on question lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabac38",
   "metadata": {
    "papermill": {
     "duration": 0.009874,
     "end_time": "2024-04-19T19:09:32.273076",
     "exception": false,
     "start_time": "2024-04-19T19:09:32.263202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n",
    "\n",
    "Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n",
    "\n",
    "In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2483ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:09:32.294500Z",
     "iopub.status.busy": "2024-04-19T19:09:32.293789Z",
     "iopub.status.idle": "2024-04-19T19:09:32.298984Z",
     "shell.execute_reply": "2024-04-19T19:09:32.298086Z"
    },
    "papermill": {
     "duration": 0.017767,
     "end_time": "2024-04-19T19:09:32.300810",
     "exception": false,
     "start_time": "2024-04-19T19:09:32.283043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "QUANT = False\n",
    "\n",
    "if QUANT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "USE_PAST_KEY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e893b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:09:32.321130Z",
     "iopub.status.busy": "2024-04-19T19:09:32.320828Z",
     "iopub.status.idle": "2024-04-19T19:10:00.232998Z",
     "shell.execute_reply": "2024-04-19T19:10:00.231856Z"
    },
    "papermill": {
     "duration": 27.924934,
     "end_time": "2024-04-19T19:10:00.235352",
     "exception": false,
     "start_time": "2024-04-19T19:09:32.310418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.39.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 19:09:46.736225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 19:09:46.736341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 19:09:47.061445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.92 s, sys: 1.64 s, total: 10.6 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if QUANT:\n",
    "    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n",
    "    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n",
    "\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    StoppingCriteria,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "import transformers\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985ab34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:00.259549Z",
     "iopub.status.busy": "2024-04-19T19:10:00.258967Z",
     "iopub.status.idle": "2024-04-19T19:10:00.622305Z",
     "shell.execute_reply": "2024-04-19T19:10:00.621132Z"
    },
    "papermill": {
     "duration": 0.377383,
     "end_time": "2024-04-19T19:10:00.624428",
     "exception": false,
     "start_time": "2024-04-19T19:10:00.247045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000aaa</td>\n",
       "      <td>What is $1-1$?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111bbb</td>\n",
       "      <td>What is $0\\times10$?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222ccc</td>\n",
       "      <td>Solve $4+x=4$ for $x$.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                 problem\n",
       "0  000aaa          What is $1-1$?\n",
       "1  111bbb    What is $0\\times10$?\n",
       "2  222ccc  Solve $4+x=4$ for $x$."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "PRIVATE = True\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217089cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:00.646545Z",
     "iopub.status.busy": "2024-04-19T19:10:00.645905Z",
     "iopub.status.idle": "2024-04-19T19:10:00.683358Z",
     "shell.execute_reply": "2024-04-19T19:10:00.682443Z"
    },
    "papermill": {
     "duration": 0.050441,
     "end_time": "2024-04-19T19:10:00.685285",
     "exception": false,
     "start_time": "2024-04-19T19:10:00.634844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229ee8</td>\n",
       "      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246d26</td>\n",
       "      <td>Each of the three-digits numbers $111$ to $999...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fc4ad</td>\n",
       "      <td>Let the `sparkle' operation on positive intege...</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430b63</td>\n",
       "      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5277ed</td>\n",
       "      <td>There exists a unique increasing geometric seq...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            problem  answer\n",
       "0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n",
       "1  246d26  Each of the three-digits numbers $111$ to $999...     250\n",
       "2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n",
       "3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n",
       "4  5277ed  There exists a unique increasing geometric seq...     211"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(df) < 5:\n",
    "    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "    PRIVATE = False\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9dad35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:00.707340Z",
     "iopub.status.busy": "2024-04-19T19:10:00.707057Z",
     "iopub.status.idle": "2024-04-19T19:10:00.712518Z",
     "shell.execute_reply": "2024-04-19T19:10:00.711755Z"
    },
    "papermill": {
     "duration": 0.018431,
     "end_time": "2024-04-19T19:10:00.714378",
     "exception": false,
     "start_time": "2024-04-19T19:10:00.695947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_parse(answer):\n",
    "    out = []\n",
    "    start = False\n",
    "    end = False\n",
    "    for l in reversed(list(answer)):\n",
    "        if l in '0123456789' and not end:\n",
    "            start = True\n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:\n",
    "                end = True\n",
    "        \n",
    "    out = reversed(out)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964bd20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:00.735522Z",
     "iopub.status.busy": "2024-04-19T19:10:00.735268Z",
     "iopub.status.idle": "2024-04-19T19:10:00.749537Z",
     "shell.execute_reply": "2024-04-19T19:10:00.748686Z"
    },
    "papermill": {
     "duration": 0.026961,
     "end_time": "2024-04-19T19:10:00.751451",
     "exception": false,
     "start_time": "2024-04-19T19:10:00.724490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def return_last_print(output, n):\n",
    "    lines = output.strip().split('\\n')\n",
    "    if lines:\n",
    "        return lines[n]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def process_code(code, return_shell_output=False):\n",
    "    \n",
    "    def repl(match):\n",
    "        if \"real\" not in match.group():\n",
    "            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "        else:\n",
    "            return \"{}{}\".format(match.group()[:-1], ')')\n",
    "    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n",
    "\n",
    "    if return_shell_output:\n",
    "        code = code.replace('\\n', '\\n    ')\n",
    "            # Add a try...except block\n",
    "        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n",
    "    \n",
    "    if not return_shell_output:\n",
    "        print(code)\n",
    "    with open('code.py', 'w') as fout:\n",
    "        fout.write(code)\n",
    "    \n",
    "    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "    try:\n",
    "        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "        return_value = return_last_print(shell_output, -1)\n",
    "        print(shell_output)\n",
    "        if return_shell_output:\n",
    "            if return_value=='FAIL':\n",
    "                CODE_STATUS = False\n",
    "                return_value = return_last_print(shell_output, -2)\n",
    "                if \"not defined\" in return_value:\n",
    "                    return_value+='\\nTry checking the formatting and imports'\n",
    "            else:\n",
    "                CODE_STATUS = True\n",
    "            return return_value, CODE_STATUS  \n",
    "        code_output = round(float(eval(return_value))) % 1000\n",
    "    except Exception as e:\n",
    "        print(e,'shell_output')\n",
    "        code_output = -1\n",
    "    \n",
    "    if return_shell_output:\n",
    "        if code_output==-1:\n",
    "            CODE_STATUS = False\n",
    "        else:\n",
    "            CODE_STATUS = True\n",
    "        return code_output, CODE_STATUS  \n",
    "    \n",
    "    \n",
    "    return code_output\n",
    "\n",
    "\n",
    "def process_text_output(output):\n",
    "    result = output    \n",
    "    try:\n",
    "        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n",
    "\n",
    "        print('BOXED', result_output)\n",
    "        if not len(result_output):\n",
    "            result_output = naive_parse(result)\n",
    "        else:\n",
    "            result_output = result_output[-1]\n",
    "\n",
    "        print('BOXED FINAL', result_output)\n",
    "        if not len(result_output):\n",
    "            result_output = -1\n",
    "        \n",
    "        else:\n",
    "            result_output = round(float(eval(result_output))) % 1000\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('ERROR PARSING TEXT')\n",
    "        result_output = -1\n",
    "    \n",
    "    return result_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2e87fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:00.773017Z",
     "iopub.status.busy": "2024-04-19T19:10:00.772743Z",
     "iopub.status.idle": "2024-04-19T19:10:01.004243Z",
     "shell.execute_reply": "2024-04-19T19:10:01.003348Z"
    },
    "papermill": {
     "duration": 0.244673,
     "end_time": "2024-04-19T19:10:01.006189",
     "exception": false,
     "start_time": "2024-04-19T19:10:00.761516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0adf147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.028586Z",
     "iopub.status.busy": "2024-04-19T19:10:01.028286Z",
     "iopub.status.idle": "2024-04-19T19:10:01.033050Z",
     "shell.execute_reply": "2024-04-19T19:10:01.032171Z"
    },
    "papermill": {
     "duration": 0.018201,
     "end_time": "2024-04-19T19:10:01.034873",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.016672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "n_repetitions = 22 if PRIVATE else 4\n",
    "TOTAL_TOKENS = 2048 # if PRIVATE else 512\n",
    "\n",
    "if PRIVATE:\n",
    "    TIME_LIMIT = 31500\n",
    "else:\n",
    "    TIME_LIMIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5df27a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.057303Z",
     "iopub.status.busy": "2024-04-19T19:10:01.057019Z",
     "iopub.status.idle": "2024-04-19T19:10:01.072843Z",
     "shell.execute_reply": "2024-04-19T19:10:01.071953Z"
    },
    "papermill": {
     "duration": 0.029393,
     "end_time": "2024-04-19T19:10:01.074823",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.045430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PRIVATE:\n",
    "\n",
    "    MODEL_PATH = \"/kaggle/input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "    DEEP = True\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "    config.gradient_checkpointing = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    device_map = [('model.embed_tokens', 0),\n",
    "                 ('model.layers.0', 0),\n",
    "                 ('model.layers.1', 0),\n",
    "                 ('model.layers.2', 0),\n",
    "                 ('model.layers.3', 0),\n",
    "                 ('model.layers.4', 0),\n",
    "                 ('model.layers.5', 0),\n",
    "                 ('model.layers.6', 0),\n",
    "                 ('model.layers.7', 0),\n",
    "                 ('model.layers.8', 0),\n",
    "                 ('model.layers.9', 0),\n",
    "                 ('model.layers.10', 0),\n",
    "                 ('model.layers.11', 0),\n",
    "                 ('model.layers.12', 0),\n",
    "                 ('model.layers.13', 0),\n",
    "                 ('model.layers.14', 0),\n",
    "                 ('model.layers.15', 0),\n",
    "                 ('model.layers.16', 0),\n",
    "                 ('model.layers.17', 0),\n",
    "                 ('model.layers.18', 0),\n",
    "                 ('model.layers.19', 0),\n",
    "                 ('model.layers.20', 0),\n",
    "                 ('model.layers.21', 0),\n",
    "                 ('model.layers.22', 1),\n",
    "                 ('model.layers.23', 1),\n",
    "                 ('model.layers.24', 1),\n",
    "                 ('model.layers.25', 1),\n",
    "                 ('model.layers.26', 1),\n",
    "                 ('model.layers.27', 1),\n",
    "                 ('model.layers.28', 1),\n",
    "                 ('model.layers.29', 1),\n",
    "                 ('model.norm', 1),\n",
    "                 ('lm_head', 1)]\n",
    "\n",
    "    device_map = {ii:jj for (ii,jj) in device_map}\n",
    "\n",
    "    if QUANT:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"sequential\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True, \n",
    "            quantization_config=quantization_config,\n",
    "            config=config\n",
    "        )\n",
    "    else:  \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            #quantization_config=quantization_config,\n",
    "            config=config\n",
    "        )\n",
    "    \n",
    "    pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype='auto',\n",
    "    device_map=device_map,\n",
    ")\n",
    "    from transformers import StoppingCriteriaList\n",
    "\n",
    "    class StoppingCriteriaSub(StoppingCriteria):\n",
    "        def __init__(self, stops = [], encounters=1):\n",
    "            super().__init__()\n",
    "            self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "            for stop in self.stops:\n",
    "                last_token = input_ids[0][-len(stop):]\n",
    "                if torch.all(torch.eq(stop,last_token)):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "\n",
    "    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n",
    "    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    model.dtype, model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc0737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T17:56:59.460578Z",
     "iopub.status.busy": "2024-04-19T17:56:59.460286Z",
     "iopub.status.idle": "2024-04-19T17:57:00.543889Z",
     "shell.execute_reply": "2024-04-19T17:57:00.543125Z",
     "shell.execute_reply.started": "2024-04-19T17:56:59.460553Z"
    },
    "papermill": {
     "duration": 0.010105,
     "end_time": "2024-04-19T19:10:01.095169",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.085064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af8ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T18:30:08.184886Z",
     "iopub.status.busy": "2024-04-19T18:30:08.184504Z",
     "iopub.status.idle": "2024-04-19T18:30:08.195062Z",
     "shell.execute_reply": "2024-04-19T18:30:08.193958Z",
     "shell.execute_reply.started": "2024-04-19T18:30:08.184855Z"
    },
    "papermill": {
     "duration": 0.010405,
     "end_time": "2024-04-19T19:10:01.115771",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.105366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2bbd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T18:30:08.673373Z",
     "iopub.status.busy": "2024-04-19T18:30:08.673020Z",
     "iopub.status.idle": "2024-04-19T18:30:08.681290Z",
     "shell.execute_reply": "2024-04-19T18:30:08.680330Z",
     "shell.execute_reply.started": "2024-04-19T18:30:08.673346Z"
    },
    "papermill": {
     "duration": 0.009927,
     "end_time": "2024-04-19T19:10:01.136057",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.126130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3303136b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.158089Z",
     "iopub.status.busy": "2024-04-19T19:10:01.157769Z",
     "iopub.status.idle": "2024-04-19T19:10:01.163415Z",
     "shell.execute_reply": "2024-04-19T19:10:01.162527Z"
    },
    "papermill": {
     "duration": 0.019007,
     "end_time": "2024-04-19T19:10:01.165402",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.146395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n",
    "\\\"{}\\\"\n",
    "To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n",
    "\n",
    "Approach:\"\"\"\n",
    "\n",
    "\n",
    "cot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n",
    "\\\"{}\\\"\n",
    "Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n",
    "\n",
    "promplt_options = [code,cot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bca5a7c1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.187353Z",
     "iopub.status.busy": "2024-04-19T19:10:01.187097Z",
     "iopub.status.idle": "2024-04-19T19:10:01.243902Z",
     "shell.execute_reply": "2024-04-19T19:10:01.242956Z"
    },
    "papermill": {
     "duration": 0.072586,
     "end_time": "2024-04-19T19:10:01.248037",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.175451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 223.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "\n",
    "tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n",
    "\n",
    "\n",
    "#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n",
    "#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n",
    "\n",
    "temperature = 0.9\n",
    "top_p = 1.0\n",
    "\n",
    "temperature_coding = 0.9\n",
    "top_p_coding = 1.0\n",
    "\n",
    "   \n",
    "total_results = {}\n",
    "total_answers = {}\n",
    "best_stats = {}\n",
    "total_outputs = {}\n",
    "question_type_counts = {}\n",
    "starting_counts = (2,3)\n",
    "    \n",
    "    \n",
    "for jj in tqdm(range(n_repetitions)):   \n",
    "    for i in tqdm(range(len(df))):\n",
    "        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n",
    "        \n",
    "        if TIME_SPENT>TIME_LIMIT:\n",
    "            break\n",
    "        \n",
    "\n",
    "        id_ = df['id'].loc[i]\n",
    "        problem = df['problem'].loc[i]\n",
    "        print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n",
    "        \n",
    "        best, best_count = best_stats.get(i,(-1,-1))\n",
    "        if best_count>np.sqrt(jj):\n",
    "            print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n",
    "            continue\n",
    "            \n",
    "        outputs = total_outputs.get(i,[])\n",
    "        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n",
    "        results = total_results.get(i,[])\n",
    "        answers = total_answers.get(i,[])\n",
    "        \n",
    "        for _ in range(5):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        try:\n",
    "            ALREADY_GEN = 0\n",
    "            code_error = None\n",
    "            code_error_count = 0\n",
    "            code_output = -1\n",
    "            #initail_message = problem  + tool_instruction \n",
    "            counts = np.array([text_answers,code_answers])\n",
    "\n",
    "            draw = choice(promplt_options, 1,\n",
    "                          p=counts/counts.sum())\n",
    "\n",
    "            initail_message = draw[0].format(problem,\"{}\")            \n",
    "            prompt = f\"User: {initail_message}\"\n",
    "\n",
    "            current_printed = len(prompt)\n",
    "            print(f\"{jj}_{prompt}\\n\")\n",
    "\n",
    "            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "            input_len = len(model_inputs['input_ids'][0])\n",
    "\n",
    "            generation_output = model.generate(**model_inputs, \n",
    "                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n",
    "                                               return_dict_in_generate=USE_PAST_KEY,\n",
    "                                               do_sample = True,\n",
    "                                               temperature = temperature,\n",
    "                                               top_p = top_p,\n",
    "                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n",
    "\n",
    "            if USE_PAST_KEY:\n",
    "                output_ids = generation_output.sequences[0]\n",
    "            else:\n",
    "                output_ids = generation_output[0]\n",
    "            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            print(f\"{decoded_output[current_printed:]}\\n\")\n",
    "            current_printed += len(decoded_output[current_printed:])\n",
    "            cummulative_code = \"\"\n",
    "            \n",
    "            \n",
    "            stop_word_cond = False\n",
    "            for stop_word in stop_words:\n",
    "                stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "                \n",
    "            \n",
    "            while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n",
    "\n",
    "                if (decoded_output[-len(\"```python\"):]==\"```python\"):\n",
    "                    temperature_inner=temperature_coding\n",
    "                    top_p_inner = top_p_coding\n",
    "                    prompt = decoded_output\n",
    "                else:\n",
    "                    temperature_inner=temperature\n",
    "                    top_p_inner = top_p\n",
    "                    try:\n",
    "                        if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n",
    "                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n",
    "                        else:\n",
    "                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n",
    "                        \n",
    "\n",
    "                        cummulative_code+=code_text\n",
    "                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n",
    "                        print('CODE RESULTS', code_output)\n",
    "\n",
    "                        if code_error==code_output:\n",
    "                            code_error_count+=1\n",
    "                        else:\n",
    "                            code_error=code_output\n",
    "                            code_error_count = 0\n",
    "\n",
    "                        if not CODE_STATUS:\n",
    "                            cummulative_code = cummulative_code[:-len(code_text)]\n",
    "\n",
    "                            if code_error_count>=1:\n",
    "                                print(\"REPEATED ERRORS\")\n",
    "                                break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print('ERROR PARSING CODE')\n",
    "                        code_output = -1\n",
    "\n",
    "                    if code_output!=-1:\n",
    "                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n",
    "                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n",
    "                        else:\n",
    "                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n",
    "                    else:\n",
    "                        prompt = decoded_output\n",
    "                        cummulative_code=\"\"\n",
    "\n",
    "\n",
    "                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    old_values = generation_output.past_key_values\n",
    "                else:\n",
    "                    old_values = None\n",
    "\n",
    "                generation_output = model.generate(**model_inputs, \n",
    "                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n",
    "                                                   return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                   past_key_values=old_values,\n",
    "                                                   do_sample = True,\n",
    "                                                   temperature = temperature_inner,\n",
    "                                                   top_p = top_p_inner,\n",
    "                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n",
    "                current_printed+=len(decoded_output[current_printed:])\n",
    "                \n",
    "                stop_word_cond = False\n",
    "                for stop_word in stop_words:\n",
    "                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n",
    "\n",
    "            if USE_PAST_KEY:\n",
    "                output_ids = generation_output.sequences[0]\n",
    "            else:\n",
    "                output_ids = generation_output[0]\n",
    "\n",
    "            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n",
    "            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n",
    "            result_output = process_text_output(raw_output)\n",
    "            \n",
    "            try:\n",
    "                code_output = round(float(eval(code_output))) % 1000\n",
    "            except Exception as e:\n",
    "                print(e,'final_eval')\n",
    "                code_output = -1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e,\"5\")\n",
    "            result_output, code_output = -1, -1\n",
    "\n",
    "        if code_output!=-1:\n",
    "            outputs.append(code_output)\n",
    "            code_answers+=1\n",
    "\n",
    "        if result_output!=-1:\n",
    "            outputs.append(result_output)\n",
    "            text_answers+=1\n",
    "\n",
    "        if len(outputs) > 0:\n",
    "            occurances = Counter(outputs).most_common()\n",
    "            print(occurances)\n",
    "            if occurances[0][1] > best_count:\n",
    "                print(\"GOOD ANSWER UPDATED!\")\n",
    "                best = occurances[0][0]\n",
    "                best_count = occurances[0][1]\n",
    "            if occurances[0][1] > 5:\n",
    "                print(\"ANSWER FOUND!\")\n",
    "                break\n",
    "\n",
    "        results.append(result_output)\n",
    "        answers.append(code_output)\n",
    "        \n",
    "        best_stats[i] = (best, best_count) \n",
    "        question_type_counts[i] = (text_answers, code_answers)\n",
    "        total_outputs[i] = outputs\n",
    "        \n",
    "        total_results[i] = results\n",
    "        total_answers[i] = answers\n",
    "\n",
    "        print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n",
    "        if DEBUG:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb03ea31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.274816Z",
     "iopub.status.busy": "2024-04-19T19:10:01.274500Z",
     "iopub.status.idle": "2024-04-19T19:10:01.280413Z",
     "shell.execute_reply": "2024-04-19T19:10:01.279462Z"
    },
    "papermill": {
     "duration": 0.021392,
     "end_time": "2024-04-19T19:10:01.282351",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.260959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if PRIVATE:\n",
    "    for ii in range(len(df)):\n",
    "        a = total_answers[ii]\n",
    "        b = total_answers[ii]\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        print(a,b)\n",
    "        a[a < 0] = b[a < 0]\n",
    "\n",
    "        pred = Counter(a.tolist()).most_common(2)\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a250af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.307730Z",
     "iopub.status.busy": "2024-04-19T19:10:01.307424Z",
     "iopub.status.idle": "2024-04-19T19:10:01.312097Z",
     "shell.execute_reply": "2024-04-19T19:10:01.311256Z"
    },
    "papermill": {
     "duration": 0.019526,
     "end_time": "2024-04-19T19:10:01.313872",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.294346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PRIVATE:\n",
    "    df['answer'] = [best_stats[ii][0] for ii in range(len(df))]\n",
    "else:\n",
    "    df['answer'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42849ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.339208Z",
     "iopub.status.busy": "2024-04-19T19:10:01.338938Z",
     "iopub.status.idle": "2024-04-19T19:10:01.357676Z",
     "shell.execute_reply": "2024-04-19T19:10:01.356946Z"
    },
    "papermill": {
     "duration": 0.033809,
     "end_time": "2024-04-19T19:10:01.359714",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.325905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688db107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.385316Z",
     "iopub.status.busy": "2024-04-19T19:10:01.384909Z",
     "iopub.status.idle": "2024-04-19T19:10:01.440009Z",
     "shell.execute_reply": "2024-04-19T19:10:01.439186Z"
    },
    "papermill": {
     "duration": 0.070275,
     "end_time": "2024-04-19T19:10:01.442288",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.372013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not PRIVATE:\n",
    "    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "    if PRIVATE:\n",
    "        df['model_answer'] = [best_stats[ii][0] for ii in range(len(df))]\n",
    "        df['match'] = df.answer == df.model_answer\n",
    "        print(f'{df.match.sum()} matches in {len(df)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1044446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.470004Z",
     "iopub.status.busy": "2024-04-19T19:10:01.469706Z",
     "iopub.status.idle": "2024-04-19T19:10:01.479839Z",
     "shell.execute_reply": "2024-04-19T19:10:01.478969Z"
    },
    "papermill": {
     "duration": 0.025691,
     "end_time": "2024-04-19T19:10:01.481868",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.456177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229ee8</td>\n",
       "      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246d26</td>\n",
       "      <td>Each of the three-digits numbers $111$ to $999...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fc4ad</td>\n",
       "      <td>Let the `sparkle' operation on positive intege...</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430b63</td>\n",
       "      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5277ed</td>\n",
       "      <td>There exists a unique increasing geometric seq...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>739bc9</td>\n",
       "      <td>For how many positive integers $m$ does the eq...</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82e2a0</td>\n",
       "      <td>Suppose that we roll four 6-sided fair dice wi...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8ee6f3</td>\n",
       "      <td>The points $\\left(x, y\\right)$ satisfying $((\\...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bedda4</td>\n",
       "      <td>Let $ABCD$ be a unit square. Let $P$ be the po...</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d7e9c9</td>\n",
       "      <td>A function $f: \\mathbb N \\to \\mathbb N$ satisf...</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            problem  answer\n",
       "0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n",
       "1  246d26  Each of the three-digits numbers $111$ to $999...     250\n",
       "2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n",
       "3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n",
       "4  5277ed  There exists a unique increasing geometric seq...     211\n",
       "5  739bc9  For how many positive integers $m$ does the eq...     199\n",
       "6  82e2a0  Suppose that we roll four 6-sided fair dice wi...     185\n",
       "7  8ee6f3  The points $\\left(x, y\\right)$ satisfying $((\\...     320\n",
       "8  bedda4  Let $ABCD$ be a unit square. Let $P$ be the po...     480\n",
       "9  d7e9c9  A function $f: \\mathbb N \\to \\mathbb N$ satisf...     199"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9538850a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T19:10:01.507780Z",
     "iopub.status.busy": "2024-04-19T19:10:01.507494Z",
     "iopub.status.idle": "2024-04-19T19:10:01.625810Z",
     "shell.execute_reply": "2024-04-19T19:10:01.624700Z"
    },
    "papermill": {
     "duration": 0.133671,
     "end_time": "2024-04-19T19:10:01.628000",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.494329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('code.py', 'w') as fout:\n",
    "    fout.write(\"print('done')\")\n",
    "\n",
    "batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "try:\n",
    "    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "    print(shell_output)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d4926",
   "metadata": {
    "papermill": {
     "duration": 0.012144,
     "end_time": "2024-04-19T19:10:01.653166",
     "exception": false,
     "start_time": "2024-04-19T19:10:01.641022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8133715,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4281572,
     "sourceId": 7369493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4720595,
     "sourceId": 8012825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4748944,
     "sourceId": 8052555,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11261,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11264,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.013492,
   "end_time": "2024-04-19T19:10:05.193916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-19T19:09:28.180424",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "21267b653022419eb6fc3f47aa4db8ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_926e7ccdad6440be85c76931860b744c",
       "placeholder": "​",
       "style": "IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "2144e851698b4707ad1c7fc29fe21b03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3963993becfa487c9ff725f211915e67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065",
       "placeholder": "​",
       "style": "IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca",
       "value": " 19/19 [10:48&lt;00:00, 33.24s/it]"
      }
     },
     "5882b6e860be4a0db012a64fc0704a3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed",
        "IPY_MODEL_d91eb83d016a4381828192a98f798f9b",
        "IPY_MODEL_3963993becfa487c9ff725f211915e67"
       ],
       "layout": "IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"
      }
     },
     "6a892a5561f742bb9db9f13859c18e90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "926e7ccdad6440be85c76931860b744c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d91eb83d016a4381828192a98f798f9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03",
       "max": 19,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e0693b32889c42b18b9a3844e045d048",
       "value": 19
      }
     },
     "e0693b32889c42b18b9a3844e045d048": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f7a725e1b0cc4ad78a62beab5f663065": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fdb32baaed7145d8a8024b615ef242ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "feef8334edb24f6da22e8bb1d8d80c67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
