{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../Data/OlympiadBench_Dataset/data/outputs.json','../Data/AMC/outputs.json','../Data/MATH/outputs.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        # Load the list from the JSON file\n",
    "        texts.extend(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tokenizer.batch_encode_plus(texts,return_attention_mask=False,add_special_tokens=True,\\\n",
    "                                    truncation=True, max_length=4096)['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import math\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    # AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b31a71266bb43f28b84fa66a2bcefed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "# config.gradient_checkpointing = True\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704418576606868\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)\n",
    "# optimizer = torch.optim.SGD(trainable_params,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_consecutive_chunk(input_list, max_length):\n",
    "    if len(input_list) <= max_length:\n",
    "        return input_list\n",
    "    max_start_index = len(input_list) - max_length\n",
    "    start_index = random.randint(0, max_start_index)\n",
    "    out = input_list[start_index:start_index + max_length]\n",
    "    out[0] = input_list[0] # Start of sentence\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 1023: train loss 1.2246937110321596\n",
      "epoch 0 iter 2047: train loss 0.9911843656300334\n",
      "epoch 0 iter 3071: train loss 0.896547615135205\n",
      "epoch 0 iter 4095: train loss 0.8517464611795731\n",
      "epoch 0 iter 5119: train loss 0.8255010330030927\n",
      "epoch 0 iter 6143: train loss 0.810331098997267\n",
      "epoch 0 iter 7167: train loss 0.8075518999467022\n",
      "epoch 0 iter 8191: train loss 0.7850602741236798\n",
      "epoch 0 iter 9215: train loss 0.7778296660544584\n",
      "epoch 0 iter 10239: train loss 0.7870377459112206\n",
      "epoch 0 iter 11263: train loss 0.7926048189619905\n",
      "epoch 0 iter 12287: train loss 0.781508650710748\n",
      "epoch 0 iter 13311: train loss 0.7560314634429233\n",
      "epoch 0 iter 14335: train loss 0.7650179842312355\n",
      "epoch 0 iter 15359: train loss 0.7638273898228363\n",
      "epoch 0 iter 16383: train loss 0.7660701704662642\n",
      "epoch 0 iter 17407: train loss 0.7554997012521198\n",
      "epoch 0 iter 18431: train loss 0.7547400112307514\n",
      "epoch 0 iter 19455: train loss 0.7511080079639214\n",
      "epoch 0 iter 20479: train loss 0.7509563310086378\n",
      "epoch 0 iter 21503: train loss 0.7591618110636773\n",
      "epoch 0 iter 22527: train loss 0.7402847331541125\n",
      "epoch 0 iter 23551: train loss 0.7457584586518351\n",
      "epoch 0 iter 24575: train loss 0.7512675450598181\n",
      "epoch 0 iter 25599: train loss 0.7450960957248753\n",
      "epoch 0 iter 26623: train loss 0.7240328644402325\n",
      "end of epoch 0, loss: 0.8009260588913182\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(texts)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_last = 0\n",
    "    skip = 0\n",
    "    tot_skip = 0\n",
    "    # for llm, batchsize = 1 still gives 100 GPU util\n",
    "    for i,input_ids in enumerate(texts):\n",
    "        # train\n",
    "        input_ids = sample_consecutive_chunk(input_ids,1200)\n",
    "        input_ids = torch.tensor(input_ids).to('cuda')[None]\n",
    "        outs = model(input_ids).logits\n",
    "        if torch.any(torch.isnan(outs)):\n",
    "            skip += 1\n",
    "            continue\n",
    "        loss = loss_fn(outs[0,:-1],input_ids[0,1:])\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()):\n",
    "            skip += 1\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        # print(i,train_loss)\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # eval    \n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"epoch {epoch} iter {i}: train loss {(train_loss-train_last)/(verbose-skip)}\")\n",
    "            train_last = train_loss\n",
    "            tot_skip += skip\n",
    "            skip = 0            \n",
    "    print(f'end of epoch {epoch}, loss: {train_loss/(i-tot_skip)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"../Model/lora\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_model_id = \"../Model/lora\"\n",
    "# from peft import PeftModel\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM, \n",
    "# )\n",
    "# # model = ... load base model\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
