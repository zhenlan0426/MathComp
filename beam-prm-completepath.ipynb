{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec7fab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T19:29:22.700641Z",
     "iopub.status.busy": "2024-04-28T19:29:22.699779Z",
     "iopub.status.idle": "2024-04-28T19:29:22.704604Z",
     "shell.execute_reply": "2024-04-28T19:29:22.703795Z"
    },
    "papermill": {
     "duration": 0.015086,
     "end_time": "2024-04-28T19:29:22.706728",
     "exception": false,
     "start_time": "2024-04-28T19:29:22.691642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# credits:\n",
    "# https://www.kaggle.com/code/bsmit1659/aimo-vllm-accelerated-tot-sc-deepseekmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ececee60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T19:29:22.719161Z",
     "iopub.status.busy": "2024-04-28T19:29:22.718891Z",
     "iopub.status.idle": "2024-04-28T19:31:43.735736Z",
     "shell.execute_reply": "2024-04-28T19:31:43.734547Z"
    },
    "papermill": {
     "duration": 141.025607,
     "end_time": "2024-04-28T19:31:43.738185",
     "exception": false,
     "start_time": "2024-04-28T19:29:22.712578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pynvml<11.5,>=11.0.0, but you have pynvml 11.5.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch -q\n",
    "!pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n",
    "\n",
    "# keep data in float16 to avoid OOM\n",
    "file_path = '/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py'\n",
    "with open(file_path, 'r') as file:\n",
    "    file_contents = file.readlines()\n",
    "file_contents = [line for line in file_contents if \"logits = logits.float()\" not in line]\n",
    "with open(file_path, 'w') as file:\n",
    "    file.writelines(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c76091d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T19:31:43.751167Z",
     "iopub.status.busy": "2024-04-28T19:31:43.750845Z",
     "iopub.status.idle": "2024-04-28T19:34:57.989863Z",
     "shell.execute_reply": "2024-04-28T19:34:57.989078Z"
    },
    "papermill": {
     "duration": 194.248072,
     "end_time": "2024-04-28T19:34:57.991994",
     "exception": false,
     "start_time": "2024-04-28T19:31:43.743922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 19:31:49,928\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-28 19:31:51 config.py:767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-28 19:31:51 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n",
      "INFO 04-28 19:31:51 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/kaggle/input/deepseek-math', tokenizer='/kaggle/input/deepseek-math', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 19:31:53 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 04-28 19:31:53 selector.py:25] Using XFormers backend.\n",
      "INFO 04-28 19:33:19 model_runner.py:104] Loading model weights took 12.8725 GB\n",
      "INFO 04-28 19:33:21 gpu_executor.py:94] # GPU blocks: 177, # CPU blocks: 1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e75315900848a3bfb051a772f744e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed)\n",
    "import torch\n",
    "import math\n",
    "\n",
    "llm = LLM(model=\"/kaggle/input/deepseek-math\",\n",
    "          dtype='half',\n",
    "          enforce_eager=True,\n",
    "          gpu_memory_utilization=0.99,\n",
    "          swap_space=4,\n",
    "          max_model_len=2048,\n",
    "          kv_cache_dtype=\"fp8_e5m2\",\n",
    "          tensor_parallel_size=1)\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "good_token = '+'\n",
    "bad_token = '-'\n",
    "step_tag = 'ки'\n",
    "\n",
    "prm_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm')\n",
    "prm_candidate_tokens = prm_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "step_tag_id = prm_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "prm_model = AutoModelForCausalLM.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm',\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"balanced_low_0\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0332192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T19:34:58.008200Z",
     "iopub.status.busy": "2024-04-28T19:34:58.007714Z",
     "iopub.status.idle": "2024-04-28T19:34:58.027691Z",
     "shell.execute_reply": "2024-04-28T19:34:58.026778Z"
    },
    "papermill": {
     "duration": 0.030474,
     "end_time": "2024-04-28T19:34:58.029780",
     "exception": false,
     "start_time": "2024-04-28T19:34:57.999306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import aimo\n",
    "env = aimo.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a11e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T19:34:58.045570Z",
     "iopub.status.busy": "2024-04-28T19:34:58.045261Z",
     "iopub.status.idle": "2024-04-28T19:34:58.051874Z",
     "shell.execute_reply": "2024-04-28T19:34:58.050989Z"
    },
    "papermill": {
     "duration": 0.016886,
     "end_time": "2024-04-28T19:34:58.053863",
     "exception": false,
     "start_time": "2024-04-28T19:34:58.036977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_prm(candidates):\n",
    "    all_log_probs = []\n",
    "    for i in range(len(candidates)):\n",
    "        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda:1\")\n",
    "        with torch.no_grad():\n",
    "            logits = prm_model(input_ids).logits[:, :, prm_candidate_tokens] # b,l,C\n",
    "            scores = logits.softmax(dim=-1)[:,:,0][input_ids == 12902].min()\n",
    "            all_log_probs.append(scores.item())\n",
    "    return all_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be95fcaa",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-28T19:34:58.069623Z",
     "iopub.status.busy": "2024-04-28T19:34:58.069325Z",
     "iopub.status.idle": "2024-04-28T19:38:59.380175Z",
     "shell.execute_reply": "2024-04-28T19:38:59.379122Z"
    },
    "papermill": {
     "duration": 241.321526,
     "end_time": "2024-04-28T19:38:59.382670",
     "exception": false,
     "start_time": "2024-04-28T19:34:58.061144",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 35/35 [02:40<00:00,  4.59s/it]\n",
      "Processed prompts: 100%|██████████| 35/35 [00:27<00:00,  1.29it/s]\n",
      "Processed prompts: 100%|██████████| 35/35 [00:45<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n",
    "# stop_words.append(\"\\n\")\n",
    "\n",
    "sampling_params = SamplingParams(temperature=1,\n",
    "                                 max_tokens=2048,\n",
    "                                 min_tokens=32,\n",
    "                                 stop=stop_words)\n",
    "\n",
    "cot_instruction = \"\\nYou are an expert at mathematical reasoning. Please reason step by step, and put your final answer within \\\\boxed{}. The answer should be an interger between 0 and 999.\"\n",
    "\n",
    "\n",
    "n = 5 # beams\n",
    "samples = 35\n",
    "max_depth = 24\n",
    "overlap_threshold = 0.6\n",
    "all_prompts = []\n",
    "total_paths = []\n",
    "total_answers = []\n",
    "\n",
    "def is_integer(num):\n",
    "    if isinstance(num, float):\n",
    "        return num.is_integer()\n",
    "    elif isinstance(num, int):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_between_0_and_999(num):\n",
    "    return 0 <= num <= 999\n",
    "\n",
    "def prm_prompt(text, current_level):\n",
    "    return f\"Step {str(current_level)}:\" + text + ' ки'\n",
    "\n",
    "def process_string(long_string, threshold=10):\n",
    "    chunks = long_string.split(\"\\n\")\n",
    "    result = []\n",
    "    current_level = 1\n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > threshold:\n",
    "            # Apply the function to chunks with length greater than the threshold\n",
    "            processed_chunk = prm_prompt(chunk, current_level)\n",
    "            result.append(processed_chunk)\n",
    "            current_level += 1  # Increment the count for chunks above the threshold\n",
    "        else:\n",
    "            # Append the chunk as is if below the threshold\n",
    "            result.append(chunk)\n",
    "    # Join the processed chunks back into a single string if needed\n",
    "    return \"\\n\".join(result),current_level\n",
    "    \n",
    "import re\n",
    "def extract_number(text):\n",
    "    patterns = [\n",
    "        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n",
    "        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n",
    "        r\"The answer is[:\\s]*([0-9]+)\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return 'parse err'\n",
    "\n",
    "def parse_strict(text):\n",
    "    try:\n",
    "        text = extract_number(text.split('\\n')[-1])\n",
    "        if text == 'parse err':\n",
    "            return 'parse err'\n",
    "        else:\n",
    "            answer = eval(text)\n",
    "            if is_integer(answer) and is_between_0_and_999(answer):\n",
    "                return int(answer)\n",
    "            else:\n",
    "                return 'parse err'\n",
    "    except:\n",
    "        return 'parse err'\n",
    "\n",
    "def tot_agg(completed_paths):\n",
    "    # [(answer,score,current_level),...]\n",
    "    if completed_paths:\n",
    "        return max(completed_paths,key=lambda x:x[1]+x[2]**2*0.00108)[0]\n",
    "    else:\n",
    "        return 37 # empty completed_paths\n",
    "\n",
    "\n",
    "for test, sample_submission in iter_test:\n",
    "    problem = test['problem']\n",
    "\n",
    "    messages = [{\"role\": \"user\",\"content\": problem + cot_instruction}]\n",
    "    base_prompt = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "    batch_responses = llm.generate([base_prompt]*samples, sampling_params)\n",
    "    outs = []\n",
    "    inputs  = []\n",
    "    for o in batch_responses:\n",
    "        text = o.outputs[0].text\n",
    "        answer = parse_strict(text)\n",
    "        if answer != 'parse err':\n",
    "            text, level = process_string(text) # add special token for PRM\n",
    "            outs.append((answer,level))\n",
    "            inputs.append(text)\n",
    "    scores = eval_prm(inputs)\n",
    "    sample_submission['answer'] = tot_agg([(answer,score,level) for score,(answer,level) in zip(scores,outs)])\n",
    "\n",
    "    env.predict(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd4cb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T19:38:59.412750Z",
     "iopub.status.busy": "2024-04-28T19:38:59.412065Z",
     "iopub.status.idle": "2024-04-28T19:38:59.416209Z",
     "shell.execute_reply": "2024-04-28T19:38:59.415291Z"
    },
    "papermill": {
     "duration": 0.021147,
     "end_time": "2024-04-28T19:38:59.418283",
     "exception": false,
     "start_time": "2024-04-28T19:38:59.397136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total_paths\n",
    "# len(set(current_level_nodes)),len(current_level_nodes),len(set(prm_inputs)),len(prm_inputs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8077274,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4782935,
     "sourceId": 8099570,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11394,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 584.579468,
   "end_time": "2024-04-28T19:39:04.225137",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-28T19:29:19.645669",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "18e75315900848a3bfb051a772f744e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_89616bf70bb04165993ff5a0195c9abf",
        "IPY_MODEL_33dd00a2846e402abeecf93b763c9eac",
        "IPY_MODEL_cd91d1ae624c49d2bab1f22297f0e968"
       ],
       "layout": "IPY_MODEL_65c604efc9cf4e05889755ec86c5c4a3"
      }
     },
     "291594e253154d178f9104f775ed6cdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2a77166cf7ab4c1daabc7bd892a233e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "33dd00a2846e402abeecf93b763c9eac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_76b71bc8a86f4077b41c4c58d6cc5b2d",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2a77166cf7ab4c1daabc7bd892a233e6",
       "value": 2.0
      }
     },
     "65c604efc9cf4e05889755ec86c5c4a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76b71bc8a86f4077b41c4c58d6cc5b2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89616bf70bb04165993ff5a0195c9abf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bb5f2b5db8184afaa02618b61bc8b1eb",
       "placeholder": "​",
       "style": "IPY_MODEL_291594e253154d178f9104f775ed6cdf",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "bb5f2b5db8184afaa02618b61bc8b1eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd91d1ae624c49d2bab1f22297f0e968": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d4cc37aff9ad45749b258f210b8209bb",
       "placeholder": "​",
       "style": "IPY_MODEL_ee1659e6e4844918bdc910310abb7132",
       "value": " 2/2 [01:30&lt;00:00, 43.96s/it]"
      }
     },
     "d4cc37aff9ad45749b258f210b8209bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee1659e6e4844918bdc910310abb7132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
