{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:03:03.749818Z","iopub.status.busy":"2024-05-18T17:03:03.749074Z","iopub.status.idle":"2024-05-18T17:08:21.725719Z","shell.execute_reply":"2024-05-18T17:08:21.724832Z"},"papermill":{"duration":317.985165,"end_time":"2024-05-18T17:08:21.727692","exception":false,"start_time":"2024-05-18T17:03:03.742527","status":"completed"},"tags":[]},"outputs":[],"source":["from vllm import LLM, SamplingParams\n","LOCAL = True\n","from functions import *\n","dtype = 'auto'\n","gpu_memory_utilization = 0.95\n","\n","import torch\n","import pandas as pd\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","import subprocess\n","import sys"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["version = \"1\"\n","MODEL_PATH = f\"../Model/PRM_LORA{version}_merged_code_policy_01\" #_merged_code_policy_01SFT"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 06-05 17:25:40 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 06-05 17:25:40 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 06-05 17:25:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='../Model/PRM_LORA1_merged_code_policy_01', tokenizer='../Model/PRM_LORA1_merged_code_policy_01', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 06-05 17:25:41 selector.py:16] Using FlashAttention backend.\n","INFO 06-05 17:25:48 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 06-05 17:25:49 gpu_executor.py:94] # GPU blocks: 2314, # CPU blocks: 2184\n"]}],"source":["llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","\n","# stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n","stop_words = [tokenizer.eos_token,\"```output\",\"```Output\",\"```output\\n\",\"```Output\\n\",\"```\\nOutput\" , \")\\n```\" , \"``````output\",\"``````Output\"]\n","# stop_words.append(\"\\n\")\n","sampling_params = SamplingParams(temperature=1,\n","                                 max_tokens=256,\n","                                #  min_tokens=32,\n","                                 stop=stop_words,\n","                                 include_stop_str_in_output=True)\n","\n","\n","def gen_prompt_codeIn1(problem):\n","    return f\"\"\"Problem: {problem}\\n\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and your final answer should be integer, not expression, list, tuple or dictionary!\n","Write the entire script covering all the steps (use comments and document it well) and print the final result.\n","Approach:\"\"\"\n","\n","def gen_prompt_codeIn2(problem):\n","    return f\"\"\"Problem: {problem}\\n\n","You are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\"\"\"\n","\n","def gen_prompt3(problem):\n","    return '''Problem: \\n'''+problem+'''\\n\n","Carefully read and understand the problem and use all information in problem statement. No Python code. Show your work step-by-step, explain your reasoning, calculations, mathematical concepts and formulas in detail.\n","Write your final answer as a single integer in the last line of your response, enclosed within \\\\boxed{}.\n","'''\n","\n","n = 5 # beams\n","n_sol = 7\n","samples = 5\n","max_depth = 16\n","temperature = 0.5"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74251149d735445ead1d8cda688a4c49","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge1_code and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import LlamaForSequenceClassification\n","prm_tokenizer = tokenizer\n","prm_model = LlamaForSequenceClassification.from_pretrained(f'../Model/PRM_LORA_merge{version}_code',\\\n","                                                    num_labels=1,\\\n","                                                    device_map=\"cpu\",\n","                                                    torch_dtype=\"auto\",\n","                                                    ).eval()\n","base_model = prm_model.model\n","prm_model.score.load_state_dict(torch.load(f'../Model/model_score{version}_code.pth'))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import json\n","with open('../Data/AMC/aime_normal.json', 'r') as file:\n","    df = json.load(file)\n","# to have consistent format as in Kaggle\n","df = pd.DataFrame(df)\n","df.rename(columns={'question': 'problem'}, inplace=True)\n","df.final_answer = df.final_answer.apply(lambda x:int(x[0]))\n","df2 = pd.read_csv(\"../Data/ai-mathematical-olympiad-prize/train.csv\")\n","df2.rename(columns={'answer': 'final_answer'}, inplace=True)\n","df = pd.concat([df2,df[['problem','final_answer']]],axis=0)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        base_prompt1 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt_codeIn1(problem)}],tokenize=False)\n","        base_prompt2 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt_codeIn2(problem)}],tokenize=False)\n","        base_prompt3 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt3(problem)}],tokenize=False)\n","        outs.append(base_prompt1)\n","        outs.append(base_prompt2)\n","        outs.append(base_prompt3)\n","    return outs"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:08:23.262621Z","iopub.status.busy":"2024-05-18T17:08:23.262138Z","iopub.status.idle":"2024-05-18T17:08:23.268774Z","shell.execute_reply":"2024-05-18T17:08:23.267916Z"},"papermill":{"duration":0.016196,"end_time":"2024-05-18T17:08:23.270775","exception":false,"start_time":"2024-05-18T17:08:23.254579","status":"completed"},"tags":[]},"outputs":[],"source":["logit2prob = lambda x: 1/(1+np.exp(-x))\n","def eval_prm(candidates):\n","    all_log_probs = []\n","    for i in range(len(candidates)):\n","        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda\")\n","        with torch.no_grad():\n","            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n","            logits = prm_model.score(hidden_states)[0]\n","        all_log_probs.append(logit2prob(logits.item()))\n","    return all_log_probs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'[Tt]he answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"[Tt]he answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"[Tt]he answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def group_and_sum(A, B):\n","    '''\n","    A = ['a','b','a']\n","    B = [1,2,3]\n","    -> {'a': 4, 'b': 2}\n","    '''\n","    result_dict = {}\n","    for a, b in zip(A, B):\n","        if a in result_dict:\n","            result_dict[a] += b\n","        else:\n","            result_dict[a] = b\n","    return result_dict\n","\n","def group_and_average(A, B):\n","    from collections import defaultdict\n","    # Create a dictionary to store sums and counts for averaging\n","    sum_dict = defaultdict(lambda: [0, 0])  # Each key maps to [sum, count]\n","    # Pair elements from A and B and aggregate sums and counts\n","    for key, value in zip(A, B):\n","        sum_dict[key][0] += value\n","        sum_dict[key][1] += 1\n","    # Calculate averages\n","    averages = {key: sum_count[0] / sum_count[1] for key, sum_count in sum_dict.items()}\n","    return averages,[averages[a] for a in A]\n","\n","def max_dict(d):\n","    return max(d.items(), key=lambda x: x[1])[0]\n","\n","def tot_agg(completed_paths):\n","    if completed_paths:\n","        answers,scores = zip(*completed_paths)\n","        groups = group_and_sum(answers, scores)\n","        return max_dict(groups)\n","    else:\n","        return 37 # empty completed_paths\n","    \n","def repeat_elements(lst, k):\n","    return [i for i in lst for _ in range(k)]\n","\n","def flatten(nested_list):\n","    \"\"\"Flatten a nested list.\"\"\"\n","    out = []\n","    lengths = []\n","    for sublist in nested_list:\n","        lengths.append(len(sublist))\n","        for item in sublist:\n","            out.append(item)\n","    return out,lengths\n","\n","def unflatten(flat_list, lengths):\n","    \"\"\"Unflatten a flat list into a nested list based on lengths.\"\"\"\n","    nested_list = []\n","    index = 0\n","    for length in lengths:\n","        nested_list.append(flat_list[index:index + length])\n","        index += length\n","    return nested_list\n","\n","def filter_input(batch_response,current_level_node):\n","    # one question filter\n","    prm_inputs = []\n","    parents = []\n","    for candidate,parent in zip(batch_response,current_level_node):\n","        if candidate.outputs[0].text not in parent:\n","            prm_input = parent + candidate.outputs[0].text\n","            prm_inputs.append(prm_input)\n","            parents.append(parent)\n","    # Get the indices of unique elements in prm_inputs\n","    unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n","    prm_inputs = [prm_inputs[i] for i in unique_indices]\n","    parents = [parents[i] for i in unique_indices]\n","    return prm_inputs,parents,len(prm_inputs)\n","\n","def filter_inputs(batch_responses,current_level_nodes,lengths):\n","    # all question filter\n","    # returned value should be flattened\n","    batch_responses,current_level_nodes = unflatten(batch_responses,lengths),unflatten(current_level_nodes,lengths)\n","    prm_inputs = []\n","    lengths = []\n","    parent_list = []\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(batch_responses) == len(uncompleted)\n","    for batch_response,current_level_node,path in zip(batch_responses,current_level_nodes,uncompleted):\n","        prm_input,parents,length = filter_input(batch_response,current_level_node)\n","        if length == 0:# all bad\n","            while len(path)<n_sol:\n","                # make complete for this question as there will be no continued effort\n","                path.append(None)\n","        else:\n","            prm_inputs.extend(prm_input)\n","            parent_list.extend(parents)\n","            lengths.append(length)\n","    return prm_inputs,parent_list,lengths\n","\n","def HasAnswer(text):\n","    patterns = [\n","        r'answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return True\n","    return False\n","\n","def IsFinished(node):\n","    if \"No Python code.\" in node:\n","        return HasAnswer(node)\n","    else:\n","        matches = re.findall(r'print\\(([^)]*)\\)', node)\n","        return len(matches)>0\n","\n","def sample_k(items, probabilities, k):\n","    \"\"\"Samples k items without replacement from a list based on probabilities, with temperature scaling.\"\"\"\n","    # Temperature scaling\n","    scaled_probs = np.exp(np.array(probabilities)/temperature)\n","    normalized_probs = scaled_probs / np.sum(scaled_probs)  # Normalize scaled probs\n","\n","    # Sampling\n","    sampled_items = np.random.choice(\n","        items, size=k, replace=False, p=normalized_probs\n","    )\n","    return sampled_items\n","\n","def get_next_node(prm_inputs,prm_scores,completed_paths):\n","    # need to update completed_paths in-place\n","    next_level_nodes = []\n","    next_level_scores = []\n","    combined = list(zip(prm_inputs,prm_scores))    \n","    for node,score in combined:\n","        finish = IsFinished(node)\n","        if finish: # finished\n","            completed_paths.append((score,node))\n","        else: # not inished\n","            next_level_nodes.append(node)\n","            next_level_scores.append(score)\n","    if len(next_level_nodes) < n:\n","        return next_level_nodes\n","    next_level_nodes = sample_k(next_level_nodes, next_level_scores, n)\n","    return next_level_nodes\n","\n","\n","def get_next_nodes(prm_inputs,prm_scores,lengths):\n","    # for completed_paths, next_level_nodes would be removed\n","    # returned value should be flattened\n","    prm_inputs,prm_scores = unflatten(prm_inputs,lengths),unflatten(prm_scores,lengths)\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(uncompleted) == len(lengths)\n","    assert len(prm_inputs) == len(lengths)\n","    assert len(prm_scores) == len(lengths)\n","    next_level_nodes,lengths = [],[]\n","    for prm_input,prm_score,completed_path in zip(prm_inputs,prm_scores,uncompleted):\n","        next_node = get_next_node(prm_input,prm_score,completed_path)\n","        if len(completed_path) < n_sol:\n","            next_level_nodes.extend(next_node)\n","            lengths.append(len(next_node))\n","    return next_level_nodes,lengths\n","\n","import gc\n","def create_llm():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","    tokenizer = llm.get_tokenizer()\n","    return llm,tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts:   3%|▎         | 507/14775 [01:03<21:09, 11.24it/s]  "]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m current_level_nodes \u001b[38;5;241m=\u001b[39m repeat_elements(current_level_nodes,samples)\n\u001b[1;32m     18\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m*\u001b[39msamples \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m---> 19\u001b[0m batch_responses \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_level_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m prm_inputs,parent_list,lengths \u001b[38;5;241m=\u001b[39m filter_inputs(batch_responses,current_level_nodes,lengths)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# release VRAM to prm_model\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:190\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, multi_modal_data)\u001b[0m\n\u001b[1;32m    177\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    178\u001b[0m         i]\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(\n\u001b[1;32m    180\u001b[0m         prompt,\n\u001b[1;32m    181\u001b[0m         sampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:218\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    216\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 218\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py:676\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[0;32m--> 676\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:114\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    110\u001b[0m                   seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[1;32m    111\u001b[0m                   blocks_to_swap_in: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    112\u001b[0m                   blocks_to_swap_out: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    113\u001b[0m                   blocks_to_copy: Dict[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SamplerOutput:\n\u001b[0;32m--> 114\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/worker.py:221\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 221\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/model_runner.py:673\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:360\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    357\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    358\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 360\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:76\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m     73\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     80\u001b[0m     logprobs, sampling_metadata, sample_results)\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:502\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    497\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    498\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    499\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    500\u001b[0m     sampling_tensors: SamplingTensors,\n\u001b[1;32m    501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:401\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    399\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 401\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    404\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups, is_prompts,\n\u001b[1;32m    405\u001b[0m                                          sampling_metadata\u001b[38;5;241m.\u001b[39mseq_data,\n\u001b[1;32m    406\u001b[0m                                          beam_search_logprobs)\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:235\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, is_prompts, random_samples)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_random_sample\u001b[39m(\n\u001b[1;32m    230\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    231\u001b[0m     is_prompts: List[\u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    232\u001b[0m     random_samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    233\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    237\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# %debug\n","# [path for path in completed_paths if len(path)<n_sol] used to track on-going questiones\n","# flattened inputs, with lengths corresponds to the uncompleted path\n","# two ways for path to complete, one is via get_next_nodes getting n_sol answers\n","# two is via filter_inputs, all continuations are bad\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","current_level_nodes = process_inputs(df.problem.tolist())\n","lengths = [3] * df.shape[0]\n","current_level = 1\n","completed_paths = [[] for _ in range(df.shape[0])]\n","data_V = []\n","data_pi = []\n","\n","while (current_level < max_depth) and (current_level_nodes):\n","    # everything at this level is flattened\n","    current_level_nodes = repeat_elements(current_level_nodes,samples)\n","    lengths = [l*samples for l in lengths]\n","    batch_responses = llm.generate(current_level_nodes, sampling_params)\n","    prm_inputs,parent_list,lengths = filter_inputs(batch_responses,current_level_nodes,lengths)\n","    \n","    # release VRAM to prm_model\n","    del llm\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    prm_model.to('cuda')\n","    prm_scores = eval_prm(prm_inputs)\n","        \n","    # save for Q-learning\n","    averages,averages_dup = group_and_average(parent_list,prm_scores)\n","    data_V.extend(list(averages.items()))\n","    advantages = [q-v for q,v in zip(prm_scores,averages_dup)]\n","    data_pi.extend(list(zip(prm_inputs,advantages,[len(p) for p in parent_list]))) # pi(a|s) only train on action part\n","    \n","    # release VRAM to llm\n","    prm_model.to('cpu')\n","    llm,tokenizer = create_llm()\n","    \n","    current_level_nodes,lengths = get_next_nodes(prm_inputs,prm_scores,lengths)\n","    current_level += 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for score,code in completed_paths[4]:\n","#     print(f\"score:{score}\\n\\n\")\n","#     print(code)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import pickle\n","with open(f\"../llmOutputs/PRM/data_V1_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(data_V, f)\n","with open(f\"../llmOutputs/PRM/data_pi1_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(data_pi, f)    \n","with open(f\"../llmOutputs/PRM/completed_paths_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# only needed when restart kernel\n","version = \"5\"\n","import subprocess\n","import sys\n","import pickle\n","with open(f\"../llmOutputs/PRM/completed_paths_code{version}.pickle\", \"rb\") as f:\n","    completed_paths = pickle.load(f)\n","import json\n","import pandas as pd\n","with open('../Data/AMC/aime_normal.json', 'r') as file:\n","    df = json.load(file)\n","# to have consistent format as in Kaggle\n","df = pd.DataFrame(df)#.iloc[:24]\n","df.rename(columns={'question': 'problem'}, inplace=True)\n","\n","def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","\n","def delete_py_files(folder):\n","    # Use glob to find all .py files in the folder and subfolders\n","    py_files = glob.glob(os.path.join(folder, '**', '*.py'), recursive=True)\n","    # Iterate over the list of .py files and delete each one\n","    for file_path in py_files:\n","        try:\n","            os.remove(file_path)\n","        except Exception as e:\n","            print(f\"Error deleting {file_path}: {e}\")\n","# Example usage\n","folder_path = 'temp'\n","delete_py_files(folder_path)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<string>:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n","<string>:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n","<string>:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n"]}],"source":["from multiprocessing import Pool\n","from itertools import chain\n","\n","ys = df.final_answer.tolist()\n","def process_paths(args):\n","    paths, y, idx = args\n","    paths = [p for p in paths if p]\n","    out = [] # (isCorrect,score,node,code,prob_i,exit_i)\n","    for j,path in enumerate(paths):# path (score,node)\n","        input = path[1]\n","        if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n","            input += \")\"\n","        splits = input.split('```')\n","        if len(splits) < 2:\n","            out.append([0,path[0],path[1],'no code',idx,1])\n","            continue\n","        code = \"from sympy import *\\n\" + input.split('```')[1][7:] \n","        node = '```'.join(splits[:4]) # only return up to the first python code. later code/reason not relevant\n","        # execute code\n","        with open(f'temp/code_{idx}_{j}.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, f'temp/code_{idx}_{j}.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=7.1)\n","        except subprocess.TimeoutExpired:\n","            out.append([0,path[0],node,code,idx,2])\n","            with open(f'temp/2/code_{idx}_{j}.py', 'w') as fout:\n","                fout.write(code)\n","            continue\n","        if process.stderr:# code.py err\n","            out.append([0,path[0],node,code,idx,3])\n","            code = code + '\\n\\n\"\"\"' + process.stderr.decode('utf-8') + '\"\"\"'\n","            with open(f'temp/3/code_{idx}_{j}.py', 'w') as fout:\n","                fout.write(code)           \n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer):\n","                    out.append([int(int(answer)==y),path[0],node,code,idx,4])\n","                    with open(f'temp/4/code_{idx}_{j}.py', 'w') as fout:\n","                        fout.write(code)                     \n","                    continue\n","                else:\n","                    out.append([0,path[0],node,code,idx,5])\n","                    code = code + '\\n\\n\"\"\"' + stdout + '\"\"\"'\n","                    with open(f'temp/5/code_{idx}_{j}.py', 'w') as fout:\n","                        fout.write(code)          \n","                    continue\n","            except:\n","                out.append([0,path[0],node,code,idx,6])\n","                code = code + '\\n\\n\"\"\"' + stdout + '\"\"\"'\n","                with open(f'temp/6/code_{idx}_{j}.py', 'w') as fout:\n","                    fout.write(code)\n","                continue\n","    return out\n","\n","# Prepare arguments for multiprocessing\n","arguments = [(paths, y, idx) for idx, (paths, y) in enumerate(zip(completed_paths, ys))]\n","with Pool(processes=16) as pool:\n","    results = pool.map(process_paths, arguments)\n","\n","completed_paths_y = list(chain(*results))\n","with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths_y, f)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["0.9413988228918314"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# AUC for score and IsCorrect\n","import pandas as pd\n","data = pd.DataFrame(completed_paths_y,columns=['isCorrect','score','node','code','prob_i','exit_i'])\n","# !pip install scikit-learn\n","from sklearn.metrics import roc_auc_score\n","roc_auc_score(data.iloc[:,0].values,data.iloc[:,1].values)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["115.96231324492194"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data.iloc[:,0].mean() * 975 # all completion correct% * 975 questions"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["284"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data.groupby(['prob_i']).isCorrect.max().sum() # any completion correct #"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["1417"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data.isCorrect.sum()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["exit_i\n","4    5240\n","3    3742\n","5    1109\n","6     959\n","2     856\n","1       8\n","Name: count, dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data.exit_i.value_counts() #(exit_i == 4) - data.isCorrect.sum() -> code run but wrong results"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# import csv\n","# with open('training_log.csv', mode='w', newline='') as log_file:\n","#     log_writer = csv.writer(log_file)\n","#     log_writer.writerow(['version', 'auc', 'mean_acc', 'max_acc', 'value_counts'])\n","#     log_writer.writerow([\"5\", 0.9413988228918314, 115.96231324492194, 284, \" \"])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# import pandas as pd\n","# log_data = pd.read_csv('training_log.csv')\n","# _,_,best_mean_acc,best_max_acc,_ = log_data.iloc[-1].tolist()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# import re\n","# single_line_comment_pattern = re.compile(r'(?<!\\\\)#.*')\n","# multi_line_comment_pattern = re.compile(r'(\\'\\'\\'|\\\"\\\"\\\")(.*?)(\\'\\'\\'|\\\"\\\"\\\")', flags=re.DOTALL)\n","# trailing_whitespace_pattern = re.compile(r'[ \\t]+$', flags=re.MULTILINE)\n","# multiple_blank_lines_pattern = re.compile(r'\\n\\s*\\n')\n","# def remove_python_comments(code):\n","#     # Remove single-line comments\n","#     code = single_line_comment_pattern.sub('', code)\n","#     # Remove multi-line comments (docstrings)\n","#     code = multi_line_comment_pattern.sub('', code)\n","#     # Remove leading and trailing whitespace from each line\n","#     code = trailing_whitespace_pattern.sub('', code)\n","#     # Reduce multiple blank lines to a single blank line\n","#     code = multiple_blank_lines_pattern.sub('\\n', code)\n","#     return code\n","\n","# def repl(match):\n","#     if \"real\" not in match.group():\n","#         return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","#     else:\n","#         return \"{}{}\".format(match.group()[:-1], ')')\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# # calculate correct%\n","# timeout = 7\n","# len_limit = 49\n","\n","# def agg_code(paths):\n","#     paths = [p for p in paths if p]\n","#     paths.sort(key=lambda x: x[0], reverse=True)\n","#     for path in paths:# path (score,node)\n","#         input = path[1]\n","#         if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n","#             input += \")\"\n","#         splits = input.split('```')\n","#         if len(splits) < 2:\n","#             continue\n","#         code = \"from sympy import *\\n\" + input.split('```')[1][7:]\n","#         if len(code) < len_limit: continue # ignore very short answer\n","#         code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n","#         # execute code\n","#         with open('code.py', 'w') as fout:\n","#             fout.write(code)\n","#         # timeout err\n","#         try:\n","#             process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","#         except subprocess.TimeoutExpired:\n","#             continue\n","#         if process.stderr:# code.py err\n","#             continue\n","#         else:\n","#             stdout = process.stdout.decode('utf8')\n","#             try:\n","#                 answer = eval(stdout)\n","#                 if is_integer(answer) and is_between_0_and_999(answer):\n","#                     return int(answer)\n","#                 else:\n","#                     continue\n","#             except:\n","#                 continue\n","#     return 37"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<string>:1: SyntaxWarning: 'float' object is not subscriptable; perhaps you missed a comma?\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 4.25 s, sys: 8min 41s, total: 8min 45s\n","Wall time: 1h 11min 59s\n"]}],"source":["# %%time\n","# yhat = []\n","# for paths in completed_paths:\n","#     yhat.append(agg_code(paths))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["202"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# ys = df.final_answer.apply(lambda x:int(x[0])).tolist()\n","# sum([y==yh for y,yh in zip(ys,yhat)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<string>:1: SyntaxWarning: 'float' object is not subscriptable; perhaps you missed a comma?\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1.48 s, sys: 12.3 s, total: 13.8 s\n","Wall time: 51min 58s\n"]}],"source":["# %%time\n","# yhat = []\n","# for paths in completed_paths:\n","#     yhat.append(agg_code(paths))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["199"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# ys = df.final_answer.apply(lambda x:int(x[0])).tolist()\n","# sum([y==yh for y,yh in zip(ys,yhat)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8300737,"sourceType":"datasetVersion"},{"datasetId":5036020,"sourceId":8450555,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":571.271793,"end_time":"2024-05-18T17:09:54.761042","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-18T17:00:23.489249","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2770dfd3b1aa4c489056caf28ff0eb19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d51b8465cf24584bcefbd8de22eb8b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2770dfd3b1aa4c489056caf28ff0eb19","placeholder":"​","style":"IPY_MODEL_71347b6fde884c478ad4c29e00602bd8","value":" 3/3 [03:12&lt;00:00, 60.64s/it]"}},"39821c375ddf49ea9e6efb9c09fc0542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a13122cbc4884a7ba1e4a7bf9c6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167f1d4ccc44aee846ccde523d2cc9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7cd8d4fb35847f08de65bd2e42ceffe","IPY_MODEL_7459d3b6efa74b03ac047eac61c9487f","IPY_MODEL_2d51b8465cf24584bcefbd8de22eb8b1"],"layout":"IPY_MODEL_412a13122cbc4884a7ba1e4a7bf9c6cf"}},"71347b6fde884c478ad4c29e00602bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7459d3b6efa74b03ac047eac61c9487f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f5fe4bfeb44f1e84c533bc8b8f2da8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39821c375ddf49ea9e6efb9c09fc0542","value":3}},"78f5fe4bfeb44f1e84c533bc8b8f2da8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7cd8d4fb35847f08de65bd2e42ceffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e209cca0c4874c2bb25ebb79bd84ae28","placeholder":"​","style":"IPY_MODEL_ce7ea779817a4237af29446e91448882","value":"Loading checkpoint shards: 100%"}},"ce7ea779817a4237af29446e91448882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e209cca0c4874c2bb25ebb79bd84ae28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
