{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T21:28:33.236687Z","iopub.status.busy":"2024-04-03T21:28:33.235888Z","iopub.status.idle":"2024-04-03T21:30:40.885067Z","shell.execute_reply":"2024-04-03T21:30:40.884103Z"},"papermill":{"duration":127.660006,"end_time":"2024-04-03T21:30:40.887589","exception":false,"start_time":"2024-04-03T21:28:33.227583","status":"completed"},"tags":[]},"outputs":[],"source":["try:\n","    import peft\n","    LOCAL = True\n","    MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","    from functions import create_next_model_folder, NoRepeatTokenLogitsProcessor,naive_parse,sentence_transformers\n","except:\n","    LOCAL = False\n","    MODEL_PATH = \"/kaggle/input/deepseek-math\"\n","    from functions_math import create_next_model_folder, NoRepeatTokenLogitsProcessor,naive_parse,sentence_transformers\n","    \n","\n","import torch\n","import gc\n","if not LOCAL:torch.backends.cuda.enable_mem_efficient_sdp(False)\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T21:30:40.903909Z","iopub.status.busy":"2024-04-03T21:30:40.903421Z","iopub.status.idle":"2024-04-03T21:30:42.260344Z","shell.execute_reply":"2024-04-03T21:30:42.259185Z"},"papermill":{"duration":1.367528,"end_time":"2024-04-03T21:30:42.262807","exception":false,"start_time":"2024-04-03T21:30:40.895279","status":"completed"},"tags":[]},"outputs":[],"source":["if LOCAL:\n","    import json\n","    with open('../Data/AMC/aime_normal.json', 'r') as file:\n","        data = json.load(file)\n","    # to have consistent format as in Kaggle\n","    data = pd.DataFrame(data)\n","    data.rename(columns={'question': 'problem'}, inplace=True)\n","else:\n","    data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n","    if len(data) < 5:\n","        data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","        PRIVATE = False\n","    else:\n","        PRIVATE = True"]},{"cell_type":"markdown","metadata":{},"source":["#### RAG"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import json\n","import numpy as np\n","from transformers import AutoModel, AutoTokenizer,AutoModelForCausalLM\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["if LOCAL:\n","    model_paths = model_names = [\"mixedbread-ai/mxbai-embed-large-v1\"]\n","    data_paths = ['context_combine3.json']\n","    FILE_PATH = '../Data/Embeddings'\n","else:\n","    # local path to Kaggle model folder\n","    model_paths = [\"/kaggle/input/westseverus-7b-dpo\"]\n","    # model name as in from HF source, needs to be of the same order as model_paths\n","    model_names = [\"mixedbread-ai/mxbai-embed-large-v1\"]\n","    data_paths = ['context_combine4.json']\n","    FILE_PATH = '/kaggle/input/embedding-db'\n","get_name = lambda x: x.split('/')[-1]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# List of str (query) -> List of list of str (context)\n","\n","# get embedding for query\n","query = data.problem.tolist()\n","contexts = []\n","query_list = []\n","for model_name in model_paths:\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModel.from_pretrained(model_name)\n","    sentence_model = sentence_transformers(model,tokenizer)\n","    query_list.append(sentence_model.encode(query,True))\n","\n","# get contexts for each query\n","for data_path in data_paths:\n","    with open(FILE_PATH + '/' + get_name(data_path) + 'context.json', 'r') as f:\n","        output_context = json.load(f)\n","    for query,model_name in zip(query_list,model_names):\n","        embeddings = np.load(FILE_PATH + '/' + model_name.split('/')[-1] + \"--\" + data_path +'.npy')\n","        index = cosine_similarity(embeddings,query).argmax(0)\n","        context = [output_context[i] for i in index]\n","        contexts.append(context)\n","\n","# contexts [[question1_model1, question1_model2,...],[question2_model1,...],...]\n","contexts = list(zip(*contexts))\n","\n","del model,tokenizer\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["#### Generation"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85e2d7680cd24ef48cbd6be536cd8838","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_PATH,\n","    device_map = \"auto\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code = True,\n","    use_flash_attention_2=LOCAL,\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-15T17:49:28.037006Z","iopub.status.busy":"2024-04-15T17:49:28.036610Z","iopub.status.idle":"2024-04-15T17:49:28.044857Z","shell.execute_reply":"2024-04-15T17:49:28.043493Z","shell.execute_reply.started":"2024-04-15T17:49:28.036978Z"},"papermill":{"duration":0.015534,"end_time":"2024-04-03T21:30:42.325795","exception":false,"start_time":"2024-04-03T21:30:42.310261","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def gen_prompt(problem):\n","    \n","    return f\"\"\"\n","### Instruction:\\n{problem}\\n\\n\n","### Response: Let's think step by step. The final response should be a integer between 0 and 999 in the last line of your response.\n","\"\"\"\n","from collections import Counter\n","def aggregate(answers):\n","    pred = Counter(answers).most_common(2)\n","    if len(pred) == 1:\n","        if pred[0][0] == \"parsing error\":\n","            return 37\n","        else:\n","            return pred[0][0]\n","    else:\n","        return pred[1][0] if pred[0][0] == \"parsing error\" else pred[0][0]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (5214 > 4096). Running this sequence through the model will result in indexing errors\n","This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"]}],"source":["outs = []\n","no_repeat_processor = [NoRepeatTokenLogitsProcessor()]\n","for context, (index, row) in zip(contexts,data.iterrows()):\n","    answers = []\n","    problem = row['problem']\n","    query_prompt = gen_prompt(problem)\n","    if LOCAL: monitor = [problem,int(row['final_answer'][0])] # LOCAL only\n","    for c in context:\n","        # problem, context -> answer #\n","        messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": c + \"\\n\" + query_prompt\n","            }\n","        ]\n","\n","        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n","        with torch.no_grad():\n","            encoded_output = model.generate(inputs, max_new_tokens=1500, do_sample=False, pad_token_id=tokenizer.eos_token_id,\\\n","                                            logits_processor=no_repeat_processor)\n","\n","        decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True).replace(c + \"\\n\" + query_prompt, '')\n","        try:\n","            answer = decoded_output.split('\\n')[-1]\n","            answer = naive_parse(answer) % 1000\n","        except:\n","            answer = 'parsing error'\n","        # End problem, context -> answer #\n","        answers.append(answer)\n","        if LOCAL:\n","            monitor.append(c)\n","            monitor.append(decoded_output)\n","            monitor.append(answer)\n","        elif not PRIVATE:\n","            print(f\"\\nContext: {c}\\n\")\n","            print(decoded_output)\n","\n","    final_answer = aggregate(answers)\n","    if LOCAL:\n","        monitor.append(final_answer)\n","        outs.append(monitor)\n","    else:\n","        outs.append(final_answer)\n","        torch.cuda.empty_cache()\n","        gc.collect()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["if LOCAL:\n","    outs_df = pd.DataFrame(outs,columns=['problem','y'] + ['context','output','yhat_i']*len(context) + ['yhat'])\n","    print(sum(outs_df.yhat == outs_df.y))\n","    out_path = create_next_model_folder('../llmOutputs')\n","    print(out_path)\n","    outs_df.to_csv(out_path+'/outputs.csv', header=True, index=False)\n","    # 49\n","    # ../llmOutputs/model2\n","else:\n","    if not PRIVATE:\n","        answers = data.answer.tolist()\n","        correct = sum([y==yhat for y,yhat in zip(answers,outs)])\n","        print(f'{correct} correct answers')    \n","    data['answer'] = outs\n","    data[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8133715,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"sourceId":172008556,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":645.368853,"end_time":"2024-04-03T21:38:35.367254","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-03T21:27:49.998401","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"08213ff2f3fe4dc58310902a0e714fc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0063cf432324329b3ec56ac67170190","placeholder":"​","style":"IPY_MODEL_1c4f80ae8e4c4075ab1c0253b8bfe72b","value":" 3/3 [01:51&lt;00:00, 36.32s/it]"}},"1c4f80ae8e4c4075ab1c0253b8bfe72b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608f103f10f7493388c58f1022606b5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"698b2618c6cb44919194413c2013fa69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91954822b9664cfabc85f9bd4ca2a82a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_698b2618c6cb44919194413c2013fa69","placeholder":"​","style":"IPY_MODEL_608f103f10f7493388c58f1022606b5c","value":"Loading checkpoint shards: 100%"}},"968fcd0a8c76415cb8b0910b41f82633":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c824dd8e4ee14f53889e4e089ae2fa27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6cee2682b74df7bbe38f0d3f105e1b","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_968fcd0a8c76415cb8b0910b41f82633","value":3}},"d0063cf432324329b3ec56ac67170190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e841af56321d48eaad41c823c88647a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91954822b9664cfabc85f9bd4ca2a82a","IPY_MODEL_c824dd8e4ee14f53889e4e089ae2fa27","IPY_MODEL_08213ff2f3fe4dc58310902a0e714fc7"],"layout":"IPY_MODEL_fa01e68ebc80458bbe20fdac92c8285e"}},"ef6cee2682b74df7bbe38f0d3f105e1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa01e68ebc80458bbe20fdac92c8285e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
