{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T21:28:33.236687Z","iopub.status.busy":"2024-04-03T21:28:33.235888Z","iopub.status.idle":"2024-04-03T21:30:40.885067Z","shell.execute_reply":"2024-04-03T21:30:40.884103Z"},"papermill":{"duration":127.660006,"end_time":"2024-04-03T21:30:40.887589","exception":false,"start_time":"2024-04-03T21:28:33.227583","status":"completed"},"tags":[]},"outputs":[],"source":["try:\n","    from vllm import LLM, SamplingParams\n","    LOCAL = True\n","    MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","    from functions import *\n","    dtype = 'auto'\n","    gpu_memory_utilization = 0.95\n","\n","except:\n","    %pip uninstall -y torch -q\n","    %pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n","    from vllm import LLM, SamplingParams\n","    LOCAL = False\n","    MODEL_PATH = \"/kaggle/input/deepseek-math\"\n","    dtype = 'half'\n","    gpu_memory_utilization = 0.99\n","    from functions_math import *\n","    import gc\n","\n","import torch\n","import pandas as pd\n","import subprocess\n","import sys\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 04-20 19:14:04 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 04-20 19:14:04 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 04-20 19:14:04 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n","WARNING 04-20 19:14:04 config.py:406] Possibly too large swap space. 40.00 GiB out of the 94.01 GiB total CPU memory is allocated for the swap space.\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 04-20 19:14:04 selector.py:16] Using FlashAttention backend.\n","INFO 04-20 19:14:05 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 04-20 19:14:11 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 04-20 19:14:11 gpu_executor.py:94] # GPU blocks: 2323, # CPU blocks: 10922\n"]}],"source":["iterations = 3\n","repeats = 7\n","temperature = 0.9\n","max_tokens = 1500\n","llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=40,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","stop_words = [tokenizer.eos_token]\n","sampling_params = {'n': repeats,\n","                    'best_of': repeats,\n","                    'temperature': temperature,\n","                    'max_tokens': max_tokens,\n","                    'stop': stop_words\n","                }"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T21:30:40.903909Z","iopub.status.busy":"2024-04-03T21:30:40.903421Z","iopub.status.idle":"2024-04-03T21:30:42.260344Z","shell.execute_reply":"2024-04-03T21:30:42.259185Z"},"papermill":{"duration":1.367528,"end_time":"2024-04-03T21:30:42.262807","exception":false,"start_time":"2024-04-03T21:30:40.895279","status":"completed"},"tags":[]},"outputs":[],"source":["if LOCAL:\n","    import json\n","    with open('../Data/AMC/aime_normal.json', 'r') as file:\n","        data = json.load(file)\n","    # to have consistent format as in Kaggle\n","    data = pd.DataFrame(data)\n","    data.rename(columns={'question': 'problem'}, inplace=True)\n","else:\n","    data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n","    if len(data) < 5:\n","        data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","        PRIVATE = False\n","    else:\n","        PRIVATE = True"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","def gen_prompt_codeIn1(problem):\n","    return f\"\"\"\n","### Problem:\\n{problem}\\n\n","### Response: Let's first reason step by step. Then write Python code to solve the problem, using brute force enumeration if necessary. \n","Limit the search space using logical deductions. The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","The final answer is an integer between 0 and 999.\n","\"\"\"\n","\n","def parse_err(input):\n","    return f\"\"\" \\n{input}\\n\n","    The code should be enclosed between ```python\\n actual code...``` Please fix.\n","\"\"\"\n","\n","def code_err(code, err):\n","    return f\"\"\" \\n{code}\\n\n","    The code encounters this error: {err}\\n.\n","    Please fix.\n","\"\"\"\n","\n","def eval_err(code):\n","    return f\"\"\" \\n{code}\\n\n","    code should only print the final number.\n","\"\"\"\n","\n","def number_range_type_err(problem,input,answer):\n","    return f\"\"\" \\nThis is the original problem: {problem}\\n \n","    \\nThis is the first solution: {input}\\n\n","    However, answer should be a integer between 0 and 999 but got {str(answer)}. Please revisit the logics and\n","    generate python code to answer the question.\n","    The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","\"\"\"    \n","def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        query_prompt = gen_prompt_codeIn1(problem)\n","        messages = [{\"role\": \"user\",\"content\": query_prompt}]\n","        input = tokenizer.apply_chat_template(messages, tokenize=False)\n","        outs.append(input)\n","    return outs\n","\n","from functools import partial\n","def get_value(output,normalize):\n","    # cumulative_logprob are negative. factor of 10 penalize it.\n","    text = output.text\n","    if \"```python\" not in text and text.count(\"print(\") != 1:\n","        factor = 10\n","    else:\n","        factor = 1\n","    if normalize:\n","        return factor*output.cumulative_logprob/len(output.token_ids)\n","    else:\n","        return factor*output.cumulative_logprob\n","\n","def process_code(inputs,problems,normalize=True):\n","    # inputs,problem is a list of str\n","    outs = []\n","    for input,problem in zip(inputs,problems):\n","        # input is vllm.outputs.RequestOutput\n","        input = max(input.outputs,key=partial(get_value,normalize=normalize)).text\n","        # parse code\n","        try:\n","            code = input.split('```')[1][7:]\n","        except: \n","            outs.append(parse_err(input))\n","            continue\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        batcmd = 'timeout 21 ' + sys.executable + ' code.py'\n","        process = subprocess.run(batcmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","        if process.stderr:# code.py err\n","            stderr = process.stderr.decode('utf8')\n","            outs.append(code_err(code, stderr))\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer) and is_between_0_and_999(answer):\n","                    outs.append(int(answer))\n","                else:\n","                    outs.append(number_range_type_err(problem,input,answer))\n","            except:\n","                outs.append(eval_err(code))\n","            \n","    return outs"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts:   5%|â–Œ         | 51/975 [02:12<33:57,  2.21s/it] "]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m     10\u001b[0m     sampling_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sampling_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m capacity\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m---> 11\u001b[0m     raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSamplingParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     outs \u001b[38;5;241m=\u001b[39m process_code(raw_outputs,problems)\n\u001b[1;32m     13\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [o \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o,\u001b[38;5;28mstr\u001b[39m)] \u001b[38;5;66;03m# invalid answers\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:190\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, multi_modal_data)\u001b[0m\n\u001b[1;32m    177\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    178\u001b[0m         i]\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(\n\u001b[1;32m    180\u001b[0m         prompt,\n\u001b[1;32m    181\u001b[0m         sampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:218\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    216\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 218\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py:676\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[0;32m--> 676\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:114\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    110\u001b[0m                   seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[1;32m    111\u001b[0m                   blocks_to_swap_in: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    112\u001b[0m                   blocks_to_swap_out: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    113\u001b[0m                   blocks_to_copy: Dict[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SamplerOutput:\n\u001b[0;32m--> 114\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/worker.py:221\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 221\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/model_runner.py:673\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:360\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    357\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    358\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 360\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:76\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m     73\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     80\u001b[0m     logprobs, sampling_metadata, sample_results)\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:502\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    497\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    498\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    499\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    500\u001b[0m     sampling_tensors: SamplingTensors,\n\u001b[1;32m    501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:401\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    399\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 401\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    404\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups, is_prompts,\n\u001b[1;32m    405\u001b[0m                                          sampling_metadata\u001b[38;5;241m.\u001b[39mseq_data,\n\u001b[1;32m    406\u001b[0m                                          beam_search_logprobs)\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/sampler.py:235\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, is_prompts, random_samples)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_random_sample\u001b[39m(\n\u001b[1;32m    230\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    231\u001b[0m     is_prompts: List[\u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    232\u001b[0m     random_samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    233\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    237\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# prepare inputs\n","problems = data.problem.tolist()\n","inputs = process_inputs(problems)\n","capacity = len(inputs) * sampling_params['best_of']\n","\n","# generation\n","final_answers = []\n","if LOCAL: monitors = []\n","for _ in range(iterations):\n","    sampling_params['n'] = sampling_params['best_of'] = capacity//len(inputs)\n","    raw_outputs = llm.generate(inputs, SamplingParams(**sampling_params))\n","    outs = process_code(raw_outputs,problems)\n","    inputs = [o for o in outs if isinstance(o,str)] # invalid answers\n","    final_answers.append(outs)\n","    if LOCAL: monitors.append(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if LOCAL:\n","    outs_df = pd.DataFrame(outs,columns=['problem','output','yhat','y'])\n","    print(f\"correct: {sum(outs_df.yhat == outs_df.y)}\")\n","    print(f\"parse error: {sum(outs_df.yhat =='parsing error')}\")\n","    out_path = create_next_model_folder('../llmOutputs')\n","    print(out_path) # ../llmOutputs/model1\n","    outs_df.to_csv(out_path+'/generations.csv', header=True, index=False)\n","else:\n","    if not PRIVATE:\n","        answers = data.answer.tolist()\n","        correct = sum([y==yhat for y,yhat in zip(answers,outs)])\n","        print(f'{correct} correct answers')    \n","    data['answer'] = outs\n","    data[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8133715,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8077274,"sourceType":"datasetVersion"},{"sourceId":172559828,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":645.368853,"end_time":"2024-04-03T21:38:35.367254","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-03T21:27:49.998401","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"08213ff2f3fe4dc58310902a0e714fc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0063cf432324329b3ec56ac67170190","placeholder":"â€‹","style":"IPY_MODEL_1c4f80ae8e4c4075ab1c0253b8bfe72b","value":" 3/3 [01:51&lt;00:00, 36.32s/it]"}},"1c4f80ae8e4c4075ab1c0253b8bfe72b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608f103f10f7493388c58f1022606b5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"698b2618c6cb44919194413c2013fa69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91954822b9664cfabc85f9bd4ca2a82a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_698b2618c6cb44919194413c2013fa69","placeholder":"â€‹","style":"IPY_MODEL_608f103f10f7493388c58f1022606b5c","value":"Loading checkpoint shards: 100%"}},"968fcd0a8c76415cb8b0910b41f82633":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c824dd8e4ee14f53889e4e089ae2fa27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6cee2682b74df7bbe38f0d3f105e1b","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_968fcd0a8c76415cb8b0910b41f82633","value":3}},"d0063cf432324329b3ec56ac67170190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e841af56321d48eaad41c823c88647a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91954822b9664cfabc85f9bd4ca2a82a","IPY_MODEL_c824dd8e4ee14f53889e4e089ae2fa27","IPY_MODEL_08213ff2f3fe4dc58310902a0e714fc7"],"layout":"IPY_MODEL_fa01e68ebc80458bbe20fdac92c8285e"}},"ef6cee2682b74df7bbe38f0d3f105e1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa01e68ebc80458bbe20fdac92c8285e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
