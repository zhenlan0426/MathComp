{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T20:53:04.564761Z","iopub.status.busy":"2024-04-22T20:53:04.564150Z","iopub.status.idle":"2024-04-22T20:55:35.414245Z","shell.execute_reply":"2024-04-22T20:55:35.413371Z","shell.execute_reply.started":"2024-04-22T20:53:04.564734Z"},"papermill":{"duration":127.660006,"end_time":"2024-04-03T21:30:40.887589","exception":false,"start_time":"2024-04-03T21:28:33.227583","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["try:\n","    from vllm import LLM, SamplingParams\n","    LOCAL = True\n","    MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","    from functions import *\n","    dtype = 'auto'\n","    gpu_memory_utilization = 0.95\n","\n","except:\n","    %pip uninstall -y torch -q\n","    %pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n","    from vllm import LLM, SamplingParams\n","    LOCAL = False\n","    MODEL_PATH = \"/kaggle/input/deepseek-math\"\n","    dtype = 'half'\n","    gpu_memory_utilization = 0.99\n","    from functions_math import *\n","\n","\n","import torch\n","import pandas as pd\n","import subprocess\n","import sys\n","import gc\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T20:55:35.416458Z","iopub.status.busy":"2024-04-22T20:55:35.415992Z","iopub.status.idle":"2024-04-22T20:57:51.080132Z","shell.execute_reply":"2024-04-22T20:57:51.079150Z","shell.execute_reply.started":"2024-04-22T20:55:35.416429Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 04-23 14:03:41 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 04-23 14:03:41 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 04-23 14:03:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 04-23 14:03:42 selector.py:16] Using FlashAttention backend.\n","INFO 04-23 14:03:42 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 04-23 14:03:44 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 04-23 14:03:44 gpu_executor.py:94] # GPU blocks: 2309, # CPU blocks: 2184\n"]}],"source":["iterations = 3\n","repeats = 6\n","timeout = 7\n","temperature = 0.7\n","max_tokens = 1600\n","normalize = True\n","llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","stop_words = [tokenizer.eos_token]\n","sampling_params = SamplingParams(n = 1, best_of= 1,\n","                                 temperature=temperature,\n","                                 max_tokens=max_tokens,\n","                                 stop=stop_words)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T20:57:51.081718Z","iopub.status.busy":"2024-04-22T20:57:51.081256Z","iopub.status.idle":"2024-04-22T20:57:51.113778Z","shell.execute_reply":"2024-04-22T20:57:51.112859Z","shell.execute_reply.started":"2024-04-22T20:57:51.081691Z"},"papermill":{"duration":1.367528,"end_time":"2024-04-03T21:30:42.262807","exception":false,"start_time":"2024-04-03T21:30:40.895279","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if LOCAL:\n","    import json\n","    with open('../Data/AMC/aime_normal.json', 'r') as file:\n","        data = json.load(file)\n","    # to have consistent format as in Kaggle\n","    data = pd.DataFrame(data)\n","    data.rename(columns={'question': 'problem'}, inplace=True)\n","else:\n","    data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n","    if len(data) < 5:\n","        data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","        PRIVATE = False\n","    else:\n","        PRIVATE = True"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T20:57:51.116350Z","iopub.status.busy":"2024-04-22T20:57:51.116014Z","iopub.status.idle":"2024-04-22T20:57:51.144349Z","shell.execute_reply":"2024-04-22T20:57:51.143082Z","shell.execute_reply.started":"2024-04-22T20:57:51.116320Z"},"trusted":true},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","def gen_prompt_codeIn1(problem):\n","    return f\"\"\"\n","### Problem:\\n{problem}\\n\n","### Response: Let's first reason step by step. Then write Python code to solve the problem, using brute force enumeration if necessary. \n","Limit the search space using logical deductions. The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","The final answer is an integer between 0 and 999.\n","\"\"\"\n","\n","def parse_err(input):\n","    return f\"\"\" \\n{input}\\n\n","    The code should be enclosed between ```python\\n actual code...``` Please fix.\n","\"\"\"\n","\n","def time_err(code):\n","    return f\"\"\" \\n{code}\\n\n","    code timed out. Try to use math reasoning to limit the search space and find ways to exit loop early where appropriate.\n","\"\"\"\n","\n","def code_err(code, err):\n","    return f\"\"\" \\n{code}\\n\n","    The code encounters this error: {err}\\n.\n","    Please fix.\n","\"\"\"\n","\n","def eval_err(code):\n","    return f\"\"\" \\n{code}\\n\n","    code should only print the final number.\n","\"\"\"\n","\n","def number_range_type_err(problem,input,answer):\n","    return f\"\"\" \\nThis is the original problem: {problem}\\n \n","    \\nThis is the first solution: {input}\\n\n","    However, answer should be a integer between 0 and 999 but got {str(answer)}. Please revisit the logics and\n","    generate python code to answer the question.\n","    The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","\"\"\"    \n","def process_inputs(inputs,prompt_fun):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        query_prompt = prompt_fun(problem)\n","        messages = [{\"role\": \"user\",\"content\": query_prompt}]\n","        input = tokenizer.apply_chat_template(messages, tokenize=False)\n","        outs.append(input)\n","    return outs\n","\n","from functools import partial\n","def get_value_code(output,normalize):\n","    # cumulative_logprob are negative. factor of 10 penalize it.\n","    output = output.outputs[0] # outpus[i] is the i-th sample with attr cumulative_logprob,text,token_ids\n","    if len(output.token_ids) == 0: return float('-inf')\n","    text = output.text\n","    if \"```python\" not in text and text.count(\"print(\") != 1: return float('-inf')\n","    if normalize:\n","        return output.cumulative_logprob/len(output.token_ids)\n","    else:\n","        return output.cumulative_logprob\n","    \n","def process_code(inputs,problems,repeats,normalize):\n","    inputs = group_elements(inputs, repeats) # [[out1_re1, out1_re2,...],[out2_re1,out2_re2...],...]\n","    outs = []\n","    for input,problem in zip(inputs,problems):\n","        # input is a list of vllm.outputs.RequestOutput\n","        input = max(input,key=partial(get_value_code,normalize=normalize)).outputs[0].text\n","        # parse err\n","        try:\n","            code = input.split('```')[1][7:]\n","        except: \n","            outs.append(parse_err(input))\n","            continue\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","        except subprocess.TimeoutExpired:\n","            outs.append(time_err(code))\n","            continue\n","        if process.stderr:# code.py err\n","            stderr = process.stderr.decode('utf8')\n","            outs.append(code_err(code, stderr))\n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer) and is_between_0_and_999(answer):\n","                    outs.append(int(answer))\n","                    continue\n","                else:\n","                    outs.append(number_range_type_err(problem,input,answer))\n","                    continue\n","            except:\n","                outs.append(eval_err(code))\n","                continue\n","    return outs\n","\n","def repeat_elements(lst, k):\n","    return [i for i in lst for _ in range(k)]\n","\n","def group_elements(lst, k):\n","    return [lst[i:i+k] for i in range(0, len(lst), k)]\n","\n","def gen_prompt_pure(problem):\n","    return '''\n","### Problem:\\n'''+problem+'''\\n\n","### Response: Let's think step by step and do not use Python code. The final answer should be a single integer in the last line of your response. The integer should be between 0 and 999.\n","the answer should be enclosed within \\\\boxed{}.\n","'''\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"The answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def parse_strict(text):\n","    try:\n","        text = extract_number(text.split('\\n')[-1])\n","        if text == 'parse err':\n","            return 'parse err'\n","        else:\n","            answer = eval(text)\n","            if is_integer(answer) and is_between_0_and_999(answer):\n","                return int(answer)\n","            else:\n","                return 'parse err'\n","    except:\n","        return 'parse err'\n","\n","def group_and_sum(A, B):\n","    result_dict = {}\n","    for a, b in zip(A, B):\n","        if a in result_dict:\n","            result_dict[a] += b\n","        else:\n","            result_dict[a] = b\n","    return list(result_dict.items())\n","\n","from math import exp\n","def get_max(input,normalize):\n","    # input is a list of vllm.outputs.RequestOutput\n","    # return the best one from \"repeats\" samples\n","    # use group by to reward repeated answer\n","    texts = []\n","    scores = []\n","    for o in input:\n","        o = o.outputs[0]\n","        answer = parse_strict(o.text)\n","        if answer != 'parse err' and len(o.token_ids)>0:\n","            texts.append(answer)\n","            scores.append(exp(o.cumulative_logprob/len(o.token_ids) if normalize else o.cumulative_logprob)) # exp to ensure reward are positive\n","    if texts:\n","        groups = group_and_sum(texts,scores)\n","        return max(groups,key=lambda x:x[1])[0]\n","    else:\n","        return 'parse err'\n","    \n","def process_pure(inputs,repeats,normalize):\n","    inputs = group_elements(inputs, repeats) # [[out1_re1, out1_re2,...],[out2_re1,out2_re2...],...]\n","    outs = []\n","    for input in inputs:\n","        # input is a list of vllm.outputs.RequestOutput\n","        outs.append(get_max(input,normalize))\n","    return outs"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T20:57:51.145989Z","iopub.status.busy":"2024-04-22T20:57:51.145670Z","iopub.status.idle":"2024-04-22T20:59:10.994314Z","shell.execute_reply":"2024-04-22T20:59:10.993046Z","shell.execute_reply.started":"2024-04-22T20:57:51.145961Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 5850/5850 [46:16<00:00,  2.11it/s]  \n","Token indices sequence length is longer than the specified maximum sequence length for this model (70249 > 4096). Running this sequence through the model will result in indexing errors\n","Processed prompts:   9%|▊         | 482/5520 [04:09<1:05:36,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:03:32 scheduler.py:245] Input prompt (2058 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  20%|█▉        | 1088/5520 [09:15<56:22,  1.31it/s]  "]},{"name":"stdout","output_type":"stream","text":["WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:08:39 scheduler.py:245] Input prompt (3170 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  26%|██▌       | 1410/5520 [12:04<21:50,  3.14it/s]  "]},{"name":"stdout","output_type":"stream","text":["WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:11:28 scheduler.py:245] Input prompt (2428 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  44%|████▎     | 2411/5520 [20:44<12:54,  4.01it/s]  "]},{"name":"stdout","output_type":"stream","text":["WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:20:08 scheduler.py:245] Input prompt (70249 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  64%|██████▍   | 3557/5520 [30:17<13:01,  2.51it/s]  "]},{"name":"stdout","output_type":"stream","text":["WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n","WARNING 04-23 15:29:40 scheduler.py:245] Input prompt (6723 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 5520/5520 [45:36<00:00,  2.02it/s]\n","Processed prompts: 100%|██████████| 5577/5577 [47:12<00:00,  1.97it/s]  \n","Processed prompts: 100%|██████████| 5652/5652 [49:50<00:00,  1.89it/s]  \n"]}],"source":["### code ###\n","# prepare inputs\n","problems = data.problem.tolist()\n","if LOCAL: \n","    solutions = data.final_answer.map(lambda x:int(x[0])).tolist()\n","inputs = process_inputs(problems,gen_prompt_codeIn1)\n","capacity = len(inputs) * repeats\n","\n","# generation\n","final_answers = []\n","if LOCAL: monitors = []\n","for iteration in range(iterations):\n","    repeats = capacity//len(inputs)\n","    inputs = repeat_elements(inputs,repeats)\n","    raw_outputs = llm.generate(inputs, sampling_params)\n","    outs = process_code(raw_outputs,problems,repeats,normalize)\n","    final_answers.append(outs)\n","    if LOCAL:\n","        monitors.extend([(iteration, p, j.text, s, a) \\\n","                            for a,o,p,s in zip(repeat_elements(outs,repeats),raw_outputs,\\\n","                                               repeat_elements(problems,repeats),repeat_elements(solutions,repeats)) \n","                                for j in o.outputs])\n","        inputs,problems,solutions = zip(*[(o,p,s) for o,p,s in zip(outs,problems,solutions) if isinstance(o,str)]) # invalid answers\n","    else:\n","        inputs,problems = zip(*[(o,p) for o,p in zip(outs,problems) if isinstance(o,str)]) # invalid answers\n","    \n","### code END ###\n","\n","### pure reasoning ###\n","# prepare inputs\n","inputs = process_inputs(problems,gen_prompt_pure)\n","repeats = capacity//len(inputs)\n","inputs = repeat_elements(inputs,repeats)\n","\n","# generation\n","raw_outputs = llm.generate(inputs, sampling_params)\n","outs = process_pure(raw_outputs,repeats,normalize)\n","final_answers.append(outs)\n","if LOCAL:\n","        monitors.extend([(iteration+1, p, j.text, s, a) \\\n","                        for a,o,p,s in zip(repeat_elements(outs,repeats),raw_outputs,\\\n","                                            repeat_elements(problems,repeats),repeat_elements(solutions,repeats)) \n","                            for j in o.outputs])\n","### pure reasoning END###\n","\n","# post-process\n","def post_process(final_answers,end_with):\n","    final_answers = final_answers[:end_with]\n","    temp = final_answers[-1]\n","    for new in final_answers[::-1][1:]: # reverse order, starts with second to last\n","        temp = iter(temp)\n","        temp = [next(temp) if isinstance(x, str) else x for x in new] # replace str in new by temp\n","    temp = [37 if isinstance(x, str) else x for x in temp]\n","    return temp"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:59:10.995223Z","iopub.status.idle":"2024-04-22T20:59:10.995531Z","shell.execute_reply":"2024-04-22T20:59:10.995392Z","shell.execute_reply.started":"2024-04-22T20:59:10.995380Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["correct # for iteration 1: 135\n","correct # for iteration 2: 142\n","correct # for iteration 3: 146\n","correct # for iteration 4: 181\n","../llmOutputs/model9\n"]}],"source":["if LOCAL:\n","    # data.final_answer is like [['14'],['65'],...]\n","    for iteration in range(1,len(final_answers)+1):\n","        temp = post_process(final_answers,iteration)\n","        print(f\"correct # for iteration {iteration}: {sum([yhat==int(y[0])for yhat, y in zip(temp,data.final_answer.tolist())])}\")\n","    out_path = create_next_model_folder('../llmOutputs')\n","    print(out_path) # ../llmOutputs/model1\n","    with open(out_path + '/final_answers.json', 'w') as f:\n","        json.dump(final_answers, f)\n","    outs_df = pd.DataFrame(monitors,columns=['iteration','problem','output','y','yhat'])\n","    outs_df.to_csv(out_path+'/generations.csv', header=True, index=False)\n","else:\n","    temp = post_process(final_answers,len(final_answers))\n","    if not PRIVATE:\n","        answers = data.answer.tolist()\n","        correct = sum([y==yhat for y,yhat in zip(answers,temp)])\n","        print(f'{correct} correct answers')    \n","    data['answer'] = temp\n","    data[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8133715,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8077274,"sourceType":"datasetVersion"},{"sourceId":172559828,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":645.368853,"end_time":"2024-04-03T21:38:35.367254","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-03T21:27:49.998401","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"08213ff2f3fe4dc58310902a0e714fc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0063cf432324329b3ec56ac67170190","placeholder":"​","style":"IPY_MODEL_1c4f80ae8e4c4075ab1c0253b8bfe72b","value":" 3/3 [01:51&lt;00:00, 36.32s/it]"}},"1c4f80ae8e4c4075ab1c0253b8bfe72b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608f103f10f7493388c58f1022606b5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"698b2618c6cb44919194413c2013fa69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91954822b9664cfabc85f9bd4ca2a82a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_698b2618c6cb44919194413c2013fa69","placeholder":"​","style":"IPY_MODEL_608f103f10f7493388c58f1022606b5c","value":"Loading checkpoint shards: 100%"}},"968fcd0a8c76415cb8b0910b41f82633":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c824dd8e4ee14f53889e4e089ae2fa27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6cee2682b74df7bbe38f0d3f105e1b","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_968fcd0a8c76415cb8b0910b41f82633","value":3}},"d0063cf432324329b3ec56ac67170190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e841af56321d48eaad41c823c88647a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91954822b9664cfabc85f9bd4ca2a82a","IPY_MODEL_c824dd8e4ee14f53889e4e089ae2fa27","IPY_MODEL_08213ff2f3fe4dc58310902a0e714fc7"],"layout":"IPY_MODEL_fa01e68ebc80458bbe20fdac92c8285e"}},"ef6cee2682b74df7bbe38f0d3f105e1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa01e68ebc80458bbe20fdac92c8285e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
