{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T21:28:33.236687Z","iopub.status.busy":"2024-04-03T21:28:33.235888Z","iopub.status.idle":"2024-04-03T21:30:40.885067Z","shell.execute_reply":"2024-04-03T21:30:40.884103Z"},"papermill":{"duration":127.660006,"end_time":"2024-04-03T21:30:40.887589","exception":false,"start_time":"2024-04-03T21:28:33.227583","status":"completed"},"tags":[]},"outputs":[],"source":["try:\n","    from vllm import LLM, SamplingParams\n","    LOCAL = True\n","    MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","    from functions import *\n","    dtype = 'auto'\n","    gpu_memory_utilization = 0.95\n","\n","except:\n","    %pip uninstall -y torch -q\n","    %pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n","    from vllm import LLM, SamplingParams\n","    LOCAL = False\n","    MODEL_PATH = \"/kaggle/input/deepseek-math\"\n","    dtype = 'half'\n","    gpu_memory_utilization = 0.99\n","    from functions_math import *\n","\n","\n","import torch\n","import pandas as pd\n","import subprocess\n","import sys\n","import gc\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 04-21 13:23:18 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 04-21 13:23:18 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 04-21 13:23:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 04-21 13:23:18 selector.py:16] Using FlashAttention backend.\n","INFO 04-21 13:23:19 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 04-21 13:23:21 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 04-21 13:23:21 gpu_executor.py:94] # GPU blocks: 2304, # CPU blocks: 2184\n"]}],"source":["iterations = 3\n","repeats = 1\n","timeout = 7\n","temperature = 0.7\n","max_tokens = 1600\n","llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","stop_words = [tokenizer.eos_token]\n","sampling_params = SamplingParams(n = 1, best_of= 1,\n","                                 temperature=temperature,\n","                                 max_tokens=max_tokens,\n","                                 stop=stop_words)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T21:30:40.903909Z","iopub.status.busy":"2024-04-03T21:30:40.903421Z","iopub.status.idle":"2024-04-03T21:30:42.260344Z","shell.execute_reply":"2024-04-03T21:30:42.259185Z"},"papermill":{"duration":1.367528,"end_time":"2024-04-03T21:30:42.262807","exception":false,"start_time":"2024-04-03T21:30:40.895279","status":"completed"},"tags":[]},"outputs":[],"source":["if LOCAL:\n","    import json\n","    with open('../Data/AMC/aime_normal.json', 'r') as file:\n","        data = json.load(file)\n","    # to have consistent format as in Kaggle\n","    data = pd.DataFrame(data)\n","    data.rename(columns={'question': 'problem'}, inplace=True)\n","else:\n","    data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n","    if len(data) < 5:\n","        data = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","        PRIVATE = False\n","    else:\n","        PRIVATE = True"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","def gen_prompt_codeIn1(problem):\n","    return f\"\"\"\n","### Problem:\\n{problem}\\n\n","### Response: Let's first reason step by step. Then write Python code to solve the problem, using brute force enumeration if necessary. \n","Limit the search space using logical deductions. The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","The final answer is an integer between 0 and 999.\n","\"\"\"\n","\n","def parse_err(input):\n","    return f\"\"\" \\n{input}\\n\n","    The code should be enclosed between ```python\\n actual code...``` Please fix.\n","\"\"\"\n","\n","def time_err(code):\n","    return f\"\"\" \\n{code}\\n\n","    code timed out. Try to use math reasoning to limit the search space and find ways to exit loop early where appropriate.\n","\"\"\"\n","\n","def code_err(code, err):\n","    return f\"\"\" \\n{code}\\n\n","    The code encounters this error: {err}\\n.\n","    Please fix.\n","\"\"\"\n","\n","def eval_err(code):\n","    return f\"\"\" \\n{code}\\n\n","    code should only print the final number.\n","\"\"\"\n","\n","def number_range_type_err(problem,input,answer):\n","    return f\"\"\" \\nThis is the original problem: {problem}\\n \n","    \\nThis is the first solution: {input}\\n\n","    However, answer should be a integer between 0 and 999 but got {str(answer)}. Please revisit the logics and\n","    generate python code to answer the question.\n","    The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","\"\"\"    \n","def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        query_prompt = gen_prompt_codeIn1(problem)\n","        messages = [{\"role\": \"user\",\"content\": query_prompt}]\n","        input = tokenizer.apply_chat_template(messages, tokenize=False)\n","        outs.append(input)\n","    return outs\n","\n","from functools import partial\n","def get_value(output,normalize):\n","    # cumulative_logprob are negative. factor of 10 penalize it.\n","    if len(output.token_ids) == 0: return float('-inf')\n","    text = output.text\n","    if \"```python\" not in text and text.count(\"print(\") != 1:\n","        factor = 10\n","    else:\n","        factor = 1\n","    if normalize:\n","        return factor*output.cumulative_logprob/len(output.token_ids)\n","    else:\n","        return factor*output.cumulative_logprob\n","\n","def process_code(inputs,problems,normalize=True):\n","    # inputs,problem is a list of str\n","    outs = []\n","    for input,problem in zip(inputs,problems):\n","        # input is vllm.outputs.RequestOutput\n","        input = max(input.outputs,key=partial(get_value,normalize=normalize)).text\n","        # parse err\n","        try:\n","            code = input.split('```')[1][7:]\n","        except: \n","            outs.append(parse_err(input))\n","            continue\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","        except subprocess.TimeoutExpired:\n","            outs.append(time_err(code))\n","            continue\n","        if process.stderr:# code.py err\n","            stderr = process.stderr.decode('utf8')\n","            outs.append(code_err(code, stderr))\n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer) and is_between_0_and_999(answer):\n","                    outs.append(int(answer))\n","                    continue\n","                else:\n","                    outs.append(number_range_type_err(problem,input,answer))\n","                    continue\n","            except:\n","                outs.append(eval_err(code))\n","                continue\n","    return outs"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 975/975 [08:06<00:00,  2.00it/s]\n","Token indices sequence length is longer than the specified maximum sequence length for this model (48670 > 4096). Running this sequence through the model will result in indexing errors\n","Processed prompts:   0%|          | 0/660 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-21 13:37:49 scheduler.py:245] Input prompt (2170 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  12%|█▏        | 81/660 [00:35<05:23,  1.79it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-21 13:38:23 scheduler.py:245] Input prompt (2059 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  23%|██▎       | 153/660 [01:10<03:53,  2.17it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-21 13:38:57 scheduler.py:245] Input prompt (48670 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  32%|███▏      | 212/660 [01:31<01:36,  4.64it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-21 13:39:19 scheduler.py:245] Input prompt (45674 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  58%|█████▊    | 381/660 [02:54<02:17,  2.03it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-21 13:40:41 scheduler.py:245] Input prompt (2070 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 660/660 [05:26<00:00,  2.02it/s]\n","Processed prompts: 100%|██████████| 530/530 [04:34<00:00,  1.93it/s]\n"]}],"source":["# prepare inputs\n","problems = data.problem.tolist()\n","inputs = process_inputs(problems)\n","# capacity = len(inputs) * sampling_params['best_of']\n","\n","# generation\n","final_answers = []\n","if LOCAL: monitors = []\n","for _ in range(iterations):\n","    # sampling_params['n'] = sampling_params['best_of'] = capacity//len(inputs)\n","    raw_outputs = llm.generate(inputs, sampling_params)\n","    outs = process_code(raw_outputs,problems)\n","    inputs,problems = zip(*[(o,p) for o,p in zip(outs,problems) if isinstance(o,str)]) # invalid answers\n","    final_answers.append(outs)\n","    if LOCAL: monitors.append([[(j.text,j.cumulative_logprob,len(j.token_ids)) for j in o.outputs] for o in raw_outputs])\n","\n","# post-process\n","temp = final_answers[-1]\n","for new in final_answers[::-1][1:]: # reverse order, starts with second to last\n","    temp = iter(temp)\n","    temp = [next(temp) if isinstance(x, str) else x for x in new] # replace str in new by temp\n","temp = [37 if isinstance(x, str) else x for x in temp]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["correct # for iteration 0: 95\n","correct # for iteration 3: 116\n","../llmOutputs/model8\n"]}],"source":["if LOCAL:\n","    # iteration 0\n","    # data.final_answer is like [['14'],['65'],...]\n","    print(f\"correct # for iteration 0: {sum([yhat==int(y[0])for yhat, y in zip(final_answers[0],data.final_answer.tolist())])}\")\n","    print(f\"correct # for iteration {iterations}: {sum([yhat==int(y[0])for yhat, y in zip(temp,data.final_answer.tolist())])}\")\n","    out_path = create_next_model_folder('../llmOutputs')\n","    print(out_path) # ../llmOutputs/model1\n","    with open(out_path + '/final_answers.json', 'w') as f:\n","        json.dump(final_answers, f)\n","    with open(out_path + '/monitors.json', 'w') as f:\n","        json.dump(monitors, f)        \n","else:\n","    if not PRIVATE:\n","        answers = data.answer.tolist()\n","        correct = sum([y==yhat for y,yhat in zip(answers,temp)])\n","        print(f'{correct} correct answers')    \n","    data['answer'] = temp\n","    data[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8133715,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8077274,"sourceType":"datasetVersion"},{"sourceId":172559828,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":645.368853,"end_time":"2024-04-03T21:38:35.367254","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-03T21:27:49.998401","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"08213ff2f3fe4dc58310902a0e714fc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0063cf432324329b3ec56ac67170190","placeholder":"​","style":"IPY_MODEL_1c4f80ae8e4c4075ab1c0253b8bfe72b","value":" 3/3 [01:51&lt;00:00, 36.32s/it]"}},"1c4f80ae8e4c4075ab1c0253b8bfe72b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608f103f10f7493388c58f1022606b5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"698b2618c6cb44919194413c2013fa69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91954822b9664cfabc85f9bd4ca2a82a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_698b2618c6cb44919194413c2013fa69","placeholder":"​","style":"IPY_MODEL_608f103f10f7493388c58f1022606b5c","value":"Loading checkpoint shards: 100%"}},"968fcd0a8c76415cb8b0910b41f82633":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c824dd8e4ee14f53889e4e089ae2fa27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6cee2682b74df7bbe38f0d3f105e1b","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_968fcd0a8c76415cb8b0910b41f82633","value":3}},"d0063cf432324329b3ec56ac67170190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e841af56321d48eaad41c823c88647a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91954822b9664cfabc85f9bd4ca2a82a","IPY_MODEL_c824dd8e4ee14f53889e4e089ae2fa27","IPY_MODEL_08213ff2f3fe4dc58310902a0e714fc7"],"layout":"IPY_MODEL_fa01e68ebc80458bbe20fdac92c8285e"}},"ef6cee2682b74df7bbe38f0d3f105e1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa01e68ebc80458bbe20fdac92c8285e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
