{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1\"\n",
    "MODEL_PATH = f\"../Model/PRM_LORA{version}_merged_code_policy_01\"\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data: {1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: clean_text\n",
    "weight = 0.5\n",
    "SFT_weight = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# separate out question and solution and only train on solution\n",
    "patterns = [\"``` and should only print the final answer.\",\\\n",
    "            \"print the final result.\\nApproach:\",\\\n",
    "            \"print the final output, as an integer not other python object such as list or tuple.\"]\n",
    "import re\n",
    "clean_text = lambda x:re.sub(r\"(<math>|<\\/math>|<cmath>|<\\/cmath>|\\\\begin\\{align\\*\\}|\\\\end\\{align\\*\\})\", \"\", x)\n",
    "def search_patterns(text, patterns):\n",
    "    for pattern in patterns:\n",
    "        # Compile the pattern\n",
    "        regex = re.compile(pattern)\n",
    "        # Find all matches of the pattern in the text\n",
    "        matches = list(regex.finditer(text))\n",
    "        # If there is one match, get the end position\n",
    "        if matches:\n",
    "            return matches[0].end()\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # {1}\n",
    "# import pickle\n",
    "# with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "#     completed_paths_y = pickle.load(f)\n",
    "\n",
    "# texts = []\n",
    "# for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "#     if y == 1:\n",
    "#         texts.append(text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"))\n",
    "\n",
    "# input_ids = []\n",
    "# lengths = []\n",
    "# for text in texts:\n",
    "#     idx = search_patterns(text,patterns)\n",
    "#     question = tokenizer.encode(text[:idx],add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # {0,1}\n",
    "# def from_gen(texts,ys,lengths):\n",
    "#     data = list(zip(texts,ys,lengths))\n",
    "#     random.shuffle(data)\n",
    "#     for text,y,l in data:\n",
    "#         text = torch.tensor(text[:MAX_LEN],device='cuda')[None]\n",
    "#         yield text,y,l\n",
    "\n",
    "# training\n",
    "# for i,(text,y,l) in enumerate(from_gen(input_ids,ys,lengths)):\n",
    "# loss = loss_fn(outs[0,l:-1],text[0,l+1:]) * y # (l,C), (l,)\n",
    "\n",
    "\n",
    "# {1}\n",
    "def from_gen(texts,lengths):\n",
    "    data = list(zip(texts,lengths))\n",
    "    random.shuffle(data)\n",
    "    for text,l in data:\n",
    "        text = torch.tensor(text[:MAX_LEN],device='cuda')[None]\n",
    "        yield text,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0,1}\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "data = []\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([clean_text(text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\")),y])\n",
    "texts,ys = zip(*data)\n",
    "\n",
    "ys = np.array(ys)\n",
    "ys = (ys-ys.mean())/ys.std()\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for text in texts:\n",
    "    idx = search_patterns(text,patterns)\n",
    "    question = tokenizer.encode(text[:idx],add_special_tokens=True)\n",
    "    answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi\n",
    "with open(f\"../llmOutputs/PRM/data_pi1_code{version}.pickle\", \"rb\") as f:\n",
    "    data_pi = pickle.load(f)\n",
    "texts2,ys2,lengths_raw = zip(*data_pi)\n",
    "ys2 = np.array(ys2)\n",
    "ys2 = ys2/ys2.std() * weight\n",
    "\n",
    "# combined\n",
    "ys = ys.tolist() + ys2.tolist()\n",
    "for text,idx in zip(texts2,lengths_raw):\n",
    "    question = tokenizer.encode(text[:idx].replace(\"<｜begin▁of▁sentence｜>User: \",\"\"),add_special_tokens=True)\n",
    "    answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SFT\n",
    "# def gen_prompt_codeIn1(problem):\n",
    "#     return f\"\"\"Problem: {problem}\\n\n",
    "# To accomplish this, first determine a python-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and your final answer should be integer, not expression, list, tuple or dictionary!\n",
    "# Write the entire script covering all the steps (use comments and document it well) and print the final result.\n",
    "# Approach:\"\"\"\n",
    "\n",
    "# def gen_prompt_codeIn2(problem):\n",
    "#     return f\"\"\"Problem: {problem}\\n\n",
    "# You are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\"\"\"\n",
    "\n",
    "# def add_prompt(problem):\n",
    "#     if np.random.rand()<0.5:\n",
    "#         return gen_prompt_codeIn1(problem)\n",
    "#     else:\n",
    "#         return gen_prompt_codeIn2(problem)\n",
    "    \n",
    "# sft = pd.read_csv(\"../Data/MATH/math.csv\")\n",
    "# sft = sft.loc[sft.boxed_number == sft.parsed] \n",
    "# sft['prob_wPrompt'] = sft.problem.apply(add_prompt)\n",
    "# for q,a in zip(sft.prob_wPrompt.tolist(),sft.code_solution.tolist()):\n",
    "#     question = tokenizer.encode(q,add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(a,add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)\n",
    "\n",
    "# ys = ys + [SFT_weight] * sft.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 2e-5\n",
    "clip = 2e-3\n",
    "from transformers import AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            trust_remote_code=True,\n",
    "                                            attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "model.gradient_checkpointing_enable()\n",
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ],\n",
    "                        #  use_dora=True,\n",
    "                        )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.3026118021621187\n",
      "iter: 2047, \n",
      " train loss: 0.28984769291355406\n",
      "iter: 3071, \n",
      " train loss: 0.28957493073585283\n"
     ]
    }
   ],
   "source": [
    "def empty_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import math\n",
    "import gc\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,l) in enumerate(from_gen(input_ids,lengths)):\n",
    "        if i > 0:\n",
    "            del outs,loss\n",
    "            empty_cache()\n",
    "        outs = model(text).logits # 1,l,C\n",
    "        loss = loss_fn(outs[0,l:-1],text[0,l+1:]) # (l,C), (l,)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()): continue\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../Model/PRM_LORA5_merged_code_policy_01 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_version = str(int(version) + 1)\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_01\"\n",
    "# !mkdir peft_model_id\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab999575d954cdf8d14e47159dbdb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model,texts,outs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_01\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "base_model2.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../Model/PRM_LORA7_merged_code_policy_01/tokenizer_config.json',\n",
       " '../Model/PRM_LORA7_merged_code_policy_01/special_tokens_map.json',\n",
       " '../Model/PRM_LORA7_merged_code_policy_01/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "tokenizer.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
