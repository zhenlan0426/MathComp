{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1\"\n",
    "MODEL_PATH = f\"../Model/PRM_LORA{version}_merged_code_policy_01\"\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data: {1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_weight = 0.5\n",
    "SFT_weight = 1\n",
    "min_loss = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# separate out question and solution and only train on solution\n",
    "patterns = [r\"Write your final answer as a single integer in the last line of your response, enclosed within \\\\boxed{}\",\\\n",
    "            r\"print the final result.\\nApproach:\",\\\n",
    "            r\"print the final output, as an integer not other python object such as list or tuple.\"]\n",
    "import re\n",
    "clean_text = lambda x:re.sub(r\"(<math>|<\\/math>|<cmath>|<\\/cmath>|\\\\begin\\{align\\*\\}|\\\\end\\{align\\*\\})\", \"\", x)\n",
    "def search_patterns(text, patterns):\n",
    "    for pattern in patterns:\n",
    "        # Compile the pattern\n",
    "        regex = re.compile(pattern)\n",
    "        # Find all matches of the pattern in the text\n",
    "        matches = list(regex.finditer(text))\n",
    "        # If there is one match, get the end position\n",
    "        if matches:\n",
    "            return matches[0].end()\n",
    "    raise Exception(\"no match\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # {1}\n",
    "# import pickle\n",
    "# with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "#     completed_paths_y = pickle.load(f)\n",
    "\n",
    "# texts = []\n",
    "# for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "#     if y == 1:\n",
    "#         texts.append(text)\n",
    "\n",
    "# input_ids = []\n",
    "# lengths = []\n",
    "# for text in texts:\n",
    "#     idx = search_patterns(text,patterns)\n",
    "#     question = tokenizer.encode(text[:idx],add_special_tokens=True)\n",
    "#     answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "#     lengths.append(len(question))\n",
    "#     input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0,1}\n",
    "def from_gen(texts,ys,lengths):\n",
    "    data = list(zip(texts,ys,lengths))\n",
    "    random.shuffle(data)\n",
    "    for text,y,l in data:\n",
    "        text = torch.tensor(text[:MAX_LEN],device='cuda')[None]\n",
    "        yield text,y,l\n",
    "\n",
    "# training\n",
    "# for i,(text,y,l) in enumerate(from_gen(input_ids,ys,lengths)):\n",
    "# loss = loss_fn(outs[0,l:-1],text[0,l+1:]) * y # (l,C), (l,)\n",
    "\n",
    "\n",
    "# {1}\n",
    "# def from_gen(texts,lengths):\n",
    "#     data = list(zip(texts,lengths))\n",
    "#     random.shuffle(data)\n",
    "#     for text,l in data:\n",
    "#         text = torch.tensor(text[:MAX_LEN],device='cuda')[None]\n",
    "#         yield text,l\n",
    "\n",
    "# training\n",
    "# for i,(text,l) in enumerate(from_gen(input_ids,lengths)):\n",
    "#     loss = loss_fn(outs[0,l:-1],text[0,l+1:]) # (l,C), (l,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0,1}\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "data = []\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([clean_text(text),y])\n",
    "texts,ys = zip(*data)\n",
    "\n",
    "ys = np.array(ys)\n",
    "ys = (ys-ys.mean())/ys.std()\n",
    "ys[ys<0] *= neg_weight\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for text in texts:\n",
    "    idx = search_patterns(text,patterns)\n",
    "    question = tokenizer.encode(text[21:idx]+\"\\n\\nAssistant:\",add_special_tokens=True)\n",
    "    answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi\n",
    "with open(f\"../llmOutputs/PRM/data_pi1_code{version}.pickle\", \"rb\") as f:\n",
    "    data_pi = pickle.load(f)\n",
    "texts2,ys2,lengths_raw = zip(*data_pi)\n",
    "ys2 = np.array(ys2)\n",
    "ys2 = ys2/ys2.std() * len(texts) / len(texts2)\n",
    "\n",
    "# combined\n",
    "ys = ys.tolist() + ys2.tolist()\n",
    "for text,idx in zip(texts2,lengths_raw):\n",
    "    question = tokenizer.encode(clean_text(text[21:idx]+\"\\n\\nAssistant:\"),add_special_tokens=True)\n",
    "    answer = tokenizer.encode(clean_text(text[idx:]),add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT - Math\n",
    "def gen_prompt_codeIn1(problem):\n",
    "    return f\"\"\"User: {problem}\\n\n",
    "Determine a sympy-based approach for solving the problem. When defining symbol, incorporate all constraints mentioned in the problem statement, e.g. real, integer, even, odd, positive, prime. If a variable represents a positive integer, Symbol('n', integer=True, positive=True). Your final answer should be integer, not expression, list, tuple or dictionary!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the final result.\\n\\nAssistant:\n",
    "\"\"\"\n",
    "\n",
    "def gen_prompt_codeIn2(problem):\n",
    "    return f\"\"\"User: {problem}\\n\n",
    "You are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\\n\\nAssistant:\"\"\"\n",
    "\n",
    "def gen_prompt3(problem):\n",
    "    return \"User: \"+problem+'''\\n\n",
    "Carefully read and understand the problem and use all information in problem statement. No Python code. Show your work step-by-step, explain your reasoning, calculations, mathematical concepts and formulas in detail.\n",
    "Write your final answer as a single integer in the last line of your response, enclosed within \\\\boxed{}.\\n\\nAssistant:\n",
    "'''\n",
    "\n",
    "def add_prompt(problem):\n",
    "    if np.random.rand()<0.5:\n",
    "        return gen_prompt_codeIn1(problem)\n",
    "    else:\n",
    "        return gen_prompt_codeIn2(problem)\n",
    "    \n",
    "sft = pd.read_csv(\"../Data/MATH/math.csv\")\n",
    "# sft = sft.loc[sft.boxed_number == sft.parsed]\n",
    "sft = sft.loc[(sft.boxed_number == sft.parsed) & (sft.level == 'Level 5')]\n",
    "sft['code_wPrompt'] = sft.problem.apply(add_prompt)\n",
    "for q,a in zip(sft.code_wPrompt.tolist(),sft.code_solution.tolist()):\n",
    "    question = tokenizer.encode(clean_text(q),add_special_tokens=True)\n",
    "    answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)\n",
    "ys = ys + [SFT_weight] * sft.shape[0]\n",
    "\n",
    "sft['pure_wPrompt'] = sft.problem.apply(gen_prompt3)\n",
    "for q,a in zip(sft.pure_wPrompt.tolist(),sft.solution.tolist()):\n",
    "    question = tokenizer.encode(clean_text(q),add_special_tokens=True)\n",
    "    answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)\n",
    "ys = ys + [SFT_weight] * sft.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4681 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# SFT - AIME (prompt included). [9:] remove \"Problem:\"\n",
    "with open(f\"../Data/ai-mathematical-olympiad-prize/10prob.pickle\", \"rb\") as f:\n",
    "    outs = pickle.load(f)\n",
    "with open(f\"../Data/AMC/aime_final.pickle\", \"rb\") as f:\n",
    "    outs2 = pickle.load(f)\n",
    "for q,a in outs:\n",
    "    question = tokenizer.encode(\"User: \"+clean_text(q[9:])+\"\\n\\nAssistant:\",add_special_tokens=True)\n",
    "    answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)\n",
    "ys = ys + [SFT_weight] * len(outs)\n",
    "\n",
    "for q,a in outs2:\n",
    "    question = tokenizer.encode(\"User: \"+clean_text(q[9:])+\"\\n\\nAssistant:\",add_special_tokens=True)\n",
    "    answer = tokenizer.encode(clean_text(a),add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)\n",
    "ys = ys + [SFT_weight] * len(outs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8579e4fbaeb74d508ea76b3d0e789980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 2e-5\n",
    "clip = 2e-3\n",
    "from transformers import AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            trust_remote_code=True,\n",
    "                                            attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "model.gradient_checkpointing_enable()\n",
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ],\n",
    "                        #  use_dora=True,\n",
    "                        )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.1357256063058564\n",
      "iter: 2047, \n",
      " train loss: 0.10721560751460828\n",
      "iter: 3071, \n",
      " train loss: 0.11950232494903043\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m outs,loss\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m outs \u001b[38;5;241m=\u001b[39m model(text)\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;66;03m# 1,l,C\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# loss = loss_fn(outs[0,l:-1],text[0,l+1:]) # (l,C), (l,)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty_cache\u001b[39m():\n\u001b[1;32m      2\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def empty_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import math\n",
    "import gc\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # for i,(text,l) in enumerate(from_gen(input_ids,lengths)):\n",
    "    for i,(text,y,l) in enumerate(from_gen(input_ids,ys,lengths)):\n",
    "        if i > 0:\n",
    "            del outs,loss\n",
    "            empty_cache()\n",
    "        outs = model(text).logits # 1,l,C\n",
    "        # loss = loss_fn(outs[0,l:-1],text[0,l+1:]) # (l,C), (l,)\n",
    "        loss = loss_fn(outs[0,l:-1],text[0,l+1:]) * y # (l,C), (l,)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()): continue\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            if (train_loss/count_loss)<min_loss:\n",
    "                break\n",
    "            train_loss = 0\n",
    "            count_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../Model/PRM_LORA1_merged_code_policy_01 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_version = str(int(version) + 1)\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_01\"\n",
    "# !mkdir peft_model_id\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e10176a7064e15a247ef5f57d7131c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model,texts,outs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_01\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "base_model2.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../Model/PRM_LORA2_merged_code_policy_01/tokenizer_config.json',\n",
       " '../Model/PRM_LORA2_merged_code_policy_01/special_tokens_map.json',\n",
       " '../Model/PRM_LORA2_merged_code_policy_01/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "tokenizer.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
