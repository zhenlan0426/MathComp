{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:03:03.749818Z","iopub.status.busy":"2024-05-18T17:03:03.749074Z","iopub.status.idle":"2024-05-18T17:08:21.725719Z","shell.execute_reply":"2024-05-18T17:08:21.724832Z"},"papermill":{"duration":317.985165,"end_time":"2024-05-18T17:08:21.727692","exception":false,"start_time":"2024-05-18T17:03:03.742527","status":"completed"},"tags":[]},"outputs":[],"source":["from vllm import LLM, SamplingParams\n","LOCAL = True\n","MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","from functions import *\n","dtype = 'auto'\n","gpu_memory_utilization = 0.95\n","\n","import torch\n","import pandas as pd\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["version = \"4\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 05-31 07:59:50 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 05-31 07:59:50 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 07:59:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["/home/zhenlan/anaconda3/envs/vllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 07:59:51 selector.py:16] Using FlashAttention backend.\n","INFO 05-31 07:59:52 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 07:59:53 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 05-31 07:59:54 gpu_executor.py:94] # GPU blocks: 2311, # CPU blocks: 2184\n"]}],"source":["llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","\n","# stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n","stop_words = [tokenizer.eos_token,\"```output\",\"```Output\",\"```output\\n\",\"```Output\\n\",\"```\\nOutput\" , \")\\n```\" , \"``````output\"]\n","# stop_words.append(\"\\n\")\n","sampling_params = SamplingParams(temperature=1,\n","                                 max_tokens=256,\n","                                #  min_tokens=32,\n","                                 stop=stop_words,\n","                                 include_stop_str_in_output=True)\n","\n","\n","def gen_prompt_codeIn1(problem):\n","    return f\"\"\"Problem: {problem}\\n\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and your final answer should be integer, not expression, list, tuple or dictionary!\n","Write the entire script covering all the steps (use comments and document it well) and print the final result.\n","Approach:\"\"\"\n","\n","def gen_prompt_codeIn2(problem):\n","    return f\"\"\"Problem: {problem}\\n\n","You are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\"\"\"\n","\n","\n","\n","n = 3 # beams\n","n_sol = 7\n","samples = 7\n","max_depth = 16\n","max_pct = 0.8"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9649f9afb2d49658fa457a97c75b160","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge4_code and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import LlamaForSequenceClassification\n","prm_tokenizer = tokenizer\n","prm_model = LlamaForSequenceClassification.from_pretrained(f'../Model/PRM_LORA_merge{version}_code',\\\n","                                                    num_labels=1,\\\n","                                                    device_map=\"cpu\",\n","                                                    torch_dtype=\"auto\",\n","                                                    ).eval()\n","base_model = prm_model.model\n","prm_model.score.load_state_dict(torch.load(f'../Model/model_score{version}_code.pth'))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import json\n","with open('../Data/AMC/aime_normal.json', 'r') as file:\n","    df = json.load(file)\n","# to have consistent format as in Kaggle\n","df = pd.DataFrame(df)#.iloc[:24]\n","df.rename(columns={'question': 'problem'}, inplace=True)\n","# import pandas as pd\n","# df = pd.read_csv('../Data/AMC/cleaned_ArtOfProblemSolving.csv').iloc[:20]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        base_prompt1 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt_codeIn1(problem)}],tokenize=False)\n","        base_prompt2 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt_codeIn2(problem)}],tokenize=False)\n","        outs.append(base_prompt1)\n","        outs.append(base_prompt2)\n","    return outs"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:08:23.262621Z","iopub.status.busy":"2024-05-18T17:08:23.262138Z","iopub.status.idle":"2024-05-18T17:08:23.268774Z","shell.execute_reply":"2024-05-18T17:08:23.267916Z"},"papermill":{"duration":0.016196,"end_time":"2024-05-18T17:08:23.270775","exception":false,"start_time":"2024-05-18T17:08:23.254579","status":"completed"},"tags":[]},"outputs":[],"source":["logit2prob = lambda x: 1/(1+np.exp(-x))\n","def eval_prm(candidates):\n","    all_log_probs = []\n","    for i in range(len(candidates)):\n","        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda\")\n","        with torch.no_grad():\n","            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n","            logits = prm_model.score(hidden_states)[0]\n","        all_log_probs.append(logit2prob(logits.item()))\n","    return all_log_probs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'[Tt]he answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"[Tt]he answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"[Tt]he answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def group_and_sum(A, B):\n","    '''\n","    A = ['a','b','a']\n","    B = [1,2,3]\n","    -> {'a': 4, 'b': 2}\n","    '''\n","    result_dict = {}\n","    for a, b in zip(A, B):\n","        if a in result_dict:\n","            result_dict[a] += b\n","        else:\n","            result_dict[a] = b\n","    return result_dict\n","\n","def group_and_average(A, B):\n","    from collections import defaultdict\n","    # Create a dictionary to store sums and counts for averaging\n","    sum_dict = defaultdict(lambda: [0, 0])  # Each key maps to [sum, count]\n","    # Pair elements from A and B and aggregate sums and counts\n","    for key, value in zip(A, B):\n","        sum_dict[key][0] += value\n","        sum_dict[key][1] += 1\n","    # Calculate averages\n","    averages = {key: sum_count[0] / sum_count[1] for key, sum_count in sum_dict.items()}\n","    return averages,[averages[a] for a in A]\n","\n","def max_dict(d):\n","    return max(d.items(), key=lambda x: x[1])[0]\n","\n","def tot_agg(completed_paths):\n","    if completed_paths:\n","        answers,scores = zip(*completed_paths)\n","        groups = group_and_sum(answers, scores)\n","        return max_dict(groups)\n","    else:\n","        return 37 # empty completed_paths\n","    \n","def repeat_elements(lst, k):\n","    return [i for i in lst for _ in range(k)]\n","\n","def flatten(nested_list):\n","    \"\"\"Flatten a nested list.\"\"\"\n","    out = []\n","    lengths = []\n","    for sublist in nested_list:\n","        lengths.append(len(sublist))\n","        for item in sublist:\n","            out.append(item)\n","    return out,lengths\n","\n","def unflatten(flat_list, lengths):\n","    \"\"\"Unflatten a flat list into a nested list based on lengths.\"\"\"\n","    nested_list = []\n","    index = 0\n","    for length in lengths:\n","        nested_list.append(flat_list[index:index + length])\n","        index += length\n","    return nested_list\n","\n","def filter_input(batch_response,current_level_node):\n","    # one question filter\n","    prm_inputs = []\n","    parents = []\n","    for candidate,parent in zip(batch_response,current_level_node):\n","        if candidate.outputs[0].text not in parent:\n","            prm_input = parent + candidate.outputs[0].text\n","            prm_inputs.append(prm_input)\n","            parents.append(parent)\n","    # Get the indices of unique elements in prm_inputs\n","    unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n","    prm_inputs = [prm_inputs[i] for i in unique_indices]\n","    parents = [parents[i] for i in unique_indices]\n","    return prm_inputs,parents,len(prm_inputs)\n","\n","def filter_inputs(batch_responses,current_level_nodes,lengths):\n","    # all question filter\n","    # returned value should be flattened\n","    batch_responses,current_level_nodes = unflatten(batch_responses,lengths),unflatten(current_level_nodes,lengths)\n","    prm_inputs = []\n","    lengths = []\n","    parent_list = []\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(batch_responses) == len(uncompleted)\n","    for batch_response,current_level_node,path in zip(batch_responses,current_level_nodes,uncompleted):\n","        prm_input,parents,length = filter_input(batch_response,current_level_node)\n","        if length == 0:# all bad\n","            while len(path)<n_sol:\n","                # make complete for this question as there will be no continued effort\n","                path.append(None)\n","        else:\n","            prm_inputs.extend(prm_input)\n","            parent_list.extend(parents)\n","            lengths.append(length)\n","    return prm_inputs,parent_list,lengths\n","\n","def IsFinished(node):\n","    matches = re.findall(r'print\\(([^)]*)\\)', node)\n","    return len(matches)>0\n","\n","def get_next_node(prm_inputs,prm_scores,completed_paths):\n","    # need to update completed_paths in-place\n","    next_level_nodes = []\n","    combined = list(zip(prm_inputs,prm_scores))\n","    combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n","    max_score = combined[0][1]\n","    for node,score in combined:\n","        finish = IsFinished(node)\n","        if finish: # finished\n","            completed_paths.append((score,node))\n","        else: # not inished\n","            if len(next_level_nodes) < n:\n","                next_level_nodes.append(node)\n","    return next_level_nodes\n","\n","\n","def get_next_nodes(prm_inputs,prm_scores,lengths):\n","    # for completed_paths, next_level_nodes would be removed\n","    # returned value should be flattened\n","    prm_inputs,prm_scores = unflatten(prm_inputs,lengths),unflatten(prm_scores,lengths)\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(uncompleted) == len(lengths)\n","    assert len(prm_inputs) == len(lengths)\n","    assert len(prm_scores) == len(lengths)\n","    next_level_nodes,lengths = [],[]\n","    for prm_input,prm_score,completed_path in zip(prm_inputs,prm_scores,uncompleted):\n","        next_node = get_next_node(prm_input,prm_score,completed_path)\n","        if len(completed_path) < n_sol:\n","            next_level_nodes.extend(next_node)\n","            lengths.append(len(next_node))\n","    return next_level_nodes,lengths\n","\n","import gc\n","def create_llm():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","    tokenizer = llm.get_tokenizer()\n","    return llm,tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 13650/13650 [25:57<00:00,  8.76it/s] \n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 08:39:45 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 08:39:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["/home/zhenlan/anaconda3/envs/vllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 08:39:45 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 08:39:52 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 08:39:53 gpu_executor.py:94] # GPU blocks: 2373, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 15981/15981 [34:52<00:00,  7.64it/s] \n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:32:26 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:32:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:32:26 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:32:28 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:32:28 gpu_executor.py:94] # GPU blocks: 2376, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 2429/2429 [06:47<00:00,  5.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:42:14 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:42:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:42:14 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:42:16 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:42:16 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 616/616 [02:17<00:00,  4.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:45:32 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:45:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:45:32 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:45:34 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:45:34 gpu_executor.py:94] # GPU blocks: 2376, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 343/343 [01:32<00:00,  3.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:47:52 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:47:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:47:52 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:47:54 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:47:54 gpu_executor.py:94] # GPU blocks: 2374, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 259/259 [01:14<00:00,  3.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:49:39 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:49:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:49:39 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:49:40 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:49:40 gpu_executor.py:94] # GPU blocks: 2375, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  45%|████▍     | 47/105 [00:23<00:13,  4.42it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:05 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 105/105 [00:39<00:00,  2.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:50:40 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:50:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:50:40 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:50:42 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:50:42 gpu_executor.py:94] # GPU blocks: 2375, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  62%|██████▏   | 65/105 [00:14<00:05,  6.80it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:50:57 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 105/105 [00:19<00:00,  5.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:51:15 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:51:15 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:51:15 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:51:17 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:51:17 gpu_executor.py:94] # GPU blocks: 2376, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 0/84 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:51:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 84/84 [00:11<00:00,  7.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:51:38 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:51:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:51:38 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:51:39 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:51:40 gpu_executor.py:94] # GPU blocks: 2374, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 42/42 [00:11<00:00,  3.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:51:59 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:51:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:51:59 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:52:01 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:52:01 gpu_executor.py:94] # GPU blocks: 2375, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:52:20 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:52:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:52:20 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:52:21 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:52:22 gpu_executor.py:94] # GPU blocks: 2374, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  10%|▉         | 2/21 [00:00<00:06,  2.76it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:23 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 21/21 [00:06<00:00,  3.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:52:36 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:52:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:52:36 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:52:38 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:52:38 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:38 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 21/21 [00:04<00:00,  4.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:52:50 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:52:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:52:50 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:52:52 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:52:52 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  10%|▉         | 2/21 [00:00<00:06,  2.73it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:52:53 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 21/21 [00:02<00:00,  7.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:53:02 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:53:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:53:02 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:53:03 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:53:04 gpu_executor.py:94] # GPU blocks: 2379, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  33%|███▎      | 7/21 [00:02<00:06,  2.18it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-31 09:53:06 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 21/21 [00:03<00:00,  6.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:53:14 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-31 09:53:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-31 09:53:14 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-31 09:53:16 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-31 09:53:16 gpu_executor.py:94] # GPU blocks: 2375, # CPU blocks: 2184\n"]}],"source":["# %debug\n","# [path for path in completed_paths if len(path)<n_sol] used to track on-going questiones\n","# flattened inputs, with lengths corresponds to the uncompleted path\n","# two ways for path to complete, one is via get_next_nodes getting n_sol answers\n","# two is via filter_inputs, all continuations are bad\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","current_level_nodes = process_inputs(df.problem.tolist())\n","lengths = [2] * df.shape[0]\n","current_level = 1\n","completed_paths = [[] for _ in range(df.shape[0])]\n","data_V = []\n","data_pi = []\n","\n","while (current_level < max_depth) and (current_level_nodes):\n","    # everything at this level is flattened\n","    current_level_nodes = repeat_elements(current_level_nodes,samples)\n","    lengths = [l*samples for l in lengths]\n","    batch_responses = llm.generate(current_level_nodes, sampling_params)\n","    prm_inputs,parent_list,lengths = filter_inputs(batch_responses,current_level_nodes,lengths)\n","    \n","    # release VRAM to prm_model\n","    del llm\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    prm_model.to('cuda')\n","    prm_scores = eval_prm(prm_inputs)\n","    \n","    # save for Q-learning\n","    averages,averages_dup = group_and_average(parent_list,prm_scores)\n","    data_V.extend(list(averages.items()))\n","    advantages = [q-v for q,v in zip(prm_scores,averages_dup)]\n","    data_pi.extend(list(zip(prm_inputs,advantages)))\n","    \n","    # release VRAM to llm\n","    prm_model.to('cpu')\n","    llm,tokenizer = create_llm()\n","    \n","    current_level_nodes,lengths = get_next_nodes(prm_inputs,prm_scores,lengths)\n","    current_level += 1"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import pickle\n","with open(f\"../llmOutputs/PRM/data_V1_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(data_V, f)\n","with open(f\"../llmOutputs/PRM/data_pi1_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(data_pi, f)    \n","with open(f\"../llmOutputs/PRM/completed_paths_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# only needed when restart kernel\n","import pickle\n","with open(f\"../llmOutputs/PRM/completed_paths_code{version}.pickle\", \"rb\") as f:\n","    completed_paths = pickle.load(f)\n","import json\n","import pandas as pd\n","with open('../Data/AMC/aime_normal.json', 'r') as file:\n","    df = json.load(file)\n","# to have consistent format as in Kaggle\n","df = pd.DataFrame(df)#.iloc[:24]\n","df.rename(columns={'question': 'problem'}, inplace=True)\n","\n","def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<string>:1: SyntaxWarning: 'float' object is not subscriptable; perhaps you missed a comma?\n","<string>:1: SyntaxWarning: 'float' object is not subscriptable; perhaps you missed a comma?\n"]}],"source":["# run code to check if correct. Generate ground truth for training\n","ys = df.final_answer.apply(lambda x:int(x[0])).tolist()\n","import subprocess\n","import sys\n","timeout = 7\n","completed_paths_y = [] # (isCorrect,score,node,code,prob_i,exit_i)\n","\n","for i,(paths,y) in enumerate(zip(completed_paths,ys)):\n","    paths = [p for p in paths if p] # filter out None\n","    paths.sort(key=lambda x: x[0], reverse=True)\n","    for path in paths:# path (score,node)\n","        input = path[1]\n","        if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n","            input += \")\"\n","        splits = input.split('```')\n","        if len(splits) < 2:\n","            completed_paths_y.append([0,path[0],path[1],'no code',i,1])\n","            continue\n","        code = \"from sympy import *\\n\" + input.split('```')[1][7:] \n","        node = '```'.join(splits[:4]) # only return up to the first python code. later code/reason not relevant\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","        except subprocess.TimeoutExpired:\n","            completed_paths_y.append([0,path[0],node,code,i,2])\n","            continue\n","        if process.stderr:# code.py err\n","            completed_paths_y.append([0,path[0],node,code,i,3])\n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer):\n","                    completed_paths_y.append([int(int(answer)==y),path[0],node,code,i,4])\n","                    continue\n","                else:\n","                    completed_paths_y.append([0,path[0],node,code,i,5])\n","                    continue\n","            except:\n","                completed_paths_y.append([0,path[0],node,code,i,6])\n","                continue\n","                \n","with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths_y, f)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["0.8556014957621463"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# AUC for score and IsCorrect\n","import pandas as pd\n","data = pd.DataFrame(completed_paths_y,columns=['isCorrect','score','node','code','prob_i','exit_i'])\n","# !pip install scikit-learn\n","from sklearn.metrics import roc_auc_score\n","roc_auc_score(data.iloc[:,0].values,data.iloc[:,1].values)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["99.69643713053203"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["data.iloc[:,0].mean() * 975 # all completion correct% * 975 questions"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["279"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["data.groupby(['prob_i']).isCorrect.max().sum() # any completion correct #"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["1280"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["data.isCorrect.sum()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["exit_i\n","4    6385\n","3    2734\n","5    1391\n","6    1251\n","2     623\n","1     134\n","Name: count, dtype: int64"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["data.exit_i.value_counts() #(exit_i == 4) - data.isCorrect.sum() -> code run but wrong results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","single_line_comment_pattern = re.compile(r'(?<!\\\\)#.*')\n","multi_line_comment_pattern = re.compile(r'(\\'\\'\\'|\\\"\\\"\\\")(.*?)(\\'\\'\\'|\\\"\\\"\\\")', flags=re.DOTALL)\n","trailing_whitespace_pattern = re.compile(r'[ \\t]+$', flags=re.MULTILINE)\n","multiple_blank_lines_pattern = re.compile(r'\\n\\s*\\n')\n","def remove_python_comments(code):\n","    # Remove single-line comments\n","    code = single_line_comment_pattern.sub('', code)\n","    # Remove multi-line comments (docstrings)\n","    code = multi_line_comment_pattern.sub('', code)\n","    # Remove leading and trailing whitespace from each line\n","    code = trailing_whitespace_pattern.sub('', code)\n","    # Reduce multiple blank lines to a single blank line\n","    code = multiple_blank_lines_pattern.sub('\\n', code)\n","    return code\n","\n","def repl(match):\n","    if \"real\" not in match.group():\n","        return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","    else:\n","        return \"{}{}\".format(match.group()[:-1], ')')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# calculate correct%\n","timeout = 7\n","len_limit = 49\n","\n","def agg_code(paths):\n","    paths = [p for p in paths if p]\n","    paths.sort(key=lambda x: x[0], reverse=True)\n","    for path in paths:# path (score,node)\n","        input = path[1]\n","        if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n","            input += \")\"\n","        splits = input.split('```')\n","        if len(splits) < 2:\n","            continue\n","        code = \"from sympy import *\\n\" + input.split('```')[1][7:]\n","        if len(code) < len_limit: continue # ignore very short answer\n","\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","        except subprocess.TimeoutExpired:\n","            continue\n","        if process.stderr:# code.py err\n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer) and is_between_0_and_999(answer):\n","                    return int(answer)\n","                else:\n","                    continue\n","            except:\n","                continue\n","    return 37"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<string>:1: SyntaxWarning: 'float' object is not subscriptable; perhaps you missed a comma?\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 4.25 s, sys: 8min 41s, total: 8min 45s\n","Wall time: 1h 11min 59s\n"]}],"source":["%%time\n","yhat = []\n","for paths in completed_paths:\n","    yhat.append(agg_code(paths))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["202"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["ys = df.final_answer.apply(lambda x:int(x[0])).tolist()\n","sum([y==yh for y,yh in zip(ys,yhat)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8300737,"sourceType":"datasetVersion"},{"datasetId":5036020,"sourceId":8450555,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":571.271793,"end_time":"2024-05-18T17:09:54.761042","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-18T17:00:23.489249","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2770dfd3b1aa4c489056caf28ff0eb19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d51b8465cf24584bcefbd8de22eb8b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2770dfd3b1aa4c489056caf28ff0eb19","placeholder":"​","style":"IPY_MODEL_71347b6fde884c478ad4c29e00602bd8","value":" 3/3 [03:12&lt;00:00, 60.64s/it]"}},"39821c375ddf49ea9e6efb9c09fc0542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a13122cbc4884a7ba1e4a7bf9c6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167f1d4ccc44aee846ccde523d2cc9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7cd8d4fb35847f08de65bd2e42ceffe","IPY_MODEL_7459d3b6efa74b03ac047eac61c9487f","IPY_MODEL_2d51b8465cf24584bcefbd8de22eb8b1"],"layout":"IPY_MODEL_412a13122cbc4884a7ba1e4a7bf9c6cf"}},"71347b6fde884c478ad4c29e00602bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7459d3b6efa74b03ac047eac61c9487f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f5fe4bfeb44f1e84c533bc8b8f2da8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39821c375ddf49ea9e6efb9c09fc0542","value":3}},"78f5fe4bfeb44f1e84c533bc8b8f2da8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7cd8d4fb35847f08de65bd2e42ceffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e209cca0c4874c2bb25ebb79bd84ae28","placeholder":"​","style":"IPY_MODEL_ce7ea779817a4237af29446e91448882","value":"Loading checkpoint shards: 100%"}},"ce7ea779817a4237af29446e91448882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e209cca0c4874c2bb25ebb79bd84ae28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
