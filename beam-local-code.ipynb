{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:03:03.749818Z","iopub.status.busy":"2024-05-18T17:03:03.749074Z","iopub.status.idle":"2024-05-18T17:08:21.725719Z","shell.execute_reply":"2024-05-18T17:08:21.724832Z"},"papermill":{"duration":317.985165,"end_time":"2024-05-18T17:08:21.727692","exception":false,"start_time":"2024-05-18T17:03:03.742527","status":"completed"},"tags":[]},"outputs":[],"source":["from vllm import LLM, SamplingParams\n","LOCAL = True\n","MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","from functions import *\n","dtype = 'auto'\n","gpu_memory_utilization = 0.95\n","\n","import torch\n","import pandas as pd\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["version = \"2\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 05-28 16:39:47 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 05-28 16:39:47 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 16:39:47 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["/home/zhenlan/anaconda3/envs/vllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 16:39:48 selector.py:16] Using FlashAttention backend.\n","INFO 05-28 16:39:48 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 16:39:56 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 05-28 16:39:57 gpu_executor.py:94] # GPU blocks: 2321, # CPU blocks: 2184\n"]}],"source":["llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","\n","# stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n","stop_words = [tokenizer.eos_token,\"```output\",\"```Output\",\"```output\\n\",\"```Output\\n\",\"```\\nOutput\" , \")\\n```\" , \"``````output\"]\n","# stop_words.append(\"\\n\")\n","sampling_params = SamplingParams(temperature=1,\n","                                 max_tokens=256,\n","                                #  min_tokens=32,\n","                                 stop=stop_words,\n","                                 include_stop_str_in_output=True)\n","\n","\n","def gen_prompt_codeIn1(problem):\n","    return f\"\"\"\n","Problem:{problem}\\n\n","Let's reason step by step and write Python (sympy) code to solve the problem, using brute force enumeration if necessary. \n","Limit the search space using logical deductions. The code should be enclosed between ```python\\n actual code...``` and should only print the final answer.\n","\"\"\"\n","\n","n = 3 # beams\n","n_sol = 7\n","samples = 7\n","max_depth = 16\n","max_pct = 0.8"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44c67c6ed64745afa58511854ba51017","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge3_code and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import LlamaForSequenceClassification\n","prm_tokenizer = tokenizer\n","prm_model = LlamaForSequenceClassification.from_pretrained('../Model/PRM_LORA_merge3_code',\\\n","                                                    num_labels=1,\\\n","                                                    device_map=\"cpu\",\n","                                                    torch_dtype=\"auto\",\n","                                                    ).eval()\n","base_model = prm_model.model\n","prm_model.score.load_state_dict(torch.load('../Model/model_score3_code.pth'))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import json\n","with open('../Data/AMC/aime_normal.json', 'r') as file:\n","    df = json.load(file)\n","# to have consistent format as in Kaggle\n","df = pd.DataFrame(df)#.iloc[:24]\n","df.rename(columns={'question': 'problem'}, inplace=True)\n","# import pandas as pd\n","# df = pd.read_csv('../Data/AMC/cleaned_ArtOfProblemSolving.csv').iloc[:20]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        query_prompt = gen_prompt_codeIn1(problem)\n","        messages = [{\"role\": \"user\",\"content\": query_prompt}]\n","        input = tokenizer.apply_chat_template(messages, tokenize=False)\n","        outs.append(input)\n","    return outs"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:08:23.262621Z","iopub.status.busy":"2024-05-18T17:08:23.262138Z","iopub.status.idle":"2024-05-18T17:08:23.268774Z","shell.execute_reply":"2024-05-18T17:08:23.267916Z"},"papermill":{"duration":0.016196,"end_time":"2024-05-18T17:08:23.270775","exception":false,"start_time":"2024-05-18T17:08:23.254579","status":"completed"},"tags":[]},"outputs":[],"source":["logit2prob = lambda x: 1/(1+np.exp(-x))\n","def eval_prm(candidates):\n","    all_log_probs = []\n","    for i in range(len(candidates)):\n","        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda\")\n","        with torch.no_grad():\n","            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n","            logits = prm_model.score(hidden_states)[0]\n","        all_log_probs.append(logit2prob(logits.item()))\n","    return all_log_probs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'[Tt]he answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"[Tt]he answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"[Tt]he answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def group_and_sum(A, B):\n","    '''\n","    A = ['a','b','a']\n","    B = [1,2,3]\n","    -> {'a': 4, 'b': 2}\n","    '''\n","    result_dict = {}\n","    for a, b in zip(A, B):\n","        if a in result_dict:\n","            result_dict[a] += b\n","        else:\n","            result_dict[a] = b\n","    return result_dict\n","\n","def group_and_average(A, B):\n","    from collections import defaultdict\n","    # Create a dictionary to store sums and counts for averaging\n","    sum_dict = defaultdict(lambda: [0, 0])  # Each key maps to [sum, count]\n","    # Pair elements from A and B and aggregate sums and counts\n","    for key, value in zip(A, B):\n","        sum_dict[key][0] += value\n","        sum_dict[key][1] += 1\n","    # Calculate averages\n","    averages = {key: sum_count[0] / sum_count[1] for key, sum_count in sum_dict.items()}\n","    return averages,[averages[a] for a in A]\n","\n","def max_dict(d):\n","    return max(d.items(), key=lambda x: x[1])[0]\n","\n","def tot_agg(completed_paths):\n","    if completed_paths:\n","        answers,scores = zip(*completed_paths)\n","        groups = group_and_sum(answers, scores)\n","        return max_dict(groups)\n","    else:\n","        return 37 # empty completed_paths\n","    \n","def repeat_elements(lst, k):\n","    return [i for i in lst for _ in range(k)]\n","\n","def flatten(nested_list):\n","    \"\"\"Flatten a nested list.\"\"\"\n","    out = []\n","    lengths = []\n","    for sublist in nested_list:\n","        lengths.append(len(sublist))\n","        for item in sublist:\n","            out.append(item)\n","    return out,lengths\n","\n","def unflatten(flat_list, lengths):\n","    \"\"\"Unflatten a flat list into a nested list based on lengths.\"\"\"\n","    nested_list = []\n","    index = 0\n","    for length in lengths:\n","        nested_list.append(flat_list[index:index + length])\n","        index += length\n","    return nested_list\n","\n","def filter_input(batch_response,current_level_node):\n","    # one question filter\n","    prm_inputs = []\n","    parents = []\n","    for candidate,parent in zip(batch_response,current_level_node):\n","        if candidate.outputs[0].text not in parent:\n","            prm_input = parent + candidate.outputs[0].text\n","            prm_inputs.append(prm_input)\n","            parents.append(parent)\n","    # Get the indices of unique elements in prm_inputs\n","    unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n","    prm_inputs = [prm_inputs[i] for i in unique_indices]\n","    parents = [parents[i] for i in unique_indices]\n","    return prm_inputs,parents,len(prm_inputs)\n","\n","def filter_inputs(batch_responses,current_level_nodes,lengths):\n","    # all question filter\n","    # returned value should be flattened\n","    batch_responses,current_level_nodes = unflatten(batch_responses,lengths),unflatten(current_level_nodes,lengths)\n","    prm_inputs = []\n","    lengths = []\n","    parent_list = []\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(batch_responses) == len(uncompleted)\n","    for batch_response,current_level_node,path in zip(batch_responses,current_level_nodes,uncompleted):\n","        prm_input,parents,length = filter_input(batch_response,current_level_node)\n","        if length == 0:# all bad\n","            while len(path)<n_sol:\n","                # make complete for this question as there will be no continued effort\n","                path.append(None)\n","        else:\n","            prm_inputs.extend(prm_input)\n","            parent_list.extend(parents)\n","            lengths.append(length)\n","    return prm_inputs,parent_list,lengths\n","\n","def IsFinished(node):\n","    matches = re.findall(r'print\\(([^)]*)\\)', node)\n","    return len(matches)>0\n","\n","def get_next_node(prm_inputs,prm_scores,completed_paths):\n","    # need to update completed_paths in-place\n","    next_level_nodes = []\n","    combined = list(zip(prm_inputs,prm_scores))\n","    combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n","    max_score = combined[0][1]\n","    for node,score in combined:\n","        finish = IsFinished(node)\n","        if finish: # finished\n","            completed_paths.append((score,node))\n","        else: # not inished\n","            if len(next_level_nodes) < n:\n","                next_level_nodes.append(node)\n","    return next_level_nodes\n","\n","\n","def get_next_nodes(prm_inputs,prm_scores,lengths):\n","    # for completed_paths, next_level_nodes would be removed\n","    # returned value should be flattened\n","    prm_inputs,prm_scores = unflatten(prm_inputs,lengths),unflatten(prm_scores,lengths)\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(uncompleted) == len(lengths)\n","    assert len(prm_inputs) == len(lengths)\n","    assert len(prm_scores) == len(lengths)\n","    next_level_nodes,lengths = [],[]\n","    for prm_input,prm_score,completed_path in zip(prm_inputs,prm_scores,uncompleted):\n","        next_node = get_next_node(prm_input,prm_score,completed_path)\n","        if len(completed_path) < n_sol:\n","            next_level_nodes.extend(next_node)\n","            lengths.append(len(next_node))\n","    return next_level_nodes,lengths\n","\n","import gc\n","def create_llm():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","    tokenizer = llm.get_tokenizer()\n","    return llm,tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 6825/6825 [12:30<00:00,  9.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 16:58:54 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 16:58:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["/home/zhenlan/anaconda3/envs/vllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 16:58:54 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 16:58:55 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 16:58:56 gpu_executor.py:94] # GPU blocks: 2376, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 18935/18935 [41:49<00:00,  7.55it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:01:54 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:01:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:01:55 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:01:56 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:01:57 gpu_executor.py:94] # GPU blocks: 2374, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 4067/4067 [12:40<00:00,  5.35it/s] \n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:20:33 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:20:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:20:34 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:20:35 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:20:35 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  20%|█▉        | 235/1190 [01:09<07:30,  2.12it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:21:47 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 1190/1190 [05:39<00:00,  3.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:28:40 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:28:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:28:40 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:28:42 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:28:42 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 525/525 [03:13<00:00,  2.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:33:16 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:33:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:33:16 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:33:17 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:33:18 gpu_executor.py:94] # GPU blocks: 2373, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 357/357 [02:34<00:00,  2.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:36:58 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:36:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:36:58 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:37:00 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:37:00 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 231/231 [01:45<00:00,  2.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:39:30 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:39:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:39:30 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:39:32 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:39:32 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 0/154 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:33 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  14%|█▍        | 22/154 [00:04<00:26,  4.94it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  42%|████▏     | 65/154 [00:07<00:10,  8.82it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:39:40 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 154/154 [00:23<00:00,  6.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:40:12 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:40:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:40:12 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:40:14 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:40:14 gpu_executor.py:94] # GPU blocks: 2376, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 0/63 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-28 18:40:14 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 63/63 [00:00<00:00, 4177.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:40:20 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-28 18:40:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-28 18:40:20 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-28 18:40:21 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-28 18:40:22 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]}],"source":["# [path for path in completed_paths if len(path)<n_sol] used to track on-going questiones\n","# flattened inputs, with lengths corresponds to the uncompleted path\n","# two ways for path to complete, one is via get_next_nodes getting n_sol answers\n","# two is via filter_inputs, all continuations are bad\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","current_level_nodes = process_inputs(df.problem.tolist())\n","lengths = [1] * len(current_level_nodes)\n","current_level = 1\n","completed_paths = [[] for _ in current_level_nodes]\n","data_V = []\n","data_pi = []\n","\n","while (current_level < max_depth) and (current_level_nodes):\n","    # everything at this level is flattened\n","    current_level_nodes = repeat_elements(current_level_nodes,samples)\n","    lengths = [l*samples for l in lengths]\n","    batch_responses = llm.generate(current_level_nodes, sampling_params)\n","    prm_inputs,parent_list,lengths = filter_inputs(batch_responses,current_level_nodes,lengths)\n","    \n","    # release VRAM to prm_model\n","    del llm\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    prm_model.to('cuda')\n","    prm_scores = eval_prm(prm_inputs)\n","    \n","    # save for Q-learning\n","    averages,averages_dup = group_and_average(parent_list,prm_scores)\n","    data_V.extend(list(averages.items()))\n","    advantages = [q-v for q,v in zip(prm_scores,averages_dup)]\n","    data_pi.extend(list(zip(prm_inputs,advantages)))\n","    \n","    # release VRAM to llm\n","    prm_model.to('cpu')\n","    llm,tokenizer = create_llm()\n","    \n","    current_level_nodes,lengths = get_next_nodes(prm_inputs,prm_scores,lengths)\n","    current_level += 1"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import pickle\n","with open(f\"../llmOutputs/PRM/data_V1_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(data_V, f)\n","with open(f\"../llmOutputs/PRM/data_pi1_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(data_pi, f)    \n","with open(f\"../llmOutputs/PRM/completed_paths_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths, f)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# timeout err\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mTimeoutExpired:\n\u001b[1;32m     26\u001b[0m     completed_paths_y\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m,path[\u001b[38;5;241m0\u001b[39m],node])\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    503\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/subprocess.py:1770\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1768\u001b[0m     fds_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(pass_fds)\n\u001b[1;32m   1769\u001b[0m     fds_to_keep\u001b[38;5;241m.\u001b[39madd(errpipe_write)\n\u001b[0;32m-> 1770\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43m_posixsubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfds_to_keep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrpipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrpipe_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_child_created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;66;03m# be sure the FD is closed no matter what\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# run code to check if correct. Generate ground truth for training\n","ys = df.final_answer.apply(lambda x:int(x[0])).tolist()\n","import subprocess\n","import sys\n","timeout = 7\n","completed_paths_y = [] # (isCorrect,score,node)\n","for paths,y in zip(completed_paths,ys):\n","    paths = [p for p in paths if p] # filter out None\n","    for path in paths:# path (score,node)\n","        input = path[1]\n","        if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n","            input += \")\"\n","        splits = input.split('```')\n","        if len(splits) <= 3: # 3 bc prompt include two ```\n","            completed_paths_y.append([0,path[0],path[1]])\n","            continue\n","        code = \"from sympy import *\\n\" + input.split('```')[3][7:] \n","        node = '```'.join(splits[:4]) # only return up to the first python code. later code/reason not relevant\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","        except subprocess.TimeoutExpired:\n","            completed_paths_y.append([0,path[0],node])\n","            continue\n","        if process.stderr:# code.py err\n","            completed_paths_y.append([0,path[0],node])\n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer):\n","                    completed_paths_y.append([int(int(answer)==y),path[0],node])\n","                    continue\n","                else:\n","                    completed_paths_y.append([0,path[0],node])\n","                    continue\n","            except:\n","                completed_paths_y.append([0,path[0],node])\n","                continue\n","                \n","with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths_y, f)                "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0.6106624122491934"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["# AUC for score and IsCorrect\n","import pandas as pd\n","data = pd.DataFrame(completed_paths_y)\n","# !pip install scikit-learn\n","from sklearn.metrics import roc_auc_score\n","roc_auc_score(data.iloc[:,0].values,data.iloc[:,1].values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# only needed when restart kernel\n","import pickle\n","with open(\"../llmOutputs/PRM/completed_paths_code.pickle\", \"rb\") as f:\n","    completed_paths = pickle.load(f)\n","\n","def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# calculate correct%\n","import subprocess\n","import sys\n","timeout = 7\n","len_limit = 49\n","\n","def agg_code(paths):\n","    paths = [p for p in paths if p]\n","    paths.sort(key=lambda x: x[0], reverse=True)\n","    for path in paths:# path (score,node)\n","        input = path[1]\n","        if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n","            input += \")\"\n","        splits = input.split('```')\n","        if len(splits) <= 3: # 3 bc prompt include two ```\n","            continue\n","        code = \"from sympy import *\\n\" + input.split('```')[3][7:] \n","        if len(code) < len_limit: continue # ignore very short answer\n","\n","        # execute code\n","        with open('code.py', 'w') as fout:\n","            fout.write(code)\n","        # timeout err\n","        try:\n","            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n","        except subprocess.TimeoutExpired:\n","            continue\n","        if process.stderr:# code.py err\n","            continue\n","        else:\n","            stdout = process.stdout.decode('utf8')\n","            try:\n","                answer = eval(stdout)\n","                if is_integer(answer) and is_between_0_and_999(answer):\n","                    return int(answer)\n","                else:\n","                    continue\n","            except:\n","                continue\n","    return 37"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["yhat = []\n","for paths in completed_paths:\n","    yhat.append(agg_code(paths))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["151"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["ys = df.final_answer.apply(lambda x:int(x[0])).tolist()\n","sum([y==yh for y,yh in zip(ys,yhat)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8300737,"sourceType":"datasetVersion"},{"datasetId":5036020,"sourceId":8450555,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":571.271793,"end_time":"2024-05-18T17:09:54.761042","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-18T17:00:23.489249","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2770dfd3b1aa4c489056caf28ff0eb19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d51b8465cf24584bcefbd8de22eb8b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2770dfd3b1aa4c489056caf28ff0eb19","placeholder":"​","style":"IPY_MODEL_71347b6fde884c478ad4c29e00602bd8","value":" 3/3 [03:12&lt;00:00, 60.64s/it]"}},"39821c375ddf49ea9e6efb9c09fc0542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a13122cbc4884a7ba1e4a7bf9c6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167f1d4ccc44aee846ccde523d2cc9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7cd8d4fb35847f08de65bd2e42ceffe","IPY_MODEL_7459d3b6efa74b03ac047eac61c9487f","IPY_MODEL_2d51b8465cf24584bcefbd8de22eb8b1"],"layout":"IPY_MODEL_412a13122cbc4884a7ba1e4a7bf9c6cf"}},"71347b6fde884c478ad4c29e00602bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7459d3b6efa74b03ac047eac61c9487f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f5fe4bfeb44f1e84c533bc8b8f2da8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39821c375ddf49ea9e6efb9c09fc0542","value":3}},"78f5fe4bfeb44f1e84c533bc8b8f2da8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7cd8d4fb35847f08de65bd2e42ceffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e209cca0c4874c2bb25ebb79bd84ae28","placeholder":"​","style":"IPY_MODEL_ce7ea779817a4237af29446e91448882","value":"Loading checkpoint shards: 100%"}},"ce7ea779817a4237af29446e91448882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e209cca0c4874c2bb25ebb79bd84ae28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
