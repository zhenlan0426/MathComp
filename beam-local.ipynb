{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:03:03.749818Z","iopub.status.busy":"2024-05-18T17:03:03.749074Z","iopub.status.idle":"2024-05-18T17:08:21.725719Z","shell.execute_reply":"2024-05-18T17:08:21.724832Z"},"papermill":{"duration":317.985165,"end_time":"2024-05-18T17:08:21.727692","exception":false,"start_time":"2024-05-18T17:03:03.742527","status":"completed"},"tags":[]},"outputs":[],"source":["from vllm import LLM, SamplingParams\n","LOCAL = True\n","MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","from functions import *\n","dtype = 'auto'\n","gpu_memory_utilization = 0.95\n","\n","import torch\n","import pandas as pd\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:25:51 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 05-21 12:25:51 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:25:51 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:25:52 selector.py:16] Using FlashAttention backend.\n","INFO 05-21 12:25:53 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:25:54 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 05-21 12:25:54 gpu_executor.py:94] # GPU blocks: 2313, # CPU blocks: 2184\n"]}],"source":["llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","\n","stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n","stop_words.append(\"\\n\")\n","sampling_params = SamplingParams(temperature=1,\n","                                 max_tokens=256,\n","                                 min_tokens=32,\n","                                 stop=stop_words)\n","\n","cot_instruction = \"\\nYou are an expert at mathematical reasoning. Please reason step by step, and put your final answer within \\\\boxed{}. The answer should be an interger between 0 and 999.\"\n","\n","\n","n = 3 # beams\n","n_sol = 3\n","samples = 7\n","max_depth = 24\n","max_pct = 0.8"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26fc6ab281994515866bbc994ddcac34","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import LlamaForSequenceClassification\n","prm_tokenizer = tokenizer\n","prm_model = LlamaForSequenceClassification.from_pretrained('../Model/PRM_LORA_merge2',\\\n","                                                    num_labels=1,\\\n","                                                    device_map=\"cpu\",\n","                                                    torch_dtype=\"auto\",\n","                                                    ).eval()\n","base_model = prm_model.model\n","prm_model.score.load_state_dict(torch.load('../Model/model_score2.pth'))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import json\n","with open('../Data/AMC/aime_normal.json', 'r') as file:\n","    df = json.load(file)\n","# to have consistent format as in Kaggle\n","df = pd.DataFrame(df).iloc[:24]\n","df.rename(columns={'question': 'problem'}, inplace=True)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        query_prompt = problem + cot_instruction\n","        messages = [{\"role\": \"user\",\"content\": query_prompt}]\n","        input = tokenizer.apply_chat_template(messages, tokenize=False)\n","        outs.append(input)\n","    return outs"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:08:23.262621Z","iopub.status.busy":"2024-05-18T17:08:23.262138Z","iopub.status.idle":"2024-05-18T17:08:23.268774Z","shell.execute_reply":"2024-05-18T17:08:23.267916Z"},"papermill":{"duration":0.016196,"end_time":"2024-05-18T17:08:23.270775","exception":false,"start_time":"2024-05-18T17:08:23.254579","status":"completed"},"tags":[]},"outputs":[],"source":["logit2prob = lambda x: 1/(1+np.exp(-x))\n","def eval_prm(candidates):\n","    all_log_probs = []\n","    for i in range(len(candidates)):\n","        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda\")\n","        with torch.no_grad():\n","            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n","            logits = prm_model.score(hidden_states)[0]\n","        all_log_probs.append(logit2prob(logits.item()))\n","    return all_log_probs"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"The answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def group_and_sum(A, B):\n","    '''\n","    A = ['a','b','a']\n","    B = [1,2,3]\n","    -> {'a': 4, 'b': 2}\n","    '''\n","    result_dict = {}\n","    for a, b in zip(A, B):\n","        if a in result_dict:\n","            result_dict[a] += b\n","        else:\n","            result_dict[a] = b\n","    return result_dict\n","\n","def group_and_average(A, B):\n","    from collections import defaultdict\n","    # Create a dictionary to store sums and counts for averaging\n","    sum_dict = defaultdict(lambda: [0, 0])  # Each key maps to [sum, count]\n","    # Pair elements from A and B and aggregate sums and counts\n","    for key, value in zip(A, B):\n","        sum_dict[key][0] += value\n","        sum_dict[key][1] += 1\n","    # Calculate averages\n","    averages = {key: sum_count[0] / sum_count[1] for key, sum_count in sum_dict.items()}\n","    return averages\n","\n","def max_dict(d):\n","    return max(d.items(), key=lambda x: x[1])[0]\n","\n","def tot_agg(completed_paths):\n","    answers,scores,_ = zip(*completed_paths)\n","    if answers:\n","        groups = group_and_sum(answers, scores)\n","        return max_dict(groups)\n","    else:\n","        return 37 # empty completed_paths\n","    \n","def repeat_elements(lst, k):\n","    return [i for i in lst for _ in range(k)]\n","\n","def flatten(nested_list):\n","    \"\"\"Flatten a nested list.\"\"\"\n","    out = []\n","    lengths = []\n","    for sublist in nested_list:\n","        lengths.append(len(sublist))\n","        for item in sublist:\n","            out.append(item)\n","    return out,lengths\n","\n","def unflatten(flat_list, lengths):\n","    \"\"\"Unflatten a flat list into a nested list based on lengths.\"\"\"\n","    nested_list = []\n","    index = 0\n","    for length in lengths:\n","        nested_list.append(flat_list[index:index + length])\n","        index += length\n","    return nested_list\n","\n","def filter_input(batch_response,current_level_node):\n","    # one question filter\n","    prm_inputs = []\n","    parents = []\n","    for candidate,parent in zip(batch_response,current_level_node):\n","        if candidate.outputs[0].text not in parent:\n","            prm_input = parent + candidate.outputs[0].text\n","            prm_inputs.append(prm_input)\n","            parents.append(parent)\n","    # Get the indices of unique elements in prm_inputs\n","    unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n","    prm_inputs = [prm_inputs[i] for i in unique_indices]\n","    parents = [parents[i] for i in unique_indices]\n","    return prm_inputs,parents,len(prm_inputs)\n","\n","def filter_inputs(batch_responses,current_level_nodes,lengths):\n","    # all question filter\n","    # returned value should be flattened\n","    batch_responses,current_level_nodes = unflatten(batch_responses,lengths),unflatten(current_level_nodes,lengths)\n","    prm_inputs = []\n","    lengths = []\n","    parent_list = []\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(batch_responses) == len(uncompleted)\n","    for batch_response,current_level_node,path in zip(batch_responses,current_level_nodes,uncompleted):\n","        prm_input,parents,length = filter_input(batch_response,current_level_node)\n","        if length == 0:# all bad\n","            while len(path)<n_sol:\n","                # make complete for this question as there will be no continued effort\n","                path.append(None)\n","        else:\n","            prm_inputs.extend(prm_input)\n","            parent_list.extend(parents)\n","            lengths.append(length)\n","    return prm_inputs,parent_list,lengths\n","\n","def get_next_node(prm_inputs,prm_scores,completed_paths):\n","    # need to update completed_paths in-place\n","    next_level_nodes = []\n","    combined = list(zip(prm_inputs,prm_scores))\n","    combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n","    max_score = combined[0][1]\n","    for node,score in combined:\n","        answer = extract_number(node)\n","        if answer == 'parse err': # not finished\n","            if len(next_level_nodes) < n:\n","                next_level_nodes.append(node)\n","        else: # finished\n","            if score > max_score * max_pct:\n","                try:\n","                    answer = eval(answer)\n","                    if is_integer(answer) and is_between_0_and_999(answer):# correct format\n","                        completed_paths.append((answer,score,node))\n","                except: # bad eval\n","                    continue\n","    return next_level_nodes\n","\n","def get_next_nodes(prm_inputs,prm_scores,lengths):\n","    # for completed_paths, next_level_nodes would be removed\n","    # returned value should be flattened\n","    prm_inputs,prm_scores = unflatten(prm_inputs,lengths),unflatten(prm_scores,lengths)\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(uncompleted) == len(lengths)\n","    assert len(prm_inputs) == len(lengths)\n","    assert len(prm_scores) == len(lengths)\n","    next_level_nodes,lengths = [],[]\n","    for prm_input,prm_score,completed_path in zip(prm_inputs,prm_scores,uncompleted):\n","        next_node = get_next_node(prm_input,prm_score,completed_path)\n","        if len(completed_path) < n_sol:\n","            next_level_nodes.extend(next_node)\n","            lengths.append(len(next_node))\n","    return next_level_nodes,lengths\n","\n","import gc\n","def create_llm():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","    tokenizer = llm.get_tokenizer()\n","    return llm,tokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 168/168 [00:09<00:00, 17.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:26:19 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:26:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:26:19 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:26:20 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:26:21 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 504/504 [00:31<00:00, 15.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:27:21 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:27:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:27:21 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:27:23 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:27:23 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 504/504 [00:39<00:00, 12.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:28:36 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:28:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:28:36 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:28:38 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:28:38 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 483/483 [00:44<00:00, 10.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:29:54 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:29:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:29:55 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:29:56 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:29:56 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 420/420 [00:39<00:00, 10.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:31:06 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:31:06 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:31:06 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:31:08 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:31:08 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 399/399 [00:40<00:00,  9.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:32:19 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:32:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:32:19 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:32:21 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:32:21 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 357/357 [00:45<00:00,  7.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:33:40 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:33:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:33:40 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:33:41 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:33:42 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 315/315 [00:40<00:00,  7.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:34:50 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:34:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:34:50 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:34:52 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:34:52 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  25%|██▌       | 80/315 [00:14<00:27,  8.60it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:35:07 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 315/315 [00:41<00:00,  7.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:36:02 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:36:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:36:02 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:36:04 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:36:04 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 294/294 [00:40<00:00,  7.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:37:12 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:37:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:37:12 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:37:14 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:37:14 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 294/294 [00:45<00:00,  6.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:38:33 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:38:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:38:33 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:38:34 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:38:34 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 294/294 [00:50<00:00,  5.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:40:03 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:40:03 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:40:03 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:40:05 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:40:05 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  11%|█         | 28/252 [00:10<00:51,  4.34it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:40:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 252/252 [00:42<00:00,  5.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:41:20 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:41:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:41:21 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:41:22 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:41:22 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  15%|█▍        | 34/231 [00:11<00:56,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:41:34 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 231/231 [00:37<00:00,  6.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:42:27 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:42:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:42:27 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:42:28 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:42:28 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 231/231 [00:40<00:00,  5.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:43:35 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:43:35 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:43:35 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:43:37 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:43:37 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  17%|█▋        | 39/231 [00:12<00:48,  4.00it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:43:49 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 231/231 [00:38<00:00,  5.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:44:40 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:44:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:44:40 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:44:42 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:44:42 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 0/231 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:44 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 1/231 [00:01<06:38,  1.73s/it]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:45 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  25%|██▌       | 58/231 [00:12<00:38,  4.47it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:44:55 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 231/231 [00:32<00:00,  7.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:45:39 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:45:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:45:39 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:45:41 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:45:41 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|          | 0/189 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 12:45:42 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 189/189 [00:22<00:00,  8.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:46:21 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:46:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:46:21 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:46:23 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:46:23 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 147/147 [00:25<00:00,  5.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:47:09 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:47:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:47:09 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:47:11 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:47:11 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 147/147 [00:26<00:00,  5.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:47:59 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:47:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:47:59 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:48:00 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:48:00 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 147/147 [00:26<00:00,  5.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:48:49 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:48:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:48:49 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:48:50 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:48:51 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 147/147 [00:26<00:00,  5.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:49:41 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:49:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:49:42 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:49:43 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:49:43 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 147/147 [00:26<00:00,  5.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:50:31 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 12:50:31 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 12:50:31 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 12:50:32 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 12:50:32 gpu_executor.py:94] # GPU blocks: 2377, # CPU blocks: 2184\n"]}],"source":["# [path for path in completed_paths if len(path)<n_sol] used to track on-going questiones\n","# flattened inputs, with lengths corresponds to the uncompleted path\n","# two ways for path to complete, one is via get_next_nodes getting n_sol answers\n","# two is via filter_inputs, all continuations are bad\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","current_level_nodes = process_inputs(df.problem.tolist())\n","lengths = [1] * len(current_level_nodes)\n","current_level = 1\n","completed_paths = [[] for _ in current_level_nodes]\n","data_out = []\n","\n","while (current_level < max_depth) and (current_level_nodes):\n","    # everything at this level is flattened\n","    current_level_nodes = repeat_elements(current_level_nodes,samples)\n","    lengths = [l*samples for l in lengths]\n","    batch_responses = llm.generate(current_level_nodes, sampling_params)\n","    prm_inputs,parent_list,lengths = filter_inputs(batch_responses,current_level_nodes,lengths)\n","    \n","    # release VRAM to prm_model\n","    del llm\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    prm_model.to('cuda')\n","    prm_scores = eval_prm(prm_inputs)\n","    \n","    # save for Q-learning\n","    data = list(group_and_average(parent_list,prm_scores).items())\n","    data_out.extend(data)\n","    \n","    # release VRAM to llm\n","    prm_model.to('cpu')\n","    llm,tokenizer = create_llm()\n","    \n","    current_level_nodes,lengths = get_next_nodes(prm_inputs,prm_scores,lengths)\n","    current_level += 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = []\n","for paths in completed_paths:\n","    if paths and paths[0]: # not empty or not None\n","        out.append(tot_agg(paths))\n","    else:\n","        out.append(float('-inf'))\n","print(f\"correct {sum([i==j for i,j in zip([int(i[0]) for i in df.final_answer.tolist()],out)])/len(out)}%\")"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["import pickle\n","with open(\"../llmOutputs/PRM/data_out.pickle\", \"wb\") as f:\n","    pickle.dump(data_out, f)\n","with open(\"../llmOutputs/PRM/completed_paths.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths, f)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# with open(\"../llmOutputs/PRM/completed_paths.pickle\", \"rb\") as f:\n","#     completed_paths = pickle.load(f)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8300737,"sourceType":"datasetVersion"},{"datasetId":5036020,"sourceId":8450555,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":571.271793,"end_time":"2024-05-18T17:09:54.761042","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-18T17:00:23.489249","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2770dfd3b1aa4c489056caf28ff0eb19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d51b8465cf24584bcefbd8de22eb8b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2770dfd3b1aa4c489056caf28ff0eb19","placeholder":"​","style":"IPY_MODEL_71347b6fde884c478ad4c29e00602bd8","value":" 3/3 [03:12&lt;00:00, 60.64s/it]"}},"39821c375ddf49ea9e6efb9c09fc0542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a13122cbc4884a7ba1e4a7bf9c6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167f1d4ccc44aee846ccde523d2cc9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7cd8d4fb35847f08de65bd2e42ceffe","IPY_MODEL_7459d3b6efa74b03ac047eac61c9487f","IPY_MODEL_2d51b8465cf24584bcefbd8de22eb8b1"],"layout":"IPY_MODEL_412a13122cbc4884a7ba1e4a7bf9c6cf"}},"71347b6fde884c478ad4c29e00602bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7459d3b6efa74b03ac047eac61c9487f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f5fe4bfeb44f1e84c533bc8b8f2da8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39821c375ddf49ea9e6efb9c09fc0542","value":3}},"78f5fe4bfeb44f1e84c533bc8b8f2da8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7cd8d4fb35847f08de65bd2e42ceffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e209cca0c4874c2bb25ebb79bd84ae28","placeholder":"​","style":"IPY_MODEL_ce7ea779817a4237af29446e91448882","value":"Loading checkpoint shards: 100%"}},"ce7ea779817a4237af29446e91448882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e209cca0c4874c2bb25ebb79bd84ae28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
