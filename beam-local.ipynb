{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:03:03.749818Z","iopub.status.busy":"2024-05-18T17:03:03.749074Z","iopub.status.idle":"2024-05-18T17:08:21.725719Z","shell.execute_reply":"2024-05-18T17:08:21.724832Z"},"papermill":{"duration":317.985165,"end_time":"2024-05-18T17:08:21.727692","exception":false,"start_time":"2024-05-18T17:03:03.742527","status":"completed"},"tags":[]},"outputs":[],"source":["from vllm import LLM, SamplingParams\n","LOCAL = True\n","MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n","from functions import *\n","dtype = 'auto'\n","gpu_memory_utilization = 0.95\n","\n","import torch\n","import pandas as pd\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 05-21 19:45:38 utils.py:253] CUDA_HOME is not found in the environment. Using /usr/local/cuda as CUDA_HOME.\n","INFO 05-21 19:45:38 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 19:45:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 19:45:39 selector.py:16] Using FlashAttention backend.\n","INFO 05-21 19:45:39 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 19:45:45 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 05-21 19:45:46 gpu_executor.py:94] # GPU blocks: 2322, # CPU blocks: 2184\n"]}],"source":["llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","tokenizer = llm.get_tokenizer()\n","\n","stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n","stop_words.append(\"\\n\")\n","sampling_params = SamplingParams(temperature=1,\n","                                 max_tokens=256,\n","                                 min_tokens=32,\n","                                 stop=stop_words)\n","\n","cot_instruction = \"\\nYou are an expert at mathematical reasoning. Please reason step by step, and put your final answer within \\\\boxed{}.\"\n","\n","\n","n = 3 # beams\n","n_sol = 3\n","samples = 7\n","max_depth = 24\n","max_pct = 0.8"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"408b20a44a394dbfb39c7a2b2cebbfa1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import LlamaForSequenceClassification\n","prm_tokenizer = tokenizer\n","prm_model = LlamaForSequenceClassification.from_pretrained('../Model/PRM_LORA_merge2',\\\n","                                                    num_labels=1,\\\n","                                                    device_map=\"cpu\",\n","                                                    torch_dtype=\"auto\",\n","                                                    ).eval()\n","base_model = prm_model.model\n","prm_model.score.load_state_dict(torch.load('../Model/model_score2.pth'))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# import json\n","# with open('../Data/AMC/aime_normal.json', 'r') as file:\n","#     df = json.load(file)\n","# # to have consistent format as in Kaggle\n","# df = pd.DataFrame(df).iloc[:24]\n","# df.rename(columns={'question': 'problem'}, inplace=True)\n","import pandas as pd\n","df = pd.read_csv('../Data/AMC/cleaned_ArtOfProblemSolving.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def process_inputs(inputs):\n","    # inputs is a list of str\n","    outs = []\n","    for problem in inputs:\n","        query_prompt = problem + cot_instruction\n","        messages = [{\"role\": \"user\",\"content\": query_prompt}]\n","        input = tokenizer.apply_chat_template(messages, tokenize=False)\n","        outs.append(input)\n","    return outs"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:08:23.262621Z","iopub.status.busy":"2024-05-18T17:08:23.262138Z","iopub.status.idle":"2024-05-18T17:08:23.268774Z","shell.execute_reply":"2024-05-18T17:08:23.267916Z"},"papermill":{"duration":0.016196,"end_time":"2024-05-18T17:08:23.270775","exception":false,"start_time":"2024-05-18T17:08:23.254579","status":"completed"},"tags":[]},"outputs":[],"source":["logit2prob = lambda x: 1/(1+np.exp(-x))\n","def eval_prm(candidates):\n","    all_log_probs = []\n","    for i in range(len(candidates)):\n","        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda\")\n","        with torch.no_grad():\n","            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n","            logits = prm_model.score(hidden_states)[0]\n","        all_log_probs.append(logit2prob(logits.item()))\n","    return all_log_probs"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"The answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def group_and_sum(A, B):\n","    '''\n","    A = ['a','b','a']\n","    B = [1,2,3]\n","    -> {'a': 4, 'b': 2}\n","    '''\n","    result_dict = {}\n","    for a, b in zip(A, B):\n","        if a in result_dict:\n","            result_dict[a] += b\n","        else:\n","            result_dict[a] = b\n","    return result_dict\n","\n","def group_and_average(A, B):\n","    from collections import defaultdict\n","    # Create a dictionary to store sums and counts for averaging\n","    sum_dict = defaultdict(lambda: [0, 0])  # Each key maps to [sum, count]\n","    # Pair elements from A and B and aggregate sums and counts\n","    for key, value in zip(A, B):\n","        sum_dict[key][0] += value\n","        sum_dict[key][1] += 1\n","    # Calculate averages\n","    averages = {key: sum_count[0] / sum_count[1] for key, sum_count in sum_dict.items()}\n","    return averages\n","\n","def max_dict(d):\n","    return max(d.items(), key=lambda x: x[1])[0]\n","\n","def tot_agg(completed_paths):\n","    answers,scores,_ = zip(*completed_paths)\n","    if answers:\n","        groups = group_and_sum(answers, scores)\n","        return max_dict(groups)\n","    else:\n","        return 37 # empty completed_paths\n","    \n","def repeat_elements(lst, k):\n","    return [i for i in lst for _ in range(k)]\n","\n","def flatten(nested_list):\n","    \"\"\"Flatten a nested list.\"\"\"\n","    out = []\n","    lengths = []\n","    for sublist in nested_list:\n","        lengths.append(len(sublist))\n","        for item in sublist:\n","            out.append(item)\n","    return out,lengths\n","\n","def unflatten(flat_list, lengths):\n","    \"\"\"Unflatten a flat list into a nested list based on lengths.\"\"\"\n","    nested_list = []\n","    index = 0\n","    for length in lengths:\n","        nested_list.append(flat_list[index:index + length])\n","        index += length\n","    return nested_list\n","\n","def filter_input(batch_response,current_level_node):\n","    # one question filter\n","    prm_inputs = []\n","    parents = []\n","    for candidate,parent in zip(batch_response,current_level_node):\n","        if candidate.outputs[0].text not in parent:\n","            prm_input = parent + candidate.outputs[0].text\n","            prm_inputs.append(prm_input)\n","            parents.append(parent)\n","    # Get the indices of unique elements in prm_inputs\n","    unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n","    prm_inputs = [prm_inputs[i] for i in unique_indices]\n","    parents = [parents[i] for i in unique_indices]\n","    return prm_inputs,parents,len(prm_inputs)\n","\n","def filter_inputs(batch_responses,current_level_nodes,lengths):\n","    # all question filter\n","    # returned value should be flattened\n","    batch_responses,current_level_nodes = unflatten(batch_responses,lengths),unflatten(current_level_nodes,lengths)\n","    prm_inputs = []\n","    lengths = []\n","    parent_list = []\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(batch_responses) == len(uncompleted)\n","    for batch_response,current_level_node,path in zip(batch_responses,current_level_nodes,uncompleted):\n","        prm_input,parents,length = filter_input(batch_response,current_level_node)\n","        if length == 0:# all bad\n","            while len(path)<n_sol:\n","                # make complete for this question as there will be no continued effort\n","                path.append(None)\n","        else:\n","            prm_inputs.extend(prm_input)\n","            parent_list.extend(parents)\n","            lengths.append(length)\n","    return prm_inputs,parent_list,lengths\n","\n","def get_next_node(prm_inputs,prm_scores,completed_paths):\n","    # need to update completed_paths in-place\n","    next_level_nodes = []\n","    combined = list(zip(prm_inputs,prm_scores))\n","    combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n","    max_score = combined[0][1]\n","    for node,score in combined:\n","        answer = extract_number(node)\n","        if answer == 'parse err': # not finished\n","            if len(next_level_nodes) < n:\n","                next_level_nodes.append(node)\n","        else: # finished\n","            if score > max_score * max_pct:\n","                try:\n","                    answer = eval(answer)\n","                    if is_integer(answer) and is_between_0_and_999(answer):# correct format\n","                        completed_paths.append((answer,score,node))\n","                except: # bad eval\n","                    continue\n","    return next_level_nodes\n","\n","def get_next_nodes(prm_inputs,prm_scores,lengths):\n","    # for completed_paths, next_level_nodes would be removed\n","    # returned value should be flattened\n","    prm_inputs,prm_scores = unflatten(prm_inputs,lengths),unflatten(prm_scores,lengths)\n","    uncompleted = [path for path in completed_paths if len(path)<n_sol]\n","    assert len(uncompleted) == len(lengths)\n","    assert len(prm_inputs) == len(lengths)\n","    assert len(prm_scores) == len(lengths)\n","    next_level_nodes,lengths = [],[]\n","    for prm_input,prm_score,completed_path in zip(prm_inputs,prm_scores,uncompleted):\n","        next_node = get_next_node(prm_input,prm_score,completed_path)\n","        if len(completed_path) < n_sol:\n","            next_level_nodes.extend(next_node)\n","            lengths.append(len(next_node))\n","    return next_level_nodes,lengths\n","\n","import gc\n","def create_llm():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    llm = LLM(model=MODEL_PATH,\n","          dtype=dtype,\n","          enforce_eager=True,\n","          gpu_memory_utilization=gpu_memory_utilization,\n","          swap_space=8,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","    tokenizer = llm.get_tokenizer()\n","    return llm,tokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts:   7%|▋         | 1558/23723 [01:25<21:30, 17.17it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:47:23 scheduler.py:245] Input prompt (2742 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  48%|████▊     | 11488/23723 [08:49<07:26, 27.43it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n","WARNING 05-21 19:54:47 scheduler.py:245] Input prompt (3981 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 23723/23723 [17:41<00:00, 22.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 20:17:12 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 20:17:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 20:17:12 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 20:17:14 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 20:17:14 gpu_executor.py:94] # GPU blocks: 2374, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 70728/70728 [1:00:05<00:00, 19.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 22:04:29 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-21 22:04:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-21 22:04:30 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-21 22:04:32 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-21 22:04:32 gpu_executor.py:94] # GPU blocks: 2373, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 69895/69895 [1:09:43<00:00, 16.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 00:07:19 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-22 00:07:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 00:07:19 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-22 00:07:21 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-22 00:07:22 gpu_executor.py:94] # GPU blocks: 2374, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  80%|████████  | 54312/67557 [1:03:59<22:34,  9.78it/s]  "]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 01:12:16 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 67557/67557 [1:18:35<00:00, 14.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 02:23:07 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-22 02:23:07 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 02:23:07 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-22 02:23:14 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-22 02:23:15 gpu_executor.py:94] # GPU blocks: 2373, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   6%|▋         | 4033/64421 [06:02<1:10:26, 14.29it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 02:30:17 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 64421/64421 [1:28:24<00:00, 12.14it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 04:52:03 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-22 04:52:03 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 04:52:04 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-22 04:52:06 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-22 04:52:07 gpu_executor.py:94] # GPU blocks: 2373, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   1%|          | 494/61040 [00:51<2:43:23,  6.18it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 04:54:04 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 61040/61040 [1:37:37<00:00, 10.42it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 07:35:29 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 05-22 07:35:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 05-22 07:35:30 weight_utils.py:177] Using model weights format ['*.safetensors']\n","INFO 05-22 07:35:32 model_runner.py:104] Loading model weights took 12.8716 GB\n","INFO 05-22 07:35:33 gpu_executor.py:94] # GPU blocks: 2373, # CPU blocks: 2184\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   1%|          | 442/57379 [00:53<1:50:14,  8.61it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 07:37:37 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  37%|███▋      | 21406/57379 [38:40<1:16:51,  7.80it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:25 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  37%|███▋      | 21473/57379 [38:47<1:56:45,  5.13it/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:15:32 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  70%|██████▉   | 40144/57379 [1:17:31<41:21,  6.94it/s]  "]},{"name":"stdout","output_type":"stream","text":["WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n","WARNING 05-22 08:54:15 scheduler.py:245] Input prompt (2049 tokens) is too long and exceeds limit of 2048\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  72%|███████▏  | 41535/57379 [1:20:46<23:41, 11.15it/s]  "]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m current_level_nodes \u001b[38;5;241m=\u001b[39m repeat_elements(current_level_nodes,samples)\n\u001b[1;32m     16\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m*\u001b[39msamples \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m---> 17\u001b[0m batch_responses \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_level_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m prm_inputs,parent_list,lengths \u001b[38;5;241m=\u001b[39m filter_inputs(batch_responses,current_level_nodes,lengths)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# release VRAM to prm_model\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:190\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, multi_modal_data)\u001b[0m\n\u001b[1;32m    177\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    178\u001b[0m         i]\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(\n\u001b[1;32m    180\u001b[0m         prompt,\n\u001b[1;32m    181\u001b[0m         sampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:218\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    216\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 218\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py:673\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[RequestOutput]:\n\u001b[1;32m    623\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs one decoding iteration and returns newly generated results.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    .. figure:: https://i.imgur.com/sv2HssD.png\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m        >>>         break\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    676\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor\u001b[38;5;241m.\u001b[39mexecute_model(\n\u001b[1;32m    677\u001b[0m             seq_group_metadata_list, scheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_in,\n\u001b[1;32m    678\u001b[0m             scheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_out,\n\u001b[1;32m    679\u001b[0m             scheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_copy)\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/core/scheduler.py:467\u001b[0m, in \u001b[0;36mScheduler.schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_manager\u001b[38;5;241m.\u001b[39maccess_all_blocks_in_seq(seq, now)\n\u001b[1;32m    463\u001b[0m     common_computed_block_nums \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_manager\u001b[38;5;241m.\u001b[39mget_common_computed_block_ids(\n\u001b[1;32m    465\u001b[0m             seq_group\u001b[38;5;241m.\u001b[39mget_seqs(status\u001b[38;5;241m=\u001b[39mSequenceStatus\u001b[38;5;241m.\u001b[39mRUNNING)))\n\u001b[0;32m--> 467\u001b[0m     seq_group_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mSequenceGroupMetadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_block_nums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_computed_block_nums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `multi_modal_data` will only be present for the 1st comm\u001b[39;49;00m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# between engine and worker.\u001b[39;49;00m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# the subsequent comms can still use delta, but\u001b[39;49;00m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `multi_modal_data` will be None.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_modal_data\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_run\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     seq_group_metadata_list\u001b[38;5;241m.\u001b[39mappend(seq_group_metadata)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# Now that the batch has been created, we can assume all blocks in the\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# batch will have been computed before the next scheduling invocation.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# This is because the engine assumes that a failure in model execution\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# will crash the vLLM instance / will not retry.\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/vllm/lib/python3.9/site-packages/vllm/sequence.py:543\u001b[0m, in \u001b[0;36mSequenceGroupMetadata.__init__\u001b[0;34m(self, request_id, is_prompt, seq_data, sampling_params, block_tables, token_chunk_size, lora_request, computed_block_nums, state, multi_modal_data)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_request \u001b[38;5;241m=\u001b[39m lora_request\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputed_block_nums \u001b[38;5;241m=\u001b[39m computed_block_nums\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_modal_data\u001b[49m \u001b[38;5;241m=\u001b[39m multi_modal_data\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m SequenceGroupState() \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m state\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_chunk_size \u001b[38;5;241m=\u001b[39m token_chunk_size\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# [path for path in completed_paths if len(path)<n_sol] used to track on-going questiones\n","# flattened inputs, with lengths corresponds to the uncompleted path\n","# two ways for path to complete, one is via get_next_nodes getting n_sol answers\n","# two is via filter_inputs, all continuations are bad\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","current_level_nodes = process_inputs(df.problem.tolist())\n","lengths = [1] * len(current_level_nodes)\n","current_level = 1\n","completed_paths = [[] for _ in current_level_nodes]\n","data_out = []\n","\n","while (current_level < max_depth) and (current_level_nodes):\n","    # everything at this level is flattened\n","    current_level_nodes = repeat_elements(current_level_nodes,samples)\n","    lengths = [l*samples for l in lengths]\n","    batch_responses = llm.generate(current_level_nodes, sampling_params)\n","    prm_inputs,parent_list,lengths = filter_inputs(batch_responses,current_level_nodes,lengths)\n","    \n","    # release VRAM to prm_model\n","    del llm\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    prm_model.to('cuda')\n","    prm_scores = eval_prm(prm_inputs)\n","    \n","    # save for Q-learning\n","    data = list(group_and_average(parent_list,prm_scores).items())\n","    data_out.extend(data)\n","    \n","    # release VRAM to llm\n","    prm_model.to('cpu')\n","    llm,tokenizer = create_llm()\n","    \n","    current_level_nodes,lengths = get_next_nodes(prm_inputs,prm_scores,lengths)\n","    current_level += 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = []\n","for paths in completed_paths:\n","    if paths and paths[0]: # not empty or not None\n","        out.append(tot_agg(paths))\n","    else:\n","        out.append(float('-inf'))\n","print(f\"correct {sum([i==j for i,j in zip(df.answer2.tolist(),out)])/df.shape[0]}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","with open(\"../llmOutputs/PRM/data_out.pickle\", \"wb\") as f:\n","    pickle.dump(data_out, f)\n","with open(\"../llmOutputs/PRM/completed_paths.pickle\", \"wb\") as f:\n","    pickle.dump(completed_paths, f)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# with open(\"../llmOutputs/PRM/completed_paths.pickle\", \"rb\") as f:\n","#     completed_paths = pickle.load(f)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8300737,"sourceType":"datasetVersion"},{"datasetId":5036020,"sourceId":8450555,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":571.271793,"end_time":"2024-05-18T17:09:54.761042","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-18T17:00:23.489249","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2770dfd3b1aa4c489056caf28ff0eb19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d51b8465cf24584bcefbd8de22eb8b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2770dfd3b1aa4c489056caf28ff0eb19","placeholder":"​","style":"IPY_MODEL_71347b6fde884c478ad4c29e00602bd8","value":" 3/3 [03:12&lt;00:00, 60.64s/it]"}},"39821c375ddf49ea9e6efb9c09fc0542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a13122cbc4884a7ba1e4a7bf9c6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167f1d4ccc44aee846ccde523d2cc9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7cd8d4fb35847f08de65bd2e42ceffe","IPY_MODEL_7459d3b6efa74b03ac047eac61c9487f","IPY_MODEL_2d51b8465cf24584bcefbd8de22eb8b1"],"layout":"IPY_MODEL_412a13122cbc4884a7ba1e4a7bf9c6cf"}},"71347b6fde884c478ad4c29e00602bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7459d3b6efa74b03ac047eac61c9487f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f5fe4bfeb44f1e84c533bc8b8f2da8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39821c375ddf49ea9e6efb9c09fc0542","value":3}},"78f5fe4bfeb44f1e84c533bc8b8f2da8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7cd8d4fb35847f08de65bd2e42ceffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e209cca0c4874c2bb25ebb79bd84ae28","placeholder":"​","style":"IPY_MODEL_ce7ea779817a4237af29446e91448882","value":"Loading checkpoint shards: 100%"}},"ce7ea779817a4237af29446e91448882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e209cca0c4874c2bb25ebb79bd84ae28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
