{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### change model path and data used to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fine-Tune Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1\"\n",
    "Model_Path = f'../Model/PRM_LORA_merge{version}_code'\n",
    "head_path = f'../Model/model_score{version}_code.pth'\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "clean_text = lambda x:re.sub(r\"(<math>|<\\/math>|<cmath>|<\\/cmath>|\\\\begin\\{align\\*\\}|\\\\end\\{align\\*\\})\", \"\", x)\n",
    "\n",
    "# RL data\n",
    "with open(f\"../llmOutputs/PRM/data_V1_code{version}.pickle\", \"rb\") as f:\n",
    "    data_V = pickle.load(f)\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "\n",
    "data = []\n",
    "for text,y in data_V:\n",
    "    data.append([clean_text(text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\")),y])\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([clean_text(text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\")),y])\n",
    "\n",
    "# SFT data # TODO: remove this later? if loss is too low, e.g. <0.1, overfit or topic classification\n",
    "with open(f\"../Data/ai-mathematical-olympiad-prize/10prob.pickle\", \"rb\") as f:\n",
    "    outs = pickle.load(f)\n",
    "with open(f\"../Data/AMC/aime_final.pickle\", \"rb\") as f:\n",
    "    outs2 = pickle.load(f)\n",
    "for q,s in outs:\n",
    "    if np.random.rand()<0.5:\n",
    "        data.append([clean_text(q+s),1])\n",
    "for q,s in outs2:\n",
    "    if np.random.rand()<0.25:\n",
    "        data.append([clean_text(q+s),1])\n",
    "import random\n",
    "random.shuffle(data)\n",
    "texts,ys = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "texts = tokenizer.batch_encode_plus(texts,return_attention_mask=False,add_special_tokens=True,\\\n",
    "                                    truncation=True, max_length=MAX_LEN)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_gen(texts,ys):\n",
    "    data = list(zip(texts,ys))\n",
    "    random.shuffle(data)\n",
    "    for text,y in data:\n",
    "        text = torch.tensor(text,device='cuda')[None]\n",
    "        y = torch.tensor([y],device='cuda',dtype=torch.float32)\n",
    "        yield text,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification,BitsAndBytesConfig,AutoConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15a150c372b4dad83d9708ffc7dcb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge1_code and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = LlamaForSequenceClassification.from_pretrained(Model_Path,\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       torch_dtype=\"auto\",\n",
    "                                                       quantization_config=quantization_config,\n",
    "                                                       attn_implementation=\"flash_attention_2\"\n",
    "                                                       )\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,509,674,496 || trainable%: 0.2879\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "base_model = get_peft_model(model.model, peft_config)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.print_trainable_parameters()\n",
    "model.score = model.score.float()\n",
    "model.score.load_state_dict(torch.load(head_path))\n",
    "model.score.weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params = list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 1.0186482066653753\n",
      "iter: 2047, \n",
      " train loss: 0.9184768533968963\n",
      "iter: 3071, \n",
      " train loss: 0.7912754519243208\n",
      "iter: 4095, \n",
      " train loss: 0.7327570735851623\n",
      "iter: 5119, \n",
      " train loss: 0.7035882661257347\n",
      "iter: 6143, \n",
      " train loss: 0.6486897827771827\n",
      "iter: 7167, \n",
      " train loss: 0.6083336817973759\n",
      "iter: 8191, \n",
      " train loss: 0.5384236243626219\n",
      "iter: 9215, \n",
      " train loss: 0.5246589001690154\n",
      "iter: 10239, \n",
      " train loss: 0.5247169548310922\n",
      "iter: 11263, \n",
      " train loss: 0.4997896201966796\n",
      "iter: 12287, \n",
      " train loss: 0.4533360378263751\n",
      "iter: 13311, \n",
      " train loss: 0.45762812863540603\n",
      "iter: 14335, \n",
      " train loss: 0.4415243639596156\n",
      "iter: 15359, \n",
      " train loss: 0.4619315887612174\n",
      "iter: 16383, \n",
      " train loss: 0.4164173425597255\n",
      "iter: 17407, \n",
      " train loss: 0.42625634140131297\n",
      "iter: 18431, \n",
      " train loss: 0.4295422995419358\n",
      "iter: 19455, \n",
      " train loss: 0.43823870234336937\n",
      "iter: 20479, \n",
      " train loss: 0.41908886995952344\n",
      "iter: 21503, \n",
      " train loss: 0.430398246509867\n",
      "iter: 22527, \n",
      " train loss: 0.4239679675483785\n",
      "iter: 23551, \n",
      " train loss: 0.40227498545573326\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "        with torch.no_grad():\n",
    "            hidden_states = base_model(text)[0][:,-1].float() # 1,d\n",
    "        logits = model.score(hidden_states)[:,0] # 1,\n",
    "        loss = loss_fn(logits,y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT head and backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params =  base_params + list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.41366537037538365\n",
      "iter: 2047, \n",
      " train loss: 0.41620476913885796\n",
      "iter: 3071, \n",
      " train loss: 0.37813808389455517\n",
      "iter: 4095, \n",
      " train loss: 0.35819730408866235\n",
      "iter: 5119, \n",
      " train loss: 0.32872925177730394\n",
      "iter: 6143, \n",
      " train loss: 0.3380288858639915\n",
      "iter: 7167, \n",
      " train loss: 0.3206686378935615\n",
      "iter: 8191, \n",
      " train loss: 0.30413909911527526\n",
      "iter: 9215, \n",
      " train loss: 0.3071033009244388\n",
      "iter: 10239, \n",
      " train loss: 0.30804539255632335\n",
      "iter: 11263, \n",
      " train loss: 0.3018386588447015\n",
      "iter: 12287, \n",
      " train loss: 0.33025867337934756\n",
      "iter: 13311, \n",
      " train loss: 0.288254887629094\n",
      "iter: 14335, \n",
      " train loss: 0.29319452295891324\n",
      "iter: 15359, \n",
      " train loss: 0.26809748683029966\n",
      "iter: 16383, \n",
      " train loss: 0.2640008098639015\n",
      "iter: 17407, \n",
      " train loss: 0.26303629281852636\n",
      "iter: 18431, \n",
      " train loss: 0.25689785744985727\n",
      "iter: 19455, \n",
      " train loss: 0.30667372214721667\n",
      "iter: 20479, \n",
      " train loss: 0.26606939207488267\n",
      "iter: 21503, \n",
      " train loss: 0.2955766957239945\n",
      "iter: 22527, \n",
      " train loss: 0.27898058057495234\n",
      "iter: 23551, \n",
      " train loss: 0.24593421520283698\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "        hidden_states = base_model(text)[0][:,-1].float() # 1,d\n",
    "        logits = model.score(hidden_states)[:,0] # 1,\n",
    "        loss = loss_fn(logits,y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../Model/PRM_LORA_merge1_code - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.score.state_dict(), f'../Model/model_score{next_version}_code.pth')\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code\"\n",
    "!mkdir peft_model_id\n",
    "base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37264df7a944a50974cebb1d2f8f1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model,base_model,texts,hidden_states,logits,loss\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import LlamaModel\n",
    "model = LlamaModel.from_pretrained(Model_Path,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "# !mkdir '../Model/PRM_LORA_merge3_code'\n",
    "base_model2.save_pretrained(f'../Model/PRM_LORA_merge{next_version}_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForSequenceClassification\n",
    "# model = LlamaForSequenceClassification.from_pretrained(f'../Model/PRM_LORA_merge{next_version}_code',\\\n",
    "#                                                        num_labels=1,\\\n",
    "#                                                        ignore_mismatched_sizes=True,\n",
    "#                                                        device_map=\"auto\",\n",
    "#                                                        torch_dtype=\"auto\",\n",
    "#                                                        attn_implementation=\"flash_attention_2\"\n",
    "#                                                        )\n",
    "# model.score.load_state_dict(torch.load(f'../Model/model_score{next_version}_code.pth'))\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "# train_loss = 0\n",
    "# count_loss = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "#         with torch.no_grad():\n",
    "#             hidden_states = model.model(text)[0][:,-1] # 1,d\n",
    "#             logits = model.score(hidden_states)[:,0] # 1,\n",
    "#             loss = loss_fn(logits,y)\n",
    "#         train_loss += loss.item()\n",
    "#         count_loss += 1\n",
    "\n",
    "#         if (i + 1) % verbose == 0:\n",
    "#             print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "#             train_loss = 0\n",
    "#             count_loss = 0\n",
    "            \n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
