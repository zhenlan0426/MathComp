{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### change model path and data used to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fine-Tune Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"3\"\n",
    "Model_Path = f'../Model/PRM_LORA_merge{version}_code'\n",
    "head_path = f'../Model/model_score{version}_code.pth'\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../llmOutputs/PRM/data_V1_code{version}.pickle\", \"rb\") as f:\n",
    "    data_V = pickle.load(f)\n",
    "# with open(\"../llmOutputs/PRM/data_pi1.pickle\", \"rb\") as f:\n",
    "#     data_pi = pickle.load(f)\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "\n",
    "data = []\n",
    "for text,y in data_V:\n",
    "    data.append([text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"),y])\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"),y])\n",
    "import random\n",
    "random.shuffle(data)\n",
    "texts,ys = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "texts = tokenizer.batch_encode_plus(texts,return_attention_mask=False,add_special_tokens=True,\\\n",
    "                                    truncation=True, max_length=MAX_LEN)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_gen(texts,ys):\n",
    "    data = list(zip(texts,ys))\n",
    "    random.shuffle(data)\n",
    "    for text,y in data:\n",
    "        text = torch.tensor(text,device='cuda')[None]\n",
    "        y = torch.tensor([y],device='cuda',dtype=torch.float32)\n",
    "        yield text,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification,BitsAndBytesConfig,AutoConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781e6f000ede44029bf45870c7af4433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge3_code and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = LlamaForSequenceClassification.from_pretrained(Model_Path,\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       torch_dtype=\"auto\",\n",
    "                                                       quantization_config=quantization_config,\n",
    "                                                       attn_implementation=\"flash_attention_2\"\n",
    "                                                       )\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,509,674,496 || trainable%: 0.2879\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "base_model = get_peft_model(model.model, peft_config)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.print_trainable_parameters()\n",
    "model.score = model.score.float()\n",
    "model.score.load_state_dict(torch.load(head_path))\n",
    "model.score.weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params = list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.26935168488117256\n",
      "iter: 2047, \n",
      " train loss: 0.2871932804980304\n",
      "iter: 3071, \n",
      " train loss: 0.2380033590917776\n",
      "iter: 4095, \n",
      " train loss: 0.25909119807374736\n",
      "iter: 5119, \n",
      " train loss: 0.1940896802586849\n",
      "iter: 6143, \n",
      " train loss: 0.22669195063508596\n",
      "iter: 7167, \n",
      " train loss: 0.19930410744984783\n",
      "iter: 8191, \n",
      " train loss: 0.23857769269017126\n",
      "iter: 9215, \n",
      " train loss: 0.22962719169754564\n",
      "iter: 10239, \n",
      " train loss: 0.1706610002420348\n",
      "iter: 11263, \n",
      " train loss: 0.21618280586085348\n",
      "iter: 12287, \n",
      " train loss: 0.2451941864209175\n",
      "iter: 13311, \n",
      " train loss: 0.1873421580150989\n",
      "iter: 14335, \n",
      " train loss: 0.21125995945902787\n",
      "iter: 15359, \n",
      " train loss: 0.2267602680796017\n",
      "iter: 16383, \n",
      " train loss: 0.2051553648495883\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "        with torch.no_grad():\n",
    "            hidden_states = base_model(text)[0][:,-1].float() # 1,d\n",
    "        logits = model.score(hidden_states)[:,0] # 1,\n",
    "        loss = loss_fn(logits,y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT head and backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params =  base_params + list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.23743040706438023\n",
      "iter: 2047, \n",
      " train loss: 0.19935922980039322\n",
      "iter: 3071, \n",
      " train loss: 0.16616363863977313\n",
      "iter: 4095, \n",
      " train loss: 0.18490789474526537\n",
      "iter: 5119, \n",
      " train loss: 0.16923691741476432\n",
      "iter: 6143, \n",
      " train loss: 0.141193058780118\n",
      "iter: 7167, \n",
      " train loss: 0.16937424881643892\n",
      "iter: 8191, \n",
      " train loss: 0.20586854194061743\n",
      "iter: 9215, \n",
      " train loss: 0.17790575974140665\n",
      "iter: 10239, \n",
      " train loss: 0.17880090023936646\n",
      "iter: 11263, \n",
      " train loss: 0.16426272363605676\n",
      "iter: 12287, \n",
      " train loss: 0.16262319491761446\n",
      "iter: 13311, \n",
      " train loss: 0.16084378251798626\n",
      "iter: 14335, \n",
      " train loss: 0.15605151505133108\n",
      "iter: 15359, \n",
      " train loss: 0.1661401333340109\n",
      "iter: 16383, \n",
      " train loss: 0.1523424692240951\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "        hidden_states = base_model(text)[0][:,-1].float() # 1,d\n",
    "        logits = model.score(hidden_states)[:,0] # 1,\n",
    "        loss = loss_fn(logits,y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘peft_model_id’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../Model/PRM_LORA_merge3_code - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.score.state_dict(), f'../Model/model_score{next_version}_code.pth')\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code\"\n",
    "!mkdir peft_model_id\n",
    "base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87c1a5bc51649cc946fa223d47b7d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaModel\n",
    "model = LlamaModel.from_pretrained(Model_Path,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "# !mkdir '../Model/PRM_LORA_merge3_code'\n",
    "base_model2.save_pretrained(f'../Model/PRM_LORA_merge{next_version}_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForSequenceClassification\n",
    "# model = LlamaForSequenceClassification.from_pretrained(f'../Model/PRM_LORA_merge{next_version}_code',\\\n",
    "#                                                        num_labels=1,\\\n",
    "#                                                        ignore_mismatched_sizes=True,\n",
    "#                                                        device_map=\"auto\",\n",
    "#                                                        torch_dtype=\"auto\",\n",
    "#                                                        attn_implementation=\"flash_attention_2\"\n",
    "#                                                        )\n",
    "# model.score.load_state_dict(torch.load(f'../Model/model_score{next_version}_code.pth'))\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "# train_loss = 0\n",
    "# count_loss = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "#         with torch.no_grad():\n",
    "#             hidden_states = model.model(text)[0][:,-1] # 1,d\n",
    "#             logits = model.score(hidden_states)[:,0] # 1,\n",
    "#             loss = loss_fn(logits,y)\n",
    "#         train_loss += loss.item()\n",
    "#         count_loss += 1\n",
    "\n",
    "#         if (i + 1) % verbose == 0:\n",
    "#             print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "#             train_loss = 0\n",
    "#             count_loss = 0\n",
    "            \n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fine-Tune Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# math = pd.read_csv('../Data/MATH/math-annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "\n",
    "data = []\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"),y])\n",
    "import random\n",
    "random.shuffle(data)\n",
    "texts,ys = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize rewards\n",
    "import numpy as np\n",
    "ys = np.array(ys)\n",
    "ys = (ys-ys.mean())/ys.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"Please reason step by step, and put your final answer within \\\\boxed\\{\\}.\"\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for text in texts:\n",
    "    idx = re.search(pattern,text).end()\n",
    "    question = tokenizer.encode(text[:idx],add_special_tokens=True)\n",
    "    answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_gen(texts,ys,lengths):\n",
    "    data = list(zip(texts,ys,lengths))\n",
    "    random.shuffle(data)\n",
    "    for text,y,l in data:\n",
    "        text = torch.tensor(text[:MAX_LEN],device='cuda')[None]\n",
    "        yield text,y,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e63117b6b64776a5436a028f27d2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            trust_remote_code=True,\n",
    "                                            attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "model.lm_head = model.lm_head.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: -0.013546783548008534\n",
      "iter: 2047, \n",
      " train loss: -0.02897482163643872\n",
      "iter: 3071, \n",
      " train loss: -0.01757911852109828\n",
      "iter: 4095, \n",
      " train loss: -0.034147584029597056\n",
      "iter: 5119, \n",
      " train loss: -0.037454732966580195\n",
      "iter: 6143, \n",
      " train loss: -0.034789292309142184\n",
      "iter: 7167, \n",
      " train loss: -0.030503184903864167\n",
      "iter: 8191, \n",
      " train loss: -0.03708435299995472\n",
      "iter: 9215, \n",
      " train loss: -0.02951386814311263\n",
      "iter: 10239, \n",
      " train loss: -0.06372847682087013\n",
      "iter: 11263, \n",
      " train loss: -0.07185396562272217\n",
      "iter: 12287, \n",
      " train loss: -0.03204263923362305\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "import math\n",
    "import gc\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y,l) in enumerate(from_gen(input_ids,ys,lengths)):\n",
    "        if i > 0:\n",
    "            del outs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outs = model.model(text)[0].float() # 1,l,C\n",
    "        outs = model.lm_head(outs)\n",
    "        \n",
    "        if torch.any(torch.isnan(outs)): continue\n",
    "        loss = loss_fn(outs[0,l:-1],text[0,l+1:]) * y # (l,C), (l,)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()): continue\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.lm_head.state_dict(), '../Model/lm_head.pth')\n",
    "model.lm_head.to(torch.bfloat16) \n",
    "model.save_pretrained(\"../Model/Policy1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LORA -> OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "model = get_peft_model(model, peft_config)\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug\n",
    "import math\n",
    "import gc\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y,l) in enumerate(from_gen(input_ids,ys,lengths)):\n",
    "        if i > 0:\n",
    "            del outs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        outs = model(text)[0] # 1,l,C\n",
    "        if torch.any(torch.isnan(outs)): continue\n",
    "        loss = loss_fn(outs[0,l:-1],text[0,l+1:]) * y # (l,C), (l,)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()): continue\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
