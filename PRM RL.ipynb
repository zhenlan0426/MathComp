{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### change model path and data used to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fine-Tune Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"5\"\n",
    "Model_Path = f'../Model/PRM_LORA_merge{version}_code'\n",
    "head_path = f'../Model/model_score{version}_code.pth'\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../llmOutputs/PRM/data_V1_code{version}.pickle\", \"rb\") as f:\n",
    "    data_V = pickle.load(f)\n",
    "# with open(\"../llmOutputs/PRM/data_pi1.pickle\", \"rb\") as f:\n",
    "#     data_pi = pickle.load(f)\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "\n",
    "# TODO: remove in next iteration\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}_rlPolicy.pickle\", \"rb\") as f:\n",
    "    completed_paths_y2 = pickle.load(f)\n",
    "\n",
    "completed_paths_y.extend(completed_paths_y2)\n",
    "data = []\n",
    "for text,y in data_V:\n",
    "    data.append([text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"),y])\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    data.append([text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"),y])\n",
    "import random\n",
    "random.shuffle(data)\n",
    "texts,ys = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "texts = tokenizer.batch_encode_plus(texts,return_attention_mask=False,add_special_tokens=True,\\\n",
    "                                    truncation=True, max_length=MAX_LEN)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_gen(texts,ys):\n",
    "    data = list(zip(texts,ys))\n",
    "    random.shuffle(data)\n",
    "    for text,y in data:\n",
    "        text = torch.tensor(text,device='cuda')[None]\n",
    "        y = torch.tensor([y],device='cuda',dtype=torch.float32)\n",
    "        yield text,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification,BitsAndBytesConfig,AutoConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1574d828f9c44f29840643f311ccc0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../Model/PRM_LORA_merge5_code and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = LlamaForSequenceClassification.from_pretrained(Model_Path,\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       torch_dtype=\"auto\",\n",
    "                                                       quantization_config=quantization_config,\n",
    "                                                       attn_implementation=\"flash_attention_2\"\n",
    "                                                       )\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,509,674,496 || trainable%: 0.2879\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "base_model = get_peft_model(model.model, peft_config)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.print_trainable_parameters()\n",
    "model.score = model.score.float()\n",
    "model.score.load_state_dict(torch.load(head_path))\n",
    "model.score.weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params = list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.22503854452406813\n",
      "iter: 2047, \n",
      " train loss: 0.19690279222504614\n",
      "iter: 3071, \n",
      " train loss: 0.17981932103066356\n",
      "iter: 4095, \n",
      " train loss: 0.1670584841767777\n",
      "iter: 5119, \n",
      " train loss: 0.16929041448020143\n",
      "iter: 6143, \n",
      " train loss: 0.19290116799493262\n",
      "iter: 7167, \n",
      " train loss: 0.21981135978239763\n",
      "iter: 8191, \n",
      " train loss: 0.1823250137604191\n",
      "iter: 9215, \n",
      " train loss: 0.20385578264722426\n",
      "iter: 10239, \n",
      " train loss: 0.17016758029421908\n",
      "iter: 11263, \n",
      " train loss: 0.15799389205130865\n",
      "iter: 12287, \n",
      " train loss: 0.16727510656346567\n",
      "iter: 13311, \n",
      " train loss: 0.19434696441385313\n",
      "iter: 14335, \n",
      " train loss: 0.14517014800367178\n",
      "iter: 15359, \n",
      " train loss: 0.2005108221746923\n",
      "iter: 16383, \n",
      " train loss: 0.1813583382499928\n",
      "iter: 17407, \n",
      " train loss: 0.17493958562772605\n",
      "iter: 18431, \n",
      " train loss: 0.21305208181183843\n",
      "iter: 19455, \n",
      " train loss: 0.19158705954032484\n",
      "iter: 20479, \n",
      " train loss: 0.18986034848603595\n",
      "iter: 21503, \n",
      " train loss: 0.16337557299448235\n",
      "iter: 22527, \n",
      " train loss: 0.18156229498163157\n",
      "iter: 23551, \n",
      " train loss: 0.17168330446293112\n",
      "iter: 24575, \n",
      " train loss: 0.21974059747844876\n",
      "iter: 25599, \n",
      " train loss: 0.2129329749786848\n",
      "iter: 26623, \n",
      " train loss: 0.1873643671242462\n",
      "iter: 27647, \n",
      " train loss: 0.19175560611802211\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "        with torch.no_grad():\n",
    "            hidden_states = base_model(text)[0][:,-1].float() # 1,d\n",
    "        logits = model.score(hidden_states)[:,0] # 1,\n",
    "        loss = loss_fn(logits,y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FT head and backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params =  base_params + list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.19985639877086214\n",
      "iter: 2047, \n",
      " train loss: 0.205439392640983\n",
      "iter: 3071, \n",
      " train loss: 0.1569971104763681\n",
      "iter: 4095, \n",
      " train loss: 0.19266401687127654\n",
      "iter: 5119, \n",
      " train loss: 0.1545685434666666\n",
      "iter: 6143, \n",
      " train loss: 0.18454259471036494\n",
      "iter: 7167, \n",
      " train loss: 0.15482757396239322\n",
      "iter: 8191, \n",
      " train loss: 0.17882152074889746\n",
      "iter: 9215, \n",
      " train loss: 0.16669698896112095\n",
      "iter: 10239, \n",
      " train loss: 0.15513845881832822\n",
      "iter: 11263, \n",
      " train loss: 0.178124901346564\n",
      "iter: 12287, \n",
      " train loss: 0.17896800365087984\n",
      "iter: 13311, \n",
      " train loss: 0.16427032280080311\n",
      "iter: 14335, \n",
      " train loss: 0.14667499106144533\n",
      "iter: 15359, \n",
      " train loss: 0.1548061366620459\n",
      "iter: 16383, \n",
      " train loss: 0.16954763650483073\n",
      "iter: 17407, \n",
      " train loss: 0.17013626196603582\n",
      "iter: 18431, \n",
      " train loss: 0.15307152537025104\n",
      "iter: 19455, \n",
      " train loss: 0.1349567736742756\n",
      "iter: 20479, \n",
      " train loss: 0.15425231566041475\n",
      "iter: 21503, \n",
      " train loss: 0.13406191560534353\n",
      "iter: 22527, \n",
      " train loss: 0.15101354175203596\n",
      "iter: 23551, \n",
      " train loss: 0.12140807655850949\n",
      "iter: 24575, \n",
      " train loss: 0.1067702972150073\n",
      "iter: 25599, \n",
      " train loss: 0.1509188842965159\n",
      "iter: 26623, \n",
      " train loss: 0.14281031255313792\n",
      "iter: 27647, \n",
      " train loss: 0.14039523757128336\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "        hidden_states = base_model(text)[0][:,-1].float() # 1,d\n",
    "        logits = model.score(hidden_states)[:,0] # 1,\n",
    "        loss = loss_fn(logits,y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘peft_model_id’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ../Model/PRM_LORA_merge5_code - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.score.state_dict(), f'../Model/model_score{next_version}_code.pth')\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code\"\n",
    "!mkdir peft_model_id\n",
    "base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeb19116e904d8f9018e7d4176f2086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model,base_model,texts,hidden_states,logits,loss\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import LlamaModel\n",
    "model = LlamaModel.from_pretrained(Model_Path,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "# !mkdir '../Model/PRM_LORA_merge3_code'\n",
    "base_model2.save_pretrained(f'../Model/PRM_LORA_merge{next_version}_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForSequenceClassification\n",
    "# model = LlamaForSequenceClassification.from_pretrained(f'../Model/PRM_LORA_merge{next_version}_code',\\\n",
    "#                                                        num_labels=1,\\\n",
    "#                                                        ignore_mismatched_sizes=True,\n",
    "#                                                        device_map=\"auto\",\n",
    "#                                                        torch_dtype=\"auto\",\n",
    "#                                                        attn_implementation=\"flash_attention_2\"\n",
    "#                                                        )\n",
    "# model.score.load_state_dict(torch.load(f'../Model/model_score{next_version}_code.pth'))\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "# train_loss = 0\n",
    "# count_loss = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for i,(text,y) in enumerate(from_gen(texts,ys)):\n",
    "#         with torch.no_grad():\n",
    "#             hidden_states = model.model(text)[0][:,-1] # 1,d\n",
    "#             logits = model.score(hidden_states)[:,0] # 1,\n",
    "#             loss = loss_fn(logits,y)\n",
    "#         train_loss += loss.item()\n",
    "#         count_loss += 1\n",
    "\n",
    "#         if (i + 1) % verbose == 0:\n",
    "#             print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "#             train_loss = 0\n",
    "#             count_loss = 0\n",
    "            \n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
