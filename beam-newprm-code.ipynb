{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046},{"sourceId":8613559,"sourceType":"datasetVersion","datasetId":5102247},{"sourceId":8613557,"sourceType":"datasetVersion","datasetId":5136886}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y torch -q\n!pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n# keep data in float16 to avoid OOM\nfile_path = '/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py'\nwith open(file_path, 'r') as file:\n    file_contents = file.readlines()\nfile_contents = [line for line in file_contents if \"logits = logits.float()\" not in line]\nwith open(file_path, 'w') as file:\n    file.writelines(file_contents)","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-28T18:35:16.260656Z","iopub.execute_input":"2024-05-28T18:35:16.261052Z","iopub.status.idle":"2024-05-28T18:37:41.861300Z","shell.execute_reply.started":"2024-05-28T18:35:16.261024Z","shell.execute_reply":"2024-05-28T18:37:41.860339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport numpy as np\nfrom transformers import LlamaForSequenceClassification\nimport torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\nllm = LLM(model=\"/kaggle/input/policy-model\",\n          dtype='half',\n          enforce_eager=True,\n          gpu_memory_utilization=0.99,\n          swap_space=4,\n          max_model_len=2048,\n          kv_cache_dtype=\"fp8_e5m2\",\n          tensor_parallel_size=1)\n\ntokenizer = llm.get_tokenizer()\n\nprm_tokenizer = tokenizer\nprm_model = LlamaForSequenceClassification.from_pretrained('/kaggle/input/prm-code',\\\n                                                    num_labels=1,\\\n                                                    device_map=\"cuda:1\",\n                                                    torch_dtype=\"auto\",\n                                                    ignore_mismatched_sizes=True,\n                                                    ).eval()\n\nbase_model = prm_model.model\nprm_model.score.load_state_dict(torch.load('/kaggle/input/prm-code/model_score8_code.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T18:37:41.864096Z","iopub.execute_input":"2024-05-28T18:37:41.864942Z","iopub.status.idle":"2024-05-28T18:42:23.010414Z","shell.execute_reply.started":"2024-05-28T18:37:41.864902Z","shell.execute_reply":"2024-05-28T18:42:23.009560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import aimo\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T18:42:23.011501Z","iopub.execute_input":"2024-05-28T18:42:23.011950Z","iopub.status.idle":"2024-05-28T18:42:24.242159Z","shell.execute_reply.started":"2024-05-28T18:42:23.011923Z","shell.execute_reply":"2024-05-28T18:42:24.241373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit2prob = lambda x: 1/(1+np.exp(-x))\ndef eval_prm(candidates):\n    all_log_probs = []\n    for i in range(len(candidates)):\n        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda:1\")\n        with torch.no_grad():\n            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n            logits = prm_model.score(hidden_states)[0]\n        all_log_probs.append(logit2prob(logits.item()))\n    return all_log_probs","metadata":{"execution":{"iopub.status.busy":"2024-05-28T18:42:24.243361Z","iopub.execute_input":"2024-05-28T18:42:24.244240Z","iopub.status.idle":"2024-05-28T18:42:24.250861Z","shell.execute_reply.started":"2024-05-28T18:42:24.244207Z","shell.execute_reply":"2024-05-28T18:42:24.249866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = [tokenizer.eos_token,\"```output\",\"```Output\",\"```output\\n\",\"```Output\\n\",\"```\\nOutput\" , \")\\n```\" , \"``````output\",\"``````Output\"]\n# stop_words.append(\"\\n\")\nsampling_params = SamplingParams(temperature=1,\n                                 max_tokens=180,\n                                #  min_tokens=32,\n                                 stop=stop_words,\n                                 include_stop_str_in_output=True\n                                )\n\ndef gen_prompt_codeIn1(problem):\n    return f\"\"\"Problem: {problem}\\n\nTo accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and your final answer should be integer, not expression, list, tuple or dictionary!\nWrite the entire script covering all the steps (use comments and document it well) and print the final result.\nApproach:\"\"\"\n\ndef gen_prompt_codeIn2(problem):\n    return f\"\"\"Problem: {problem}\\n\nYou are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\"\"\"\n\nn = 1 # beams\nn_sol = 1\nsamples = 21\nmax_depth = 24\nmax_pct = 0.88\ntimeout = 7\nlen_limit = 49\n\n\nall_prompts = []\ntotal_paths = []\ntotal_answers = []\n\ndef is_integer(num):\n    if isinstance(num, float):\n        return num.is_integer()\n    elif isinstance(num, int):\n        return True\n    else:\n        return False\n    \ndef is_between_0_and_999(num):\n    return 0 <= num <= 999\n\nimport re\ndef extract_number(text):\n    patterns = [\n        r'[Tt]he answer is.*\\\\boxed\\{(.*?)\\}',\n        r\"[Tt]he answer is[:\\s]*\\$([0-9]+)\\$\",\n        r\"[Tt]he answer is[:\\s]*([0-9]+)\"\n    ]\n    for pattern in patterns:\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n    return 'parse err'\n\ndef repeat_elements(lst, k):\n    return [i for i in lst for _ in range(k)]\n\ndef filter_input(batch_response,current_level_node):\n    # one question filter\n    prm_inputs = []\n    for candidate,parent in zip(batch_response,current_level_node):\n        if candidate.outputs[0].text not in parent:\n            prm_input = parent + candidate.outputs[0].text\n            prm_inputs.append(prm_input)\n    # Get the indices of unique elements in prm_inputs\n    unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n    prm_inputs = [prm_inputs[i] for i in unique_indices]\n    return prm_inputs\n\ndef IsFinished(node):\n    matches = re.findall(r'print\\(([^)]*)\\)', node)\n    return len(matches)>0\n\ndef get_next_node(prm_inputs,prm_scores):\n    # need to update completed_paths in-place\n    if len(prm_inputs) == 0: return []\n    next_level_nodes = []\n    combined = list(zip(prm_inputs,prm_scores))\n    combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n    max_score = combined[0][1]\n    for node,score in combined:\n        finish = IsFinished(node)\n        if finish: # finished\n            if score > max_score * max_pct:\n                completed_paths.append((score,node))\n        else: # not inished\n            if (len(next_level_nodes) < n) and (score > max_score * max_pct):\n                next_level_nodes.append(node)\n    return next_level_nodes\n\ndef repl(match):\n    if \"real\" not in match.group():\n        return \"{}{}\".format(match.group()[:-1], ', real=True)')\n    else:\n        return \"{}{}\".format(match.group()[:-1], ')')\n    \nsingle_line_comment_pattern = re.compile(r'(?<!\\\\)#.*')\nmulti_line_comment_pattern = re.compile(r'(\\'\\'\\'|\\\"\\\"\\\")(.*?)(\\'\\'\\'|\\\"\\\"\\\")', flags=re.DOTALL)\ntrailing_whitespace_pattern = re.compile(r'[ \\t]+$', flags=re.MULTILINE)\nmultiple_blank_lines_pattern = re.compile(r'\\n\\s*\\n')\n\ndef remove_python_comments(code):\n    # Remove single-line comments\n    code = single_line_comment_pattern.sub('', code)\n    # Remove multi-line comments (docstrings)\n    code = multi_line_comment_pattern.sub('', code)\n    # Remove leading and trailing whitespace from each line\n    code = trailing_whitespace_pattern.sub('', code)\n    # Reduce multiple blank lines to a single blank line\n    code = multiple_blank_lines_pattern.sub('\\n', code)\n    return code\n\nimport subprocess\nimport sys\ndef agg_code(paths):\n    paths = [p for p in paths if p]\n    paths.sort(key=lambda x: x[0], reverse=True)\n    code_set = set()\n    for path in paths:# path (score,node)\n        input = path[1]\n        if input[-12:]==\"print(result\": # stop token was not included. print(result) might miss a \")\"\n            input += \")\"\n        splits = input.split('```')\n        if len(splits) < 2:\n            continue\n        code = \"from sympy import *\\n\" + input.split('```')[1][7:] \n        if len(code) < len_limit: continue # ignore very short answer\n        clean_code = remove_python_comments(code)\n        if clean_code in code_set:\n            continue\n        else:\n            code_set.add(clean_code)\n        code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n        # execute code\n        with open('code.py', 'w') as fout:\n            fout.write(code)\n        # timeout err\n        try:\n            process = subprocess.run([sys.executable, 'code.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n        except subprocess.TimeoutExpired:\n            continue\n        if process.stderr:# code.py err\n            continue\n        else:\n            stdout = process.stdout.decode('utf8')\n            try:\n                answer = eval(stdout)\n                if is_integer(answer) and is_between_0_and_999(answer):\n                    return int(answer)\n                else:\n                    continue\n            except:\n                continue\n    return 37\n\nfor test, sample_submission in iter_test:\n    problem = test['problem'].values[0]\n    base_prompt1 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt_codeIn1(problem)}],tokenize=False)\n    base_prompt2 = tokenizer.apply_chat_template([{\"role\": \"user\",\"content\": gen_prompt_codeIn2(problem)}],tokenize=False)\n    current_level = 1\n    current_level_nodes = [base_prompt1,base_prompt2]\n    completed_paths = []\n    completed_path_splits = []\n    try:\n        while (len(completed_paths) < n_sol) and (current_level < max_depth) and (current_level_nodes):\n            current_level_nodes = repeat_elements(current_level_nodes,samples)\n            batch_responses = llm.generate(current_level_nodes, sampling_params)\n            prm_inputs = filter_input(batch_responses,current_level_nodes)\n            prm_scores = eval_prm(prm_inputs)\n            current_level_nodes = get_next_node(prm_inputs,prm_scores)\n        sample_submission['answer'] = agg_code(completed_paths)\n    except:\n        sample_submission['answer'] = 37\n    env.predict(sample_submission)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-05-28T18:42:24.253441Z","iopub.execute_input":"2024-05-28T18:42:24.253937Z","iopub.status.idle":"2024-05-28T18:42:59.023122Z","shell.execute_reply.started":"2024-05-28T18:42:24.253905Z","shell.execute_reply":"2024-05-28T18:42:59.022162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_paths\n# len(set(current_level_nodes)),len(current_level_nodes),len(set(prm_inputs)),len(prm_inputs)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T18:42:59.024542Z","iopub.execute_input":"2024-05-28T18:42:59.025261Z","iopub.status.idle":"2024-05-28T18:42:59.028986Z","shell.execute_reply.started":"2024-05-28T18:42:59.025227Z","shell.execute_reply":"2024-05-28T18:42:59.028102Z"},"trusted":true},"execution_count":null,"outputs":[]}]}