{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ee5a28d7b14757bc1d7a053ca7377d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 1023: train loss 1.2250459004571894\n",
      "epoch 0 iter 2047: train loss 1.097773865389172\n",
      "epoch 0 iter 3071: train loss 0.970116772732581\n",
      "epoch 0 iter 4095: train loss 0.9048753228416899\n",
      "epoch 0 iter 5119: train loss 0.8852845648143557\n",
      "epoch 0 iter 6143: train loss 0.8505273452901747\n",
      "epoch 0 iter 7167: train loss 0.8277944275614573\n",
      "epoch 0 iter 8191: train loss 0.8189215795209748\n",
      "epoch 0 iter 9215: train loss 0.8168886959756492\n",
      "epoch 0 iter 10239: train loss 0.8177528268715832\n",
      "epoch 0 iter 11263: train loss 0.7985045031964546\n",
      "epoch 0 iter 12287: train loss 0.8106034819138586\n",
      "epoch 0 iter 13311: train loss 0.7889349279430462\n",
      "epoch 0 iter 14335: train loss 0.7809478161652805\n",
      "epoch 0 iter 15359: train loss 0.780915419170924\n",
      "epoch 0 iter 16383: train loss 0.8007599455813761\n",
      "epoch 0 iter 17407: train loss 0.7841280383363483\n",
      "epoch 0 iter 18431: train loss 0.7871896027645562\n",
      "epoch 0 iter 19455: train loss 0.7729098980780691\n",
      "epoch 0 iter 20479: train loss 0.7760596466396237\n",
      "epoch 0 iter 21503: train loss 0.7877617846534122\n",
      "epoch 0 iter 22527: train loss 0.7733363682345953\n",
      "epoch 0 iter 23551: train loss 0.7745722282706993\n",
      "epoch 0 iter 24575: train loss 0.7460884120991977\n",
      "epoch 0 iter 25599: train loss 0.7718490747611213\n",
      "epoch 0 iter 26623: train loss 0.7690884109397302\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from functions import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import math\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    # AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model\n",
    "\n",
    "\"\"\" Data \"\"\"\n",
    "files = ['../Data/OlympiadBench_Dataset/data/outputs.json','../Data/AMC/outputs.json','../Data/MATH/outputs.json']\n",
    "texts = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        # Load the list from the JSON file\n",
    "        texts.extend(json.load(f))\n",
    "from transformers import AutoTokenizer\n",
    "MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "texts = tokenizer.batch_encode_plus(texts,return_attention_mask=False,add_special_tokens=True,\\\n",
    "                                    truncation=True, max_length=4096)['input_ids']\n",
    "\n",
    "\"\"\" Model \"\"\"\n",
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "alpha = 0.05\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "config_dict = random_peft_config()\n",
    "peft_config = config_map[config_dict['config_type']](**config_dict['config_kwargs'])\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.print_trainable_parameters()\n",
    "a,b = model.get_nb_trainable_parameters()\n",
    "trainable = a/b\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)\n",
    "# optimizer = torch.optim.SGD(trainable_params,lr=lr)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "performance = []\n",
    "logit_offset = config_dict['config_kwargs'].get('num_virtual_tokens',0) if config_dict['config_type'] == 'PromptEncoderConfig' else 0\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(texts)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_last = 0\n",
    "    skip = 0\n",
    "    tot_skip = 0\n",
    "    # for llm, batchsize = 1 still gives 100 GPU util\n",
    "    for i,input_ids in enumerate(texts):\n",
    "        # train\n",
    "        input_ids = sample_consecutive_chunk(input_ids,1200)\n",
    "        input_ids = torch.tensor(input_ids).to('cuda')[None]\n",
    "        outs = model(input_ids).logits\n",
    "        if torch.any(torch.isnan(outs)):\n",
    "            skip += 1\n",
    "            continue\n",
    "        loss = loss_fn(outs[0,logit_offset:-1],input_ids[0,1:])\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()):\n",
    "            skip += 1\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        # print(i,train_loss)\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # eval    \n",
    "        if (i + 1) % verbose == 0:\n",
    "            temp = (train_loss-train_last)/(verbose-skip)\n",
    "            print(f\"epoch {epoch} iter {i}: train loss {temp}\")\n",
    "            performance.append((epoch,i,temp))\n",
    "            train_last = train_loss\n",
    "            tot_skip += skip\n",
    "            skip = 0\n",
    "\n",
    "# Save model/config/performance\n",
    "peft_model_id = create_next_model_folder()\n",
    "model.save_pretrained(peft_model_id)\n",
    "with open(peft_model_id+'/config_dict.json', 'w') as f:\n",
    "    json.dump(config_dict, f)\n",
    "with open(peft_model_id+'/performance.json', 'w') as f:\n",
    "    json.dump({'loss_hist':performance,'trainable_pct':trainable}, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
