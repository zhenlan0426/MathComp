{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c704d4c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T21:07:38.572957Z",
     "iopub.status.busy": "2024-06-18T21:07:38.572255Z",
     "iopub.status.idle": "2024-06-18T21:12:09.385771Z",
     "shell.execute_reply": "2024-06-18T21:12:09.384774Z"
    },
    "papermill": {
     "duration": 270.822589,
     "end_time": "2024-06-18T21:12:09.387850",
     "exception": false,
     "start_time": "2024-06-18T21:07:38.565261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.39.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 21:07:46.497182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-18 21:07:46.497322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-18 21:07:46.609732: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d19c798bb044766805ae2f30bc47b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/19 [00:11<03:34, 11.90s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "\n",
      " 26%|██▋       | 5/19 [00:15<00:34,  2.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval() arg 1 must be a string, bytes or code object final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "\n",
      " 53%|█████▎    | 10/19 [00:19<00:13,  1.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval() arg 1 must be a string, bytes or code object final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "\n",
      "100%|██████████| 19/19 [00:25<00:00,  1.34s/it]\n",
      "1it [00:25, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval() arg 1 must be a string, bytes or code object final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/19 [00:13<04:09, 13.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float() argument must be a string or a real number, not 'list' final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 2/19 [00:22<03:01, 10.69s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 19/19 [00:31<00:00,  1.67s/it]\n",
      "2it [00:57, 29.13s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "\n",
      "  5%|▌         | 1/19 [00:08<02:25,  8.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval() arg 1 must be a string, bytes or code object final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "\n",
      " 11%|█         | 2/19 [00:18<02:38,  9.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval() arg 1 must be a string, bytes or code object final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "\n",
      " 26%|██▋       | 5/19 [00:25<01:01,  4.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval() arg 1 must be a string, bytes or code object final_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 19/19 [00:40<00:00,  2.15s/it]\n",
      "3it [01:38, 32.68s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "NOTEBOOK_START_TIME = time.time()\n",
    "import aimo\n",
    "\n",
    "env = aimo.make_env()\n",
    "iter_test = env.iter_test()\n",
    "DEBUG = False\n",
    "\n",
    "QUANT = False\n",
    "\n",
    "if QUANT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "USE_PAST_KEY = True\n",
    "\n",
    "if QUANT:\n",
    "    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n",
    "    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n",
    "\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    StoppingCriteria,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "import transformers\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "set_seed(42)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "PRIVATE = True\n",
    "\n",
    "# df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n",
    "# df.head()\n",
    "# if len(df) < 5:\n",
    "#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "#     PRIVATE = False\n",
    "# df.head()\n",
    "def naive_parse(answer):\n",
    "    out = []\n",
    "    start = False\n",
    "    end = False\n",
    "    for l in reversed(list(answer)):\n",
    "        if l in '0123456789' and not end:\n",
    "            start = True\n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:\n",
    "                end = True\n",
    "        \n",
    "    out = reversed(out)\n",
    "    return ''.join(out)\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def return_last_print(output, n):\n",
    "    lines = output.strip().split('\\n')\n",
    "    if lines:\n",
    "        return lines[n]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def process_code(code, return_shell_output=False):\n",
    "    \n",
    "    def repl(match):\n",
    "        if \"real\" not in match.group():\n",
    "            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "        else:\n",
    "            return \"{}{}\".format(match.group()[:-1], ')')\n",
    "    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n",
    "\n",
    "    if return_shell_output:\n",
    "        code_next = code.replace('\\n', '\\n    ')  # Indent the existing code\n",
    "        # Add a try...except block with proper indentation\n",
    "        code_next = \"\\ntry:\\n    from sympy import *\\n    {}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code_next)\n",
    "    \n",
    "    if not return_shell_output:\n",
    "        print(code)\n",
    "    with open('code.py', 'w') as fout:\n",
    "        fout.write(code)\n",
    "    \n",
    "    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "    try:\n",
    "        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "        return_value = return_last_print(shell_output, -1)\n",
    "        print(shell_output)\n",
    "        if return_shell_output:\n",
    "            if return_value=='FAIL':\n",
    "                CODE_STATUS = False\n",
    "                return_value = return_last_print(shell_output, -2)\n",
    "                if \"not defined\" in return_value:\n",
    "                    return_value+='\\nTry checking the formatting and imports'\n",
    "            else:\n",
    "                CODE_STATUS = True\n",
    "            return return_value, CODE_STATUS  \n",
    "        code_output = round(float(eval(return_value))) % 1000\n",
    "    except Exception as e:\n",
    "        print(e,'shell_output')\n",
    "        code_output = -1\n",
    "    \n",
    "    if return_shell_output:\n",
    "        if code_output==-1:\n",
    "            CODE_STATUS = False\n",
    "        else:\n",
    "            CODE_STATUS = True\n",
    "        return code_output, CODE_STATUS  \n",
    "    \n",
    "    \n",
    "    return code_output\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "n_repetitions = 19 if PRIVATE else 4 # Original notebook had 22 but times out :(\n",
    "TOTAL_TOKENS = 2048 # if PRIVATE else 512\n",
    "\n",
    "if PRIVATE:\n",
    "    TIME_LIMIT = 31500\n",
    "else:\n",
    "    TIME_LIMIT = 1\n",
    "if PRIVATE:\n",
    "\n",
    "    MODEL_PATH = \"/kaggle/input/policy-model\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "    DEEP = True\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "    config.gradient_checkpointing = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    device_map = [('model.embed_tokens', 0),\n",
    "                 ('model.layers.0', 0),\n",
    "                 ('model.layers.1', 0),\n",
    "                 ('model.layers.2', 0),\n",
    "                 ('model.layers.3', 0),\n",
    "                 ('model.layers.4', 0),\n",
    "                 ('model.layers.5', 0),\n",
    "                 ('model.layers.6', 0),\n",
    "                 ('model.layers.7', 0),\n",
    "                 ('model.layers.8', 0),\n",
    "                 ('model.layers.9', 0),\n",
    "                 ('model.layers.10', 0),\n",
    "                 ('model.layers.11', 0),\n",
    "                 ('model.layers.12', 0),\n",
    "                 ('model.layers.13', 0),\n",
    "                 ('model.layers.14', 0),\n",
    "                 ('model.layers.15', 0),\n",
    "                 ('model.layers.16', 0),\n",
    "                 ('model.layers.17', 0),\n",
    "                 ('model.layers.18', 0),\n",
    "                 ('model.layers.19', 0),\n",
    "                 ('model.layers.20', 0),\n",
    "                 ('model.layers.21', 0),\n",
    "                 ('model.layers.22', 1),\n",
    "                 ('model.layers.23', 1),\n",
    "                 ('model.layers.24', 1),\n",
    "                 ('model.layers.25', 1),\n",
    "                 ('model.layers.26', 1),\n",
    "                 ('model.layers.27', 1),\n",
    "                 ('model.layers.28', 1),\n",
    "                 ('model.layers.29', 1),\n",
    "                 ('model.norm', 1),\n",
    "                 ('lm_head', 1)]\n",
    "\n",
    "    device_map = {ii:jj for (ii,jj) in device_map}\n",
    "\n",
    "    if QUANT:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"sequential\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True, \n",
    "            quantization_config=quantization_config,\n",
    "            config=config\n",
    "        )\n",
    "    else:  \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            #quantization_config=quantization_config,\n",
    "            config=config\n",
    "        )\n",
    "    \n",
    "    pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype='auto',\n",
    "    device_map=device_map,\n",
    ")\n",
    "    from transformers import StoppingCriteriaList\n",
    "\n",
    "    class StoppingCriteriaSub(StoppingCriteria):\n",
    "        def __init__(self, stops = [], encounters=1):\n",
    "            super().__init__()\n",
    "            self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "            for stop in self.stops:\n",
    "                if stop.dim() == 0:\n",
    "                    last_token = input_ids[0][-1]\n",
    "                else:\n",
    "                    last_token = input_ids[0][-len(stop):]\n",
    "                if torch.all(torch.eq(stop,last_token)):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "\n",
    "    stop_words = [tokenizer.eos_token,\"```output\",\"```Output\",\"```output\\n\",\"```Output\\n\",\"```\\nOutput\" , \")\\n```\" , \"``````output\",\"``````Output\"]  \n",
    "    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    model.dtype, model.hf_device_map\n",
    "    \n",
    "def code(problem):\n",
    "    return \"User: \"+problem+\"\"\"\\n\n",
    "Determine a sympy-based approach for solving the problem. When defining symbol, incorporate all constraints mentioned in the problem statement, e.g. real, integer, even, odd, positive, prime. If a variable represents a positive integer, Symbol('n', integer=True, positive=True). Your final answer should be integer, not expression, list, tuple or dictionary!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the result.At last output the final answer is \\\\boxed{}.\\n\\nAssistant:\"\"\"\n",
    "\n",
    "def cot(problem):\n",
    "    return \"User: \"+problem+'''\\n\n",
    "Carefully read and understand the problem and use all information in problem statement. No Python code. Show your work step-by-step, explain your reasoning, calculations, mathematical concepts and formulas in detail.\n",
    "At last output the final answer is \\\\boxed{}.\\n\\nAssistant:\n",
    "'''\n",
    "promplt_options = [code,cot]\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "\n",
    "def HasAnswer(text):\n",
    "    text = text[-77:]\n",
    "    patterns = [\n",
    "        r'answer is.*\\\\boxed\\{(.*?)\\}',\n",
    "        r\"answer is[:\\s]*\\$(.+?)\\$\",\n",
    "        r\"answer is[:\\s]*(.+)\",\n",
    "        r'print\\((\\d+)\\)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def process_text_output(text):\n",
    "    text = text[-77:]\n",
    "    patterns = [\n",
    "        r'answer is.*\\\\boxed\\{(.*?)\\}',\n",
    "        r\"answer is[:\\s]*\\$(.+?)\\$\",\n",
    "        r\"answer is[:\\s]*(.+)\",\n",
    "        r'print\\((\\d+)\\)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = list(re.finditer(pattern, text))\n",
    "        if match:\n",
    "            out = match[-1].group(1)\n",
    "            try:\n",
    "                out = int(out) % 1000\n",
    "                return out\n",
    "            except:\n",
    "                return -1\n",
    "    return -1\n",
    "\n",
    "def extract_code(text):\n",
    "    splits = text.split('```')\n",
    "    if len(splits) < 2:\n",
    "        text = text.split(\"Assistant:\")[1]\n",
    "        match = re.search(r\"print\\(.+?\\)\", text)  # Non-greedy match within parentheses\n",
    "        if match:\n",
    "            return text[:match.end()].strip()\n",
    "        return \"no code\"\n",
    "    return text.split('```')[1][7:]\n",
    "\n",
    "temperature = 0.9\n",
    "top_p = 1.0\n",
    "\n",
    "temperature_coding = 0.9\n",
    "top_p_coding = 1.0\n",
    "\n",
    "   \n",
    "total_results = {}\n",
    "total_answers = {}\n",
    "best_stats = {}\n",
    "total_outputs = {}\n",
    "question_type_counts = {}\n",
    "starting_counts = (2,3)\n",
    "    \n",
    "for i, (test, sample_submission) in tqdm(enumerate(iter_test)):\n",
    "    # print(f\"Solving problem {i} ...\")\n",
    "    TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n",
    "\n",
    "    if TIME_SPENT>TIME_LIMIT:\n",
    "        sample_submission['answer'] = 37\n",
    "        env.predict(sample_submission)\n",
    "        continue\n",
    "        \n",
    "    for jj in tqdm(range(n_repetitions)):   \n",
    "        problem = test['problem'].values[0]\n",
    "        # print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n",
    "        \n",
    "        best, best_count = best_stats.get(i,(-1,-1))\n",
    "        if best_count>np.sqrt(jj):\n",
    "            # print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n",
    "            continue\n",
    "            \n",
    "        outputs = total_outputs.get(i,[])\n",
    "        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n",
    "        results = total_results.get(i,[])\n",
    "        answers = total_answers.get(i,[])\n",
    "        \n",
    "        for _ in range(5):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        try:\n",
    "            ALREADY_GEN = 0\n",
    "            code_error = None\n",
    "            code_error_count = 0\n",
    "            code_output = -1\n",
    "            #initail_message = problem  + tool_instruction \n",
    "            counts = np.array([text_answers,code_answers])\n",
    "\n",
    "            draw = choice(promplt_options, 1,\n",
    "                          p=counts/counts.sum())\n",
    "\n",
    "            prompt = draw[0](problem)\n",
    "            current_printed = len(prompt)\n",
    "            # print(f\"{jj}_{prompt}\\n\")\n",
    "\n",
    "            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "            input_len = len(model_inputs['input_ids'][0])\n",
    "\n",
    "            generation_output = model.generate(**model_inputs, \n",
    "                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n",
    "                                               return_dict_in_generate=USE_PAST_KEY,\n",
    "                                               do_sample = True,\n",
    "                                               temperature = temperature,\n",
    "                                               top_p = top_p,\n",
    "                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n",
    "\n",
    "            if USE_PAST_KEY:\n",
    "                output_ids = generation_output.sequences[0]\n",
    "            else:\n",
    "                output_ids = generation_output[0]\n",
    "            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            # print(f\"decoded_output:{decoded_output[current_printed:]}\\n\")\n",
    "            current_printed += len(decoded_output[current_printed:])\n",
    "            cummulative_code = \"\"\n",
    "            answered = HasAnswer(decoded_output)\n",
    "                \n",
    "            \n",
    "            while not answered and (ALREADY_GEN<(TOTAL_TOKENS)):\n",
    "                code = extract_code(decoded_output)\n",
    "                if code == 'no code':\n",
    "                    temperature_inner=temperature_coding\n",
    "                    top_p_inner = top_p_coding\n",
    "                    prompt = decoded_output\n",
    "                else:\n",
    "                    temperature_inner=temperature\n",
    "                    top_p_inner = top_p\n",
    "                    try:\n",
    "                        cummulative_code+=code\n",
    "                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n",
    "                        # print('CODE RESULTS', code_output)\n",
    "\n",
    "                        if code_error==code_output:\n",
    "                            code_error_count+=1\n",
    "                        else:\n",
    "                            code_error=code_output\n",
    "                            code_error_count = 0\n",
    "\n",
    "                        if not CODE_STATUS:\n",
    "                            cummulative_code = cummulative_code[:-len(code)]\n",
    "\n",
    "                            if code_error_count>=1:\n",
    "                                # print(\"REPEATED ERRORS\")\n",
    "                                break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # print(e)\n",
    "                        # print('ERROR PARSING CODE')\n",
    "                        code_output = -1\n",
    "\n",
    "                    if code_output!=-1:\n",
    "                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n",
    "                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n",
    "                        else:\n",
    "                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n",
    "                    else:\n",
    "                        prompt = decoded_output\n",
    "                        cummulative_code=\"\"\n",
    "\n",
    "\n",
    "                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    old_values = generation_output.past_key_values\n",
    "                else:\n",
    "                    old_values = None\n",
    "\n",
    "                generation_output = model.generate(**model_inputs, \n",
    "                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n",
    "                                                   return_dict_in_generate=USE_PAST_KEY,\n",
    "                                                   past_key_values=old_values,\n",
    "                                                   do_sample = True,\n",
    "                                                   temperature = temperature_inner,\n",
    "                                                   top_p = top_p_inner,\n",
    "                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n",
    "\n",
    "                if USE_PAST_KEY:\n",
    "                    output_ids = generation_output.sequences[0]\n",
    "                else:\n",
    "                    output_ids = generation_output[0]\n",
    "                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                # print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n",
    "                current_printed+=len(decoded_output[current_printed:])\n",
    "                answered = HasAnswer(decoded_output)\n",
    "\n",
    "            if USE_PAST_KEY:\n",
    "                output_ids = generation_output.sequences[0]\n",
    "            else:\n",
    "                output_ids = generation_output[0]\n",
    "\n",
    "            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n",
    "            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n",
    "            result_output = process_text_output(raw_output)\n",
    "            \n",
    "            try:\n",
    "                code_output = round(float(eval(code_output))) % 1000\n",
    "            except Exception as e:\n",
    "                print(e,'final_eval')\n",
    "                code_output = -1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e,\"5\")\n",
    "            result_output, code_output = -1, -1\n",
    "\n",
    "        if code_output!=-1:\n",
    "            outputs.append(code_output)\n",
    "            code_answers+=1\n",
    "\n",
    "        if result_output!=-1:\n",
    "            outputs.append(result_output)\n",
    "            text_answers+=1\n",
    "\n",
    "        if len(outputs) > 0:\n",
    "            occurances = Counter(outputs).most_common()\n",
    "            # print(occurances)\n",
    "            if occurances[0][1] > best_count:\n",
    "                # print(\"GOOD ANSWER UPDATED!\")\n",
    "                best = occurances[0][0]\n",
    "                best_count = occurances[0][1]\n",
    "            if occurances[0][1] > 5:\n",
    "                # print(\"ANSWER FOUND!\")\n",
    "                break\n",
    "\n",
    "        results.append(result_output)\n",
    "        answers.append(code_output)\n",
    "        \n",
    "        best_stats[i] = (best, best_count) \n",
    "        question_type_counts[i] = (text_answers, code_answers)\n",
    "        total_outputs[i] = outputs\n",
    "        \n",
    "        total_results[i] = results\n",
    "        total_answers[i] = answers\n",
    "\n",
    "        # print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n",
    "        if DEBUG:\n",
    "            break\n",
    "            \n",
    "    # print(f\"Predicted best answer: {best_stats}\")\n",
    "    sample_submission['answer'] = best_stats[i][0]\n",
    "    env.predict(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f736b9",
   "metadata": {
    "papermill": {
     "duration": 0.011401,
     "end_time": "2024-06-18T21:12:09.411203",
     "exception": false,
     "start_time": "2024-06-18T21:12:09.399802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4281572,
     "sourceId": 7369493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4720595,
     "sourceId": 8012825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4748944,
     "sourceId": 8052555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5136886,
     "sourceId": 8720688,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11261,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11264,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 276.282215,
   "end_time": "2024-06-18T21:12:12.093861",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-18T21:07:35.811646",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04e5d3baa7ff47e5b5dbd3fa293d9856": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_531cb4191c2b425a90ada5eb7d5b377d",
       "placeholder": "​",
       "style": "IPY_MODEL_2f0776bad6d445308826692497a1bf59",
       "value": " 3/3 [02:34&lt;00:00, 49.94s/it]"
      }
     },
     "0af52225ed624bf38c476aa058f1fadc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f0776bad6d445308826692497a1bf59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3db26c5672bf48e1b6b4e7641ab9b004": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0af52225ed624bf38c476aa058f1fadc",
       "placeholder": "​",
       "style": "IPY_MODEL_6fb41eff3e2c4110a269dd2df28d2045",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "531cb4191c2b425a90ada5eb7d5b377d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6164f8b80a2742178866b4803a8233d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d19c798bb044766805ae2f30bc47b58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3db26c5672bf48e1b6b4e7641ab9b004",
        "IPY_MODEL_8817904124b2413e8b59f5972d66ff8c",
        "IPY_MODEL_04e5d3baa7ff47e5b5dbd3fa293d9856"
       ],
       "layout": "IPY_MODEL_d16a476be5d941d2b7ba816de938b7f5"
      }
     },
     "6fb41eff3e2c4110a269dd2df28d2045": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8817904124b2413e8b59f5972d66ff8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6164f8b80a2742178866b4803a8233d1",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9266a3684a344af2972b91498399cd64",
       "value": 3.0
      }
     },
     "9266a3684a344af2972b91498399cd64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d16a476be5d941d2b7ba816de938b7f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
