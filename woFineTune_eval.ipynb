{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../Data/OlympiadBench_Dataset/data/outputs.json','../Data/AMC/outputs.json','../Data/MATH/outputs.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        # Load the list from the JSON file\n",
    "        texts.extend(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tokenizer.batch_encode_plus(texts,return_attention_mask=False,add_special_tokens=True,\\\n",
    "                                    truncation=True, max_length=4096)['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import math\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    # AutoConfig,\n",
    ")\n",
    "from functions import create_next_model_folder\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80a64501e8b436c8bb385ab8f5f7e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "# config.gradient_checkpointing = True\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_consecutive_chunk(input_list, max_length):\n",
    "    if len(input_list) <= max_length:\n",
    "        return input_list\n",
    "    max_start_index = len(input_list) - max_length\n",
    "    start_index = random.randint(0, max_start_index)\n",
    "    out = input_list[start_index:start_index + max_length]\n",
    "    out[0] = input_list[0] # Start of sentence\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 1023: train loss 1.239826615070342\n",
      "epoch 0 iter 2047: train loss 1.2475226878159447\n",
      "epoch 0 iter 3071: train loss 1.280505065456964\n",
      "epoch 0 iter 4095: train loss 1.2600050553301116\n",
      "epoch 0 iter 5119: train loss 1.212381665056455\n",
      "epoch 0 iter 6143: train loss 1.2290100864629494\n",
      "epoch 0 iter 7167: train loss 1.2642828941025073\n",
      "epoch 0 iter 8191: train loss 1.2603929835167946\n",
      "epoch 0 iter 9215: train loss 1.260551017941907\n",
      "epoch 0 iter 10239: train loss 1.2473127340199426\n",
      "epoch 0 iter 11263: train loss 1.2428824665985303\n",
      "epoch 0 iter 12287: train loss 1.1943137583730277\n",
      "epoch 0 iter 13311: train loss 1.2367846277193166\n",
      "epoch 0 iter 14335: train loss 1.249982453766279\n",
      "epoch 0 iter 15359: train loss 1.2675882217881735\n",
      "epoch 0 iter 16383: train loss 1.243489224521909\n",
      "epoch 0 iter 17407: train loss 1.241614089012728\n",
      "epoch 0 iter 18431: train loss 1.2548915603838395\n",
      "epoch 0 iter 19455: train loss 1.2316435200918932\n",
      "epoch 0 iter 20479: train loss 1.2304505299252924\n",
      "epoch 0 iter 21503: train loss 1.2282017485413235\n",
      "epoch 0 iter 22527: train loss 1.2521355719072744\n",
      "epoch 0 iter 23551: train loss 1.2723122937604785\n",
      "epoch 0 iter 24575: train loss 1.2805807028344134\n",
      "epoch 0 iter 25599: train loss 1.24684621833876\n",
      "epoch 0 iter 26623: train loss 1.2663864695932716\n",
      "end of epoch 0, loss: 1.2473670012192322\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(texts)\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_last = 0\n",
    "    skip = 0\n",
    "    tot_skip = 0\n",
    "    # for llm, batchsize = 1 still gives 100 GPU util\n",
    "    for i,input_ids in enumerate(texts):\n",
    "        input_ids = sample_consecutive_chunk(input_ids,1200)\n",
    "        input_ids = torch.tensor(input_ids).to('cuda')[None]\n",
    "        with torch.no_grad():\n",
    "            outs = model(input_ids).logits\n",
    "            loss = loss_fn(outs[0,:-1],input_ids[0,1:])\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # eval    \n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"epoch {epoch} iter {i}: train loss {(train_loss-train_last)/(verbose-skip)}\")\n",
    "            train_last = train_loss\n",
    "            tot_skip += skip\n",
    "            skip = 0            \n",
    "    print(f'end of epoch {epoch}, loss: {train_loss/(i-tot_skip)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_model_id = \"../Model/FT/model2\"\n",
    "# from peft import PeftModel\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM, \n",
    "# )\n",
    "# # model = ... load base model\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
