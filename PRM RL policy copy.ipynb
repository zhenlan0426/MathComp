{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MAX_LEN = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"4\"\n",
    "MODEL_PATH = \"deepseek-ai/deepseek-math-7b-rl\" # TODO: f\"../Model/PRM_LORA{version}_merged_code_policy_1SFT\"\n",
    "next_version = str(int(version) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data: {1} + SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# {0,1}\n",
    "with open(f\"../llmOutputs/PRM/completed_paths_y_code{version}.pickle\", \"rb\") as f:\n",
    "    completed_paths_y = pickle.load(f)\n",
    "\n",
    "texts = []\n",
    "for y,score,text,code,prob_i,exit_i in completed_paths_y:\n",
    "    if y == 1:\n",
    "        texts.append(text.replace(\"<｜begin▁of▁sentence｜>User: \",\"\"))\n",
    "\n",
    "# separate out question and solution and only train on solution\n",
    "patterns = [r\"``` and should only print the final answer.\",\\\n",
    "            r\"print the final result.\\nApproach:\",\\\n",
    "            r\"print the final output, as an integer not other python object such as list or tuple.\"]\n",
    "\n",
    "def search_patterns(text, patterns):\n",
    "    for pattern in patterns:\n",
    "        # Compile the pattern\n",
    "        regex = re.compile(pattern)\n",
    "        # Find all matches of the pattern in the text\n",
    "        matches = list(regex.finditer(text))\n",
    "        # If there is one match, get the end position\n",
    "        if matches:\n",
    "            return matches[0].end()\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for text in texts:\n",
    "    idx = search_patterns(text,patterns)\n",
    "    question = tokenizer.encode(text[:idx],add_special_tokens=True)\n",
    "    answer = tokenizer.encode(text[idx:],add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT\n",
    "def gen_prompt_codeIn1(problem):\n",
    "    return f\"\"\"Problem: {problem}\\n\n",
    "To accomplish this, first determine a python-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and your final answer should be integer, not expression, list, tuple or dictionary!\n",
    "Write the entire script covering all the steps (use comments and document it well) and print the final result.\n",
    "Approach:\"\"\"\n",
    "\n",
    "def gen_prompt_codeIn2(problem):\n",
    "    return f\"\"\"Problem: {problem}\\n\n",
    "You are an expert at solving math problem. Analyze this problem and think step by step to develop a python solution. Your solution should include reasoning steps in Python comments, explaining your thought process and the mathematical principles you applied. print the final output, as an integer not other python object such as list or tuple.\"\"\"\n",
    "\n",
    "def add_prompt(problem):\n",
    "    if np.random.rand()<0.5:\n",
    "        return gen_prompt_codeIn1(problem)\n",
    "    else:\n",
    "        return gen_prompt_codeIn2(problem)\n",
    "    \n",
    "sft = pd.read_csv(\"../Data/MATH/math.csv\")\n",
    "sft = sft.loc[sft.boxed_number == sft.parsed] \n",
    "sft['prob_wPrompt'] = sft.problem.apply(add_prompt)\n",
    "for q,a in zip(sft.prob_wPrompt.tolist(),sft.code_solution.tolist()):\n",
    "    question = tokenizer.encode(q,add_special_tokens=True)\n",
    "    answer = tokenizer.encode(a,add_special_tokens=False)\n",
    "    lengths.append(len(question))\n",
    "    input_ids.append(question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_gen(texts,lengths):\n",
    "    data = list(zip(texts,lengths))\n",
    "    random.shuffle(data)\n",
    "    for text,l in data:\n",
    "        text = torch.tensor(text[:MAX_LEN],device='cuda')[None]\n",
    "        yield text,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 2e-5\n",
    "clip = 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eec3c735c044b00b6b6ece4907bd0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            trust_remote_code=True,\n",
    "                                            attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ],\n",
    "                        #  use_dora=True,\n",
    "                        )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.5784544173631734\n",
      "iter: 2047, \n",
      " train loss: 0.5092757509264629\n",
      "iter: 3071, \n",
      " train loss: 0.47744069523832877\n",
      "iter: 4095, \n",
      " train loss: 0.4225507049341104\n",
      "iter: 5119, \n",
      " train loss: 0.40374303952557966\n",
      "iter: 6143, \n",
      " train loss: 0.36452333986380836\n",
      "iter: 7167, \n",
      " train loss: 0.35588240875222255\n",
      "iter: 8191, \n",
      " train loss: 0.3458845899244616\n"
     ]
    }
   ],
   "source": [
    "def empty_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import math\n",
    "import gc\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "train_loss = 0\n",
    "count_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,l) in enumerate(from_gen(input_ids,lengths)):\n",
    "        if i > 0:\n",
    "            del outs,loss\n",
    "            empty_cache()\n",
    "        outs = model(text).logits # 1,l,C\n",
    "        loss = loss_fn(outs[0,l:-1],text[0,l+1:]) # (l,C), (l,)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()): continue\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        count_loss += 1\n",
    "            \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/count_loss}\")\n",
    "            train_loss = 0\n",
    "            count_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/torch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_version = str(int(version) + 1)\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_1SFT\"\n",
    "# !mkdir peft_model_id\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fca21693524951a1c7b5f2b86775e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model,texts,outs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\\\n",
    "                                    device_map=\"auto\",\n",
    "                                    torch_dtype=\"auto\",\n",
    "                                    attn_implementation=\"flash_attention_2\"\n",
    "                                    )\n",
    "from peft import PeftModel\n",
    "peft_model_id = f\"../Model/PRM_LORA{next_version}_code_policy_1SFT\"\n",
    "base_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "base_model2 = base_model.merge_and_unload()\n",
    "base_model2.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_1SFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../Model/PRM_LORA5_merged_code_policy_1SFT/tokenizer_config.json',\n",
       " '../Model/PRM_LORA5_merged_code_policy_1SFT/special_tokens_map.json',\n",
       " '../Model/PRM_LORA5_merged_code_policy_1SFT/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")\n",
    "tokenizer.save_pretrained(f\"../Model/PRM_LORA{next_version}_merged_code_policy_1SFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
