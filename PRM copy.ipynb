{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "from datasets import load_dataset,load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen data\n",
    "with open('../Data/PRM_data/gen_texts.pkl', 'rb') as file:\n",
    "    gen_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_targets.pkl', 'rb') as file:\n",
    "    gen_targets = pickle.load(file)\n",
    "with open('../Data/PRM_data/gen_starts_ends.pkl', 'rb') as file:\n",
    "    gen_starts_ends = pickle.load(file)\n",
    "\n",
    "# sol data\n",
    "with open('../Data/PRM_data/sol_texts.pkl', 'rb') as file:\n",
    "    sol_texts = pickle.load(file)\n",
    "with open('../Data/PRM_data/sol_starts_ends.pkl', 'rb') as file:\n",
    "    sol_starts_ends = pickle.load(file)\n",
    "\n",
    "# Math-Shepherd\n",
    "dataset = load_dataset('../Data/Math-Shepherd')\n",
    "\n",
    "# MMOS\n",
    "dataset2 = load_from_disk('../Data/MMOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1200\n",
    "def shuffle_lists(*args):\n",
    "    combined = list(zip(*args))\n",
    "    random.shuffle(combined)\n",
    "    return list(zip(*combined))\n",
    "\n",
    "def np2torch(input,addBatchDim=True):\n",
    "    if addBatchDim:\n",
    "        return torch.tensor(input,device='cuda')[None]\n",
    "    else:\n",
    "        return torch.tensor(input,device='cuda')\n",
    "\n",
    "def from_shepherd(dataset):\n",
    "    # yield token_id, index, target, data_source\n",
    "    dataset = dataset.shuffle()\n",
    "    for data in dataset['train']:\n",
    "        if len(data['index']) != len(data['targets']): continue\n",
    "        if max(data['index']) > MAX_LEN:\n",
    "            out = list(zip(*[(d,t) for d,t in zip(data['index'],data['targets']) if d<MAX_LEN]))\n",
    "            if len(out) == 2:\n",
    "                index,targets = out\n",
    "            else:\n",
    "                continue # out is [] -> all index > MAX_LEN\n",
    "        else:\n",
    "            index,targets = data['index'],data['targets']\n",
    "        yield np2torch(data['input_id'][:max(index)+1]),np2torch(index,False),np2torch(targets).float(),0\n",
    "\n",
    "def from_mmos(dataset,num_of_points=5):\n",
    "    # yield token_id, index, target, data_source\n",
    "    dataset = dataset.shuffle()\n",
    "    for data in dataset['train']:\n",
    "        text = data['input_id']\n",
    "        start,end = data['starts_ends']\n",
    "        end = min(end,MAX_LEN)\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            continue\n",
    "        else: \n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # targets = np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text[:max(index)+1]),np2torch(index,False),\\\n",
    "                    torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),3\n",
    "\n",
    "def from_sol(texts,starts_ends,num_of_points=5):\n",
    "    texts,starts_ends = shuffle_lists(texts,starts_ends)\n",
    "    for text,(start,end) in zip(texts,starts_ends):\n",
    "        end = min(end,MAX_LEN)\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            continue\n",
    "        else: \n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # targets = np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text[:max(index)+1]),np2torch(index,False),\\\n",
    "                    torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),1\n",
    "        \n",
    "def from_genData(texts,targets,starts_ends,num_of_points=5):\n",
    "    texts,targets,starts_ends = shuffle_lists(texts,targets,starts_ends)\n",
    "    for text,y,(start,end) in zip(texts,targets,starts_ends):\n",
    "        end = min(end,MAX_LEN)\n",
    "        if start>=end:# use entire sol when it is shorter than 10\n",
    "            continue\n",
    "        else:\n",
    "            index = np.random.randint(start,end,num_of_points)\n",
    "            # target = y * np.exp(-(end-index)/end) # discount\n",
    "            yield np2torch(text[:max(index)+1]),np2torch(index,False),\\\n",
    "                    y*torch.ones((1,num_of_points),device='cuda',dtype=torch.float32),2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained('deepseek-ai/deepseek-math-7b-rl',\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       torch_dtype=\"auto\",\\\n",
    "                                                       attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "model.score = model.score.float()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.score.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, loss: [0.6983305301484475, 0.7101281848121598, 0.7084300920061352]\n",
      "iter: 2047, loss: [0.6851908006863288, 0.6916047665031075, 0.7124841071643437]\n",
      "iter: 3071, loss: [0.6872878611262593, 0.6627943027786344, 0.719804101500693]\n",
      "iter: 4095, loss: [0.6924264471202303, 0.6356581202175611, 0.7236687314440633]\n",
      "iter: 5119, loss: [0.6937833166958993, 0.6292938889820905, 0.7132304342261507]\n",
      "iter: 6143, loss: [0.6900924494888776, 0.6158084587918388, 0.707914371119916]\n",
      "iter: 7167, loss: [0.688401895009877, 0.6027550736655238, 0.709160310134553]\n",
      "iter: 8191, loss: [0.6910998361152515, 0.6003385509563681, 0.6963855191584556]\n",
      "iter: 9215, loss: [0.6799317163106633, 0.5954016733762116, 0.6894079299091943]\n",
      "iter: 10239, loss: [0.6862649718337744, 0.5795238888508414, 0.6891365681301084]\n",
      "iter: 11263, loss: [0.6809413582957976, 0.5622165842839351, 0.6912893049003791]\n",
      "iter: 12287, loss: [0.6772415956094467, 0.5540770841272253, 0.6917170327255103]\n",
      "iter: 13311, loss: [0.6718460233155583, 0.5399993615241344, 0.697696793671937]\n",
      "iter: 14335, loss: [0.67761338053391, 0.5347885759520041, 0.695259722446766]\n",
      "iter: 15359, loss: [0.6802033429796045, 0.5261828938597127, 0.6839521568366859]\n",
      "iter: 16383, loss: [0.6660483233914697, 0.5267328168989277, 0.6634416073037867]\n",
      "iter: 17407, loss: [0.6664484161556813, 0.5090130091237882, 0.6688284661588082]\n",
      "iter: 18431, loss: [0.6794882827490306, 0.48941919512567467, 0.6859997877102794]\n",
      "iter: 19455, loss: [0.6631345910585521, 0.5029350039371647, 0.6714248845451757]\n",
      "iter: 20479, loss: [0.6719226281196751, 0.48861458670358854, 0.6666884588356242]\n",
      "iter: 21503, loss: [0.6642173840677983, 0.48814652834022254, 0.6593430133159559]\n",
      "iter: 22527, loss: [0.6690172856329474, 0.4850848616043494, 0.6508324335367359]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in zip(from_shepherd(dataset),\\\n",
    "                    from_sol(sol_texts,sol_starts_ends),\\\n",
    "                    from_genData(gen_texts,gen_targets,gen_starts_ends)):\n",
    "        for d in data:\n",
    "            # if d is None: continue # zip_longest will return None for shorter iterable\n",
    "            text,index,target,source = d\n",
    "            hidden_states = model.model(text)[0].float()\n",
    "            logits = model.score(hidden_states)[:,index,0]\n",
    "            loss = loss_fn(logits,target)\n",
    "            loss.backward()\n",
    "            \n",
    "            train_loss[source] += loss.item()\n",
    "            count_loss[source] += 1\n",
    "            i += 1\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(model.score.parameters(),clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            if (i + 1) % verbose == 0:\n",
    "                print(f\"iter: {i}, loss: {[l/c if c!=0 else 'N/A' for l,c in zip(train_loss,count_loss)]}\")\n",
    "                train_loss = [0,0,0]\n",
    "                count_loss = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../Model/PRM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "alpha_factor = 4.0\n",
    "accumulation_steps = 64\n",
    "verbose = 1024\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "topics_num = 4\n",
    "weights=[0.5,0.2,0.2,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification,BitsAndBytesConfig,AutoConfig\n",
    "import torch\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a64a774549c409e98216c1408327d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = LlamaForSequenceClassification.from_pretrained('../Model/PRM',\\\n",
    "                                                       num_labels=1,\\\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       torch_dtype=\"auto\",\n",
    "                                                       quantization_config=quantization_config,\n",
    "                                                       attn_implementation=\"flash_attention_2\"\n",
    "                                                       )\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,739,200 || all params: 6,509,674,496 || trainable%: 0.287866928085493\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(r=8, # low rank \n",
    "                         lora_alpha = 16, # see below \n",
    "                         lora_dropout = 0.1, \n",
    "                         bias=\"none\",#'none', 'all' or 'lora_only' \n",
    "                         target_modules = [ \"q_proj\", \n",
    "                                            \"k_proj\", \n",
    "                                            \"v_proj\", \n",
    "                                            \"o_proj\", \n",
    "                                            \"gate_proj\", \n",
    "                                            \"up_proj\", \n",
    "                                            \"down_proj\" \n",
    "                                        ] \n",
    "                        )\n",
    "base_model = get_peft_model(model.model, peft_config)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.print_trainable_parameters()\n",
    "model.score = model.score.float()\n",
    "model.score.weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def sample_from_iterables(weights,*iterables):\n",
    "    while True:\n",
    "        iterable = random.choices(iterables, weights=weights, k=1)[0]\n",
    "        try:\n",
    "            yield next(iterable)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha  # Store alpha in the context\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.alpha, None  # Use stored alpha, return None for alpha's grad\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def forward(self, x, alpha):\n",
    "        return GradientReversalFunction.apply(x, alpha)\n",
    "\n",
    "class revLinear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(revLinear, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(input_dim, input_dim),nn.CELU(),nn.Linear(input_dim, output_dim))\n",
    "        self.grad_rev = GradientReversalLayer()\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        return self.layers(self.grad_rev(x, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model = revLinear(model.score.weight.shape[1],topics_num).to('cuda').float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [param for param in base_model.parameters() if param.requires_grad]\n",
    "trainable_params =  base_params + \\\n",
    "                    list(model.score.parameters())\n",
    "                    # list(topic_model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1023, \n",
      " train loss: 0.6580159974400885\n",
      "iter: 2047, \n",
      " train loss: 0.6415111924288794\n",
      "iter: 3071, \n",
      " train loss: 0.5934078045393107\n",
      "iter: 4095, \n",
      " train loss: 0.5148563318325614\n",
      "iter: 5119, \n",
      " train loss: 0.4851639556000009\n",
      "iter: 6143, \n",
      " train loss: 0.4959308831639646\n",
      "iter: 7167, \n",
      " train loss: 0.44096636618996854\n",
      "iter: 8191, \n",
      " train loss: 0.43119324900726497\n",
      "iter: 9215, \n",
      " train loss: 0.42783852473348816\n",
      "iter: 10239, \n",
      " train loss: 0.41793171603058\n",
      "iter: 11263, \n",
      " train loss: 0.38764809867461736\n",
      "iter: 12287, \n",
      " train loss: 0.38431894192444815\n",
      "iter: 13311, \n",
      " train loss: 0.3792582213463902\n",
      "iter: 14335, \n",
      " train loss: 0.3853120165790642\n",
      "iter: 15359, \n",
      " train loss: 0.3791149910425702\n",
      "iter: 16383, \n",
      " train loss: 0.39037251335412293\n",
      "iter: 17407, \n",
      " train loss: 0.3745948937043977\n",
      "iter: 18431, \n",
      " train loss: 0.3699627665919252\n",
      "iter: 19455, \n",
      " train loss: 0.37997911753427616\n",
      "iter: 20479, \n",
      " train loss: 0.36232350547061287\n",
      "iter: 21503, \n",
      " train loss: 0.3599839823541515\n",
      "iter: 22527, \n",
      " train loss: 0.35588806807163564\n",
      "iter: 23551, \n",
      " train loss: 0.363729654024155\n",
      "iter: 24575, \n",
      " train loss: 0.3513036070335147\n",
      "iter: 25599, \n",
      " train loss: 0.3702043626217346\n",
      "iter: 26623, \n",
      " train loss: 0.3375970507411239\n",
      "iter: 27647, \n",
      " train loss: 0.36255564908105953\n",
      "iter: 28671, \n",
      " train loss: 0.3742576848489989\n",
      "iter: 29695, \n",
      " train loss: 0.35945183976059525\n",
      "iter: 30719, \n",
      " train loss: 0.3303105464769942\n",
      "iter: 31743, \n",
      " train loss: 0.35175313287618337\n",
      "iter: 32767, \n",
      " train loss: 0.3348795379524745\n",
      "iter: 33791, \n",
      " train loss: 0.35314198803962427\n",
      "iter: 34815, \n",
      " train loss: 0.37356516793897754\n",
      "iter: 35839, \n",
      " train loss: 0.334336210022002\n",
      "iter: 36863, \n",
      " train loss: 0.34692502664188396\n",
      "iter: 37887, \n",
      " train loss: 0.35770038145255967\n",
      "iter: 38911, \n",
      " train loss: 0.33799798429936345\n",
      "iter: 39935, \n",
      " train loss: 0.31977283933156286\n",
      "iter: 40959, \n",
      " train loss: 0.3497250914817869\n",
      "iter: 41983, \n",
      " train loss: 0.33203337037821257\n",
      "iter: 43007, \n",
      " train loss: 0.3076027496781535\n",
      "iter: 44031, \n",
      " train loss: 0.3455345031743491\n",
      "iter: 45055, \n",
      " train loss: 0.3322520410163179\n",
      "iter: 46079, \n",
      " train loss: 0.33196727400536474\n",
      "iter: 47103, \n",
      " train loss: 0.35057080735737145\n",
      "iter: 48127, \n",
      " train loss: 0.32414315507719493\n",
      "iter: 49151, \n",
      " train loss: 0.32332484592052424\n",
      "iter: 50175, \n",
      " train loss: 0.3601031469652298\n",
      "iter: 51199, \n",
      " train loss: 0.33110447426679457\n",
      "iter: 52223, \n",
      " train loss: 0.3286141971143479\n",
      "iter: 53247, \n",
      " train loss: 0.33318311088623886\n",
      "iter: 54271, \n",
      " train loss: 0.3388760014054242\n",
      "iter: 55295, \n",
      " train loss: 0.3354968413696042\n",
      "iter: 56319, \n",
      " train loss: 0.33559198874377216\n",
      "iter: 57343, \n",
      " train loss: 0.31854866122375824\n",
      "iter: 58367, \n",
      " train loss: 0.32973831655999675\n",
      "iter: 59391, \n",
      " train loss: 0.3126752934567776\n",
      "iter: 60415, \n",
      " train loss: 0.3454164292107862\n",
      "iter: 61439, \n",
      " train loss: 0.3302470355163223\n",
      "iter: 62463, \n",
      " train loss: 0.3212001568336973\n",
      "iter: 63487, \n",
      " train loss: 0.32081017712891935\n",
      "iter: 64511, \n",
      " train loss: 0.3080612129825795\n",
      "iter: 65535, \n",
      " train loss: 0.30644632145606465\n",
      "iter: 66559, \n",
      " train loss: 0.3237285777846637\n",
      "iter: 67583, \n",
      " train loss: 0.3032658897534475\n",
      "iter: 68607, \n",
      " train loss: 0.3091934371557272\n",
      "iter: 69631, \n",
      " train loss: 0.3110261849900553\n",
      "iter: 70655, \n",
      " train loss: 0.32265433676923294\n",
      "iter: 71679, \n",
      " train loss: 0.3150660263630698\n",
      "iter: 72703, \n",
      " train loss: 0.3032327932572798\n",
      "iter: 73727, \n",
      " train loss: 0.305297705024941\n",
      "iter: 74751, \n",
      " train loss: 0.32705904424506116\n",
      "iter: 75775, \n",
      " train loss: 0.297307789897161\n",
      "iter: 76799, \n",
      " train loss: 0.3316621695497588\n",
      "iter: 77823, \n",
      " train loss: 0.3068464861684106\n",
      "iter: 78847, \n",
      " train loss: 0.30799773516866935\n",
      "iter: 79871, \n",
      " train loss: 0.3200336185799415\n",
      "iter: 80895, \n",
      " train loss: 0.3259802861717276\n",
      "iter: 81919, \n",
      " train loss: 0.30983894768110076\n",
      "iter: 82943, \n",
      " train loss: 0.3023426868759316\n",
      "iter: 83967, \n",
      " train loss: 0.2920564187511445\n",
      "iter: 84991, \n",
      " train loss: 0.3168632283103534\n",
      "iter: 86015, \n",
      " train loss: 0.2941260061021467\n",
      "iter: 87039, \n",
      " train loss: 0.2954494763891944\n",
      "iter: 88063, \n",
      " train loss: 0.29830575817976523\n",
      "iter: 89087, \n",
      " train loss: 0.3190024316435256\n",
      "iter: 90111, \n",
      " train loss: 0.32033662743538116\n",
      "iter: 91135, \n",
      " train loss: 0.30569178626529947\n",
      "iter: 92159, \n",
      " train loss: 0.31139109038764445\n",
      "iter: 93183, \n",
      " train loss: 0.31466965717646644\n",
      "iter: 94207, \n",
      " train loss: 0.3090007631514027\n",
      "iter: 95231, \n",
      " train loss: 0.3273730520699587\n",
      "iter: 96255, \n",
      " train loss: 0.3165556023220688\n",
      "iter: 97279, \n",
      " train loss: 0.32915894861355355\n",
      "iter: 98303, \n",
      " train loss: 0.31443401437695684\n",
      "iter: 99327, \n",
      " train loss: 0.2888145981676473\n",
      "iter: 100351, \n",
      " train loss: 0.31415575362177606\n",
      "iter: 101375, \n",
      " train loss: 0.32107528722519874\n",
      "iter: 102399, \n",
      " train loss: 0.2978408689891694\n",
      "iter: 103423, \n",
      " train loss: 0.2974238171923389\n",
      "iter: 104447, \n",
      " train loss: 0.2989034692924406\n",
      "iter: 105471, \n",
      " train loss: 0.2855462327074747\n",
      "iter: 106495, \n",
      " train loss: 0.30175960927448386\n",
      "iter: 107519, \n",
      " train loss: 0.31530340175123683\n",
      "iter: 108543, \n",
      " train loss: 0.30443252032216606\n",
      "iter: 109567, \n",
      " train loss: 0.31091834741027924\n",
      "iter: 110591, \n",
      " train loss: 0.29163409660492334\n",
      "iter: 111615, \n",
      " train loss: 0.3158359731363021\n",
      "iter: 112639, \n",
      " train loss: 0.29502949715390514\n",
      "iter: 113663, \n",
      " train loss: 0.3055705847144168\n",
      "iter: 114687, \n",
      " train loss: 0.27667699564375425\n",
      "iter: 115711, \n",
      " train loss: 0.310032847877153\n",
      "iter: 116735, \n",
      " train loss: 0.29929135798545303\n",
      "iter: 117759, \n",
      " train loss: 0.31963120355564456\n",
      "iter: 118783, \n",
      " train loss: 0.299145184051099\n",
      "iter: 119807, \n",
      " train loss: 0.2898275500668035\n",
      "iter: 120831, \n",
      " train loss: 0.3009852274390141\n",
      "iter: 121855, \n",
      " train loss: 0.30676578473915583\n",
      "iter: 122879, \n",
      " train loss: 0.2829508472728719\n",
      "iter: 123903, \n",
      " train loss: 0.30432563950395775\n",
      "iter: 124927, \n",
      " train loss: 0.30603549035561173\n",
      "iter: 125951, \n",
      " train loss: 0.2967743259453641\n",
      "iter: 126975, \n",
      " train loss: 0.28232115424754056\n",
      "iter: 127999, \n",
      " train loss: 0.31542596835174663\n",
      "iter: 129023, \n",
      " train loss: 0.2973596868993127\n",
      "iter: 130047, \n",
      " train loss: 0.29662721465143704\n",
      "iter: 131071, \n",
      " train loss: 0.2761506743090649\n",
      "iter: 132095, \n",
      " train loss: 0.2925488746542442\n",
      "iter: 133119, \n",
      " train loss: 0.2802622793063847\n",
      "iter: 134143, \n",
      " train loss: 0.30397501497151325\n",
      "iter: 135167, \n",
      " train loss: 0.27220477967091483\n",
      "iter: 136191, \n",
      " train loss: 0.282844483101087\n",
      "iter: 137215, \n",
      " train loss: 0.2769471492609341\n",
      "iter: 138239, \n",
      " train loss: 0.29006280273222274\n",
      "iter: 139263, \n",
      " train loss: 0.28594929220957965\n",
      "iter: 140287, \n",
      " train loss: 0.2670214922934804\n",
      "iter: 141311, \n",
      " train loss: 0.2627284425931009\n",
      "iter: 142335, \n",
      " train loss: 0.3017909435498609\n",
      "iter: 143359, \n",
      " train loss: 0.280080570615894\n",
      "iter: 144383, \n",
      " train loss: 0.31142149587526546\n",
      "iter: 145407, \n",
      " train loss: 0.2943758359008939\n",
      "iter: 146431, \n",
      " train loss: 0.281525668529639\n",
      "iter: 147455, \n",
      " train loss: 0.28675040558559317\n",
      "iter: 148479, \n",
      " train loss: 0.26790910492883313\n",
      "iter: 149503, \n",
      " train loss: 0.2882938056902731\n",
      "iter: 150527, \n",
      " train loss: 0.28287823108911425\n",
      "iter: 151551, \n",
      " train loss: 0.273049667283658\n",
      "iter: 152575, \n",
      " train loss: 0.3032979068856889\n",
      "iter: 153599, \n",
      " train loss: 0.26697610143793327\n",
      "iter: 154623, \n",
      " train loss: 0.3103255011113788\n",
      "iter: 155647, \n",
      " train loss: 0.2793852206316956\n",
      "iter: 156671, \n",
      " train loss: 0.2792548467036511\n",
      "iter: 157695, \n",
      " train loss: 0.2505981952436116\n",
      "iter: 158719, \n",
      " train loss: 0.2735137408064645\n",
      "iter: 159743, \n",
      " train loss: 0.2951831937931786\n",
      "iter: 160767, \n",
      " train loss: 0.2779391500475299\n",
      "iter: 161791, \n",
      " train loss: 0.2891835617917877\n",
      "iter: 162815, \n",
      " train loss: 0.2686442863047489\n",
      "iter: 163839, \n",
      " train loss: 0.28730005255476954\n",
      "iter: 164863, \n",
      " train loss: 0.27998774410423266\n",
      "iter: 165887, \n",
      " train loss: 0.29549427582560384\n",
      "iter: 166911, \n",
      " train loss: 0.2924626704640332\n",
      "iter: 167935, \n",
      " train loss: 0.2779483231547033\n",
      "iter: 168959, \n",
      " train loss: 0.28058452303241666\n",
      "iter: 169983, \n",
      " train loss: 0.2622734346395532\n",
      "iter: 171007, \n",
      " train loss: 0.28715302218927263\n",
      "iter: 172031, \n",
      " train loss: 0.3178262504842735\n",
      "iter: 173055, \n",
      " train loss: 0.2625825104986461\n",
      "iter: 174079, \n",
      " train loss: 0.29160084937979036\n",
      "iter: 175103, \n",
      " train loss: 0.27702611462711957\n",
      "iter: 176127, \n",
      " train loss: 0.2939631780291734\n",
      "iter: 177151, \n",
      " train loss: 0.27381343287498794\n",
      "iter: 178175, \n",
      " train loss: 0.27273225929604905\n",
      "iter: 179199, \n",
      " train loss: 0.2686439983230571\n",
      "iter: 180223, \n",
      " train loss: 0.2823628097648623\n",
      "iter: 181247, \n",
      " train loss: 0.2738156240375531\n",
      "iter: 182271, \n",
      " train loss: 0.2892713351246812\n",
      "iter: 183295, \n",
      " train loss: 0.2672530003476936\n",
      "iter: 184319, \n",
      " train loss: 0.28983191123114693\n",
      "iter: 185343, \n",
      " train loss: 0.28060130511596526\n",
      "iter: 186367, \n",
      " train loss: 0.2741746559285616\n",
      "iter: 187391, \n",
      " train loss: 0.2893369488073745\n",
      "iter: 188415, \n",
      " train loss: 0.2677906101564531\n",
      "iter: 189439, \n",
      " train loss: 0.28810509460478784\n",
      "iter: 190463, \n",
      " train loss: 0.2743687914111774\n",
      "iter: 191487, \n",
      " train loss: 0.26901696008195586\n",
      "iter: 192511, \n",
      " train loss: 0.2918153502406824\n",
      "iter: 193535, \n",
      " train loss: 0.2587805750301868\n",
      "iter: 194559, \n",
      " train loss: 0.27895792607597514\n",
      "iter: 195583, \n",
      " train loss: 0.27974652886086915\n",
      "iter: 196607, \n",
      " train loss: 0.26683314867764807\n",
      "iter: 197631, \n",
      " train loss: 0.2689796058294007\n",
      "iter: 198655, \n",
      " train loss: 0.28208885605931755\n",
      "iter: 199679, \n",
      " train loss: 0.2835476902343146\n",
      "iter: 200703, \n",
      " train loss: 0.28109808037896755\n",
      "iter: 201727, \n",
      " train loss: 0.27789505849375473\n",
      "iter: 202751, \n",
      " train loss: 0.2589466868783461\n",
      "iter: 203775, \n",
      " train loss: 0.26315121001272246\n",
      "iter: 204799, \n",
      " train loss: 0.2738730805842522\n",
      "iter: 205823, \n",
      " train loss: 0.2723646741694239\n",
      "iter: 206847, \n",
      " train loss: 0.2552801728014913\n",
      "iter: 207871, \n",
      " train loss: 0.2659722709859693\n",
      "iter: 208895, \n",
      " train loss: 0.2741090173747125\n",
      "iter: 209919, \n",
      " train loss: 0.27965752232614705\n",
      "iter: 210943, \n",
      " train loss: 0.28846965296298777\n",
      "iter: 211967, \n",
      " train loss: 0.2662253967944821\n",
      "iter: 212991, \n",
      " train loss: 0.25497709119429146\n",
      "iter: 214015, \n",
      " train loss: 0.26665094994817196\n",
      "iter: 215039, \n",
      " train loss: 0.28052034801561376\n",
      "iter: 216063, \n",
      " train loss: 0.28325000663392075\n",
      "iter: 217087, \n",
      " train loss: 0.2873637525784005\n",
      "iter: 218111, \n",
      " train loss: 0.2859386294763908\n",
      "iter: 219135, \n",
      " train loss: 0.2771623542918462\n",
      "iter: 220159, \n",
      " train loss: 0.26051020018701365\n",
      "iter: 221183, \n",
      " train loss: 0.2594224197491144\n",
      "iter: 222207, \n",
      " train loss: 0.2569765792529779\n",
      "iter: 223231, \n",
      " train loss: 0.29934201931649795\n",
      "iter: 224255, \n",
      " train loss: 0.2813592784818866\n",
      "iter: 225279, \n",
      " train loss: 0.261413815742884\n",
      "iter: 226303, \n",
      " train loss: 0.237595559347767\n",
      "iter: 227327, \n",
      " train loss: 0.255507443732796\n",
      "iter: 228351, \n",
      " train loss: 0.25394379465382144\n",
      "iter: 229375, \n",
      " train loss: 0.2789496546782999\n",
      "iter: 230399, \n",
      " train loss: 0.26728362677275186\n",
      "iter: 231423, \n",
      " train loss: 0.27281424887161165\n",
      "iter: 232447, \n",
      " train loss: 0.2777237580342131\n",
      "iter: 233471, \n",
      " train loss: 0.27960550922895777\n",
      "iter: 234495, \n",
      " train loss: 0.2646103230587755\n",
      "iter: 235519, \n",
      " train loss: 0.26736656952260773\n",
      "iter: 236543, \n",
      " train loss: 0.27535381604565146\n",
      "iter: 237567, \n",
      " train loss: 0.2575793218736919\n",
      "iter: 238591, \n",
      " train loss: 0.2614951969828354\n",
      "iter: 239615, \n",
      " train loss: 0.2820651167265851\n",
      "iter: 240639, \n",
      " train loss: 0.27502207686376323\n",
      "iter: 241663, \n",
      " train loss: 0.2565151380948123\n",
      "iter: 242687, \n",
      " train loss: 0.26262901474743217\n",
      "iter: 243711, \n",
      " train loss: 0.2708260916667484\n",
      "iter: 244735, \n",
      " train loss: 0.2559693560563403\n",
      "iter: 245759, \n",
      " train loss: 0.26073298153545466\n",
      "iter: 246783, \n",
      " train loss: 0.24649464948060995\n",
      "iter: 247807, \n",
      " train loss: 0.2680835317634944\n",
      "iter: 248831, \n",
      " train loss: 0.28097329888302625\n",
      "iter: 249855, \n",
      " train loss: 0.24269009574879874\n",
      "iter: 250879, \n",
      " train loss: 0.25094612246758174\n",
      "iter: 251903, \n",
      " train loss: 0.27048511940338926\n",
      "iter: 252927, \n",
      " train loss: 0.2532411635563676\n",
      "iter: 253951, \n",
      " train loss: 0.2728594960859425\n",
      "iter: 254975, \n",
      " train loss: 0.27796685239610497\n",
      "iter: 255999, \n",
      " train loss: 0.2723385372775624\n",
      "iter: 257023, \n",
      " train loss: 0.25762335633754674\n",
      "iter: 258047, \n",
      " train loss: 0.2749846122576969\n",
      "iter: 259071, \n",
      " train loss: 0.28303558907327897\n",
      "iter: 260095, \n",
      " train loss: 0.2635358020243075\n",
      "iter: 261119, \n",
      " train loss: 0.2321159183952659\n",
      "iter: 262143, \n",
      " train loss: 0.2675070144274798\n",
      "iter: 263167, \n",
      " train loss: 0.24705505754707247\n",
      "iter: 264191, \n",
      " train loss: 0.26727035614291594\n",
      "iter: 265215, \n",
      " train loss: 0.2604288537263528\n",
      "iter: 266239, \n",
      " train loss: 0.2521235575336789\n",
      "iter: 267263, \n",
      " train loss: 0.25947193722666384\n",
      "iter: 268287, \n",
      " train loss: 0.2697761247442827\n",
      "iter: 269311, \n",
      " train loss: 0.26114211516164687\n",
      "iter: 270335, \n",
      " train loss: 0.2594542571702334\n",
      "iter: 271359, \n",
      " train loss: 0.24251568988219674\n",
      "iter: 272383, \n",
      " train loss: 0.2597007115680867\n",
      "iter: 273407, \n",
      " train loss: 0.24544850352646108\n",
      "iter: 274431, \n",
      " train loss: 0.2472101821452668\n",
      "iter: 275455, \n",
      " train loss: 0.22989524864777877\n",
      "iter: 276479, \n",
      " train loss: 0.24702491898850099\n",
      "iter: 277503, \n",
      " train loss: 0.23690476110411396\n",
      "iter: 278527, \n",
      " train loss: 0.25754239142267465\n",
      "iter: 279551, \n",
      " train loss: 0.27713947064376043\n",
      "iter: 280575, \n",
      " train loss: 0.26627035774521346\n",
      "iter: 281599, \n",
      " train loss: 0.2822465417977469\n",
      "iter: 282623, \n",
      " train loss: 0.25478413291233437\n",
      "iter: 283647, \n",
      " train loss: 0.2549316755740847\n",
      "iter: 284671, \n",
      " train loss: 0.2656324309431284\n",
      "iter: 285695, \n",
      " train loss: 0.2535471628793289\n",
      "iter: 286719, \n",
      " train loss: 0.26097959450231656\n",
      "iter: 287743, \n",
      " train loss: 0.2574658838540813\n",
      "iter: 288767, \n",
      " train loss: 0.25600526401325396\n",
      "iter: 289791, \n",
      " train loss: 0.2600821676037981\n",
      "iter: 290815, \n",
      " train loss: 0.259662826819806\n",
      "iter: 291839, \n",
      " train loss: 0.2633461404144555\n",
      "iter: 292863, \n",
      " train loss: 0.2524126286008652\n",
      "iter: 293887, \n",
      " train loss: 0.250626704133424\n",
      "iter: 294911, \n",
      " train loss: 0.24961164146770898\n",
      "iter: 295935, \n",
      " train loss: 0.25766389620596897\n",
      "iter: 296959, \n",
      " train loss: 0.29406400532860744\n",
      "iter: 297983, \n",
      " train loss: 0.2489249877096995\n",
      "iter: 299007, \n",
      " train loss: 0.25138365473645763\n",
      "iter: 300031, \n",
      " train loss: 0.2503818189949243\n",
      "iter: 301055, \n",
      " train loss: 0.2558054768186082\n",
      "iter: 302079, \n",
      " train loss: 0.2627467936056007\n",
      "iter: 303103, \n",
      " train loss: 0.2626043044574544\n",
      "iter: 304127, \n",
      " train loss: 0.2564590899403356\n",
      "iter: 305151, \n",
      " train loss: 0.2589394208587237\n",
      "iter: 306175, \n",
      " train loss: 0.2623403579666501\n",
      "iter: 307199, \n",
      " train loss: 0.2551126130172179\n",
      "iter: 308223, \n",
      " train loss: 0.24269436252890841\n",
      "iter: 309247, \n",
      " train loss: 0.25222285161319746\n",
      "iter: 310271, \n",
      " train loss: 0.24311416503721262\n",
      "iter: 311295, \n",
      " train loss: 0.22218251402000533\n",
      "iter: 312319, \n",
      " train loss: 0.2669873699908294\n",
      "iter: 313343, \n",
      " train loss: 0.2433573840106078\n",
      "iter: 314367, \n",
      " train loss: 0.23374130122161318\n",
      "iter: 315391, \n",
      " train loss: 0.24678038360005417\n",
      "iter: 316415, \n",
      " train loss: 0.2411334644812655\n",
      "iter: 317439, \n",
      " train loss: 0.23220502779946628\n",
      "iter: 318463, \n",
      " train loss: 0.25722486241383535\n",
      "iter: 319487, \n",
      " train loss: 0.2552095586705434\n",
      "iter: 320511, \n",
      " train loss: 0.2375613412281723\n",
      "iter: 321535, \n",
      " train loss: 0.24561685414579415\n",
      "iter: 322559, \n",
      " train loss: 0.25449265123043574\n",
      "iter: 323583, \n",
      " train loss: 0.275215957648129\n",
      "iter: 324607, \n",
      " train loss: 0.248213237730738\n",
      "iter: 325631, \n",
      " train loss: 0.2377253906720398\n",
      "iter: 326655, \n",
      " train loss: 0.24285373225154672\n",
      "iter: 327679, \n",
      " train loss: 0.2785544030630831\n",
      "iter: 328703, \n",
      " train loss: 0.26671979145203295\n",
      "iter: 329727, \n",
      " train loss: 0.2663988662456518\n",
      "iter: 330751, \n",
      " train loss: 0.2385403383887592\n",
      "iter: 331775, \n",
      " train loss: 0.24877825640029982\n",
      "iter: 332799, \n",
      " train loss: 0.24349590847154445\n",
      "iter: 333823, \n",
      " train loss: 0.245168794166716\n",
      "iter: 334847, \n",
      " train loss: 0.22663864855728377\n",
      "iter: 335871, \n",
      " train loss: 0.2580237756915267\n",
      "iter: 336895, \n",
      " train loss: 0.22661948451829517\n",
      "iter: 337919, \n",
      " train loss: 0.24129871144165804\n",
      "iter: 338943, \n",
      " train loss: 0.2472367945906342\n",
      "iter: 339967, \n",
      " train loss: 0.24573607880716963\n",
      "iter: 340991, \n",
      " train loss: 0.25151526574830996\n",
      "iter: 342015, \n",
      " train loss: 0.237093790921449\n",
      "iter: 343039, \n",
      " train loss: 0.2587270323530788\n",
      "iter: 344063, \n",
      " train loss: 0.2470387601851911\n",
      "iter: 345087, \n",
      " train loss: 0.24461289635321748\n",
      "iter: 346111, \n",
      " train loss: 0.2467194994807187\n",
      "iter: 347135, \n",
      " train loss: 0.22404203718659232\n",
      "iter: 348159, \n",
      " train loss: 0.22820169353829556\n",
      "iter: 349183, \n",
      " train loss: 0.2545493789421016\n",
      "iter: 350207, \n",
      " train loss: 0.24896696156866938\n",
      "iter: 351231, \n",
      " train loss: 0.24770445252208617\n",
      "iter: 352255, \n",
      " train loss: 0.24395912989621138\n",
      "iter: 353279, \n",
      " train loss: 0.24348319556085585\n",
      "iter: 354303, \n",
      " train loss: 0.24608334874878324\n",
      "iter: 355327, \n",
      " train loss: 0.24458945858356174\n",
      "iter: 356351, \n",
      " train loss: 0.22340388551376122\n",
      "iter: 357375, \n",
      " train loss: 0.25990352218192925\n",
      "iter: 358399, \n",
      " train loss: 0.26644126122131695\n",
      "iter: 359423, \n",
      " train loss: 0.23817287629566408\n",
      "iter: 360447, \n",
      " train loss: 0.2431163877618019\n",
      "iter: 361471, \n",
      " train loss: 0.24200733089639925\n",
      "iter: 362495, \n",
      " train loss: 0.24555869838320632\n",
      "iter: 363519, \n",
      " train loss: 0.25051917369785315\n",
      "iter: 364543, \n",
      " train loss: 0.23127251566438645\n",
      "iter: 365567, \n",
      " train loss: 0.23084042148946082\n",
      "iter: 366591, \n",
      " train loss: 0.27035673427030815\n",
      "iter: 367615, \n",
      " train loss: 0.2400145764783872\n",
      "iter: 368639, \n",
      " train loss: 0.23871143797728678\n",
      "iter: 369663, \n",
      " train loss: 0.24377495016372563\n",
      "iter: 370687, \n",
      " train loss: 0.24897501901813257\n",
      "iter: 371711, \n",
      " train loss: 0.25640371191457234\n",
      "iter: 372735, \n",
      " train loss: 0.22481292434383704\n",
      "iter: 373759, \n",
      " train loss: 0.2521000419696975\n",
      "iter: 374783, \n",
      " train loss: 0.2538774311787506\n",
      "iter: 375807, \n",
      " train loss: 0.23367822162487073\n",
      "iter: 376831, \n",
      " train loss: 0.2565458043080753\n",
      "iter: 377855, \n",
      " train loss: 0.23090126811587197\n",
      "iter: 378879, \n",
      " train loss: 0.22911802625014843\n",
      "iter: 379903, \n",
      " train loss: 0.24089370678132127\n",
      "iter: 380927, \n",
      " train loss: 0.2504288138633797\n",
      "iter: 381951, \n",
      " train loss: 0.26018783903231224\n",
      "iter: 382975, \n",
      " train loss: 0.2418404300334771\n",
      "iter: 383999, \n",
      " train loss: 0.25363036084229407\n",
      "iter: 385023, \n",
      " train loss: 0.2514359858730728\n",
      "iter: 386047, \n",
      " train loss: 0.24375258450652382\n",
      "iter: 387071, \n",
      " train loss: 0.2574554146279979\n",
      "iter: 388095, \n",
      " train loss: 0.24125618869413756\n",
      "iter: 389119, \n",
      " train loss: 0.2416717766000147\n",
      "iter: 390143, \n",
      " train loss: 0.2524490677670883\n",
      "iter: 391167, \n",
      " train loss: 0.2614653392608943\n",
      "iter: 392191, \n",
      " train loss: 0.23755446962320548\n",
      "iter: 393215, \n",
      " train loss: 0.250252380216466\n",
      "iter: 394239, \n",
      " train loss: 0.2313458519686833\n",
      "iter: 395263, \n",
      " train loss: 0.23251386656170325\n",
      "iter: 396287, \n",
      " train loss: 0.2509765703092057\n",
      "iter: 397311, \n",
      " train loss: 0.23681793822055397\n",
      "iter: 398335, \n",
      " train loss: 0.23010433382174256\n",
      "iter: 399359, \n",
      " train loss: 0.2318730806620124\n",
      "iter: 400383, \n",
      " train loss: 0.2522172393919391\n",
      "iter: 401407, \n",
      " train loss: 0.24375069720179\n",
      "iter: 402431, \n",
      " train loss: 0.23900536739488132\n",
      "iter: 403455, \n",
      " train loss: 0.2598581056952298\n",
      "iter: 404479, \n",
      " train loss: 0.2527091132281498\n",
      "iter: 405503, \n",
      " train loss: 0.23887468364404185\n",
      "iter: 406527, \n",
      " train loss: 0.23341063027771725\n",
      "iter: 407551, \n",
      " train loss: 0.24234586534001323\n",
      "iter: 408575, \n",
      " train loss: 0.25280798425487205\n",
      "iter: 409599, \n",
      " train loss: 0.2082150807747496\n",
      "iter: 410623, \n",
      " train loss: 0.23739802743399707\n",
      "iter: 411647, \n",
      " train loss: 0.24280872120414188\n",
      "iter: 412671, \n",
      " train loss: 0.22647003245560882\n",
      "iter: 413695, \n",
      " train loss: 0.23944371702140188\n",
      "iter: 414719, \n",
      " train loss: 0.23257293609114527\n",
      "iter: 415743, \n",
      " train loss: 0.22109760807458656\n",
      "iter: 416767, \n",
      " train loss: 0.24310350939686032\n",
      "iter: 417791, \n",
      " train loss: 0.2377920200618462\n",
      "iter: 418815, \n",
      " train loss: 0.2740475137593137\n",
      "iter: 419839, \n",
      " train loss: 0.25756023136472095\n",
      "iter: 420863, \n",
      " train loss: 0.24556520309837992\n",
      "iter: 421887, \n",
      " train loss: 0.2221383245037032\n",
      "iter: 422911, \n",
      " train loss: 0.2322938954908409\n",
      "iter: 423935, \n",
      " train loss: 0.2406262719083827\n",
      "iter: 424959, \n",
      " train loss: 0.2440950101336341\n",
      "iter: 425983, \n",
      " train loss: 0.22614106380513022\n",
      "iter: 427007, \n",
      " train loss: 0.23221881338196226\n",
      "iter: 428031, \n",
      " train loss: 0.25183157001873724\n",
      "iter: 429055, \n",
      " train loss: 0.23445997752031644\n",
      "iter: 430079, \n",
      " train loss: 0.21834139565774535\n",
      "iter: 431103, \n",
      " train loss: 0.23711332331777157\n",
      "iter: 432127, \n",
      " train loss: 0.21442730259386167\n",
      "iter: 433151, \n",
      " train loss: 0.22556263206136862\n",
      "iter: 434175, \n",
      " train loss: 0.24184369722010501\n",
      "iter: 435199, \n",
      " train loss: 0.2572000761044535\n",
      "iter: 436223, \n",
      " train loss: 0.23836177348630372\n",
      "iter: 437247, \n",
      " train loss: 0.21820693688624715\n",
      "iter: 438271, \n",
      " train loss: 0.2116541589911094\n",
      "iter: 439295, \n",
      " train loss: 0.24043489119407013\n",
      "iter: 440319, \n",
      " train loss: 0.23422379178492747\n",
      "iter: 441343, \n",
      " train loss: 0.23211192977601058\n",
      "iter: 442367, \n",
      " train loss: 0.2547860570562648\n",
      "iter: 443391, \n",
      " train loss: 0.2326557331434458\n",
      "iter: 444415, \n",
      " train loss: 0.25841057514827526\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss_topic = torch.nn.CrossEntropyLoss()\n",
    "asym =lambda x: x if x<0 else 2*x\n",
    "# sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "softplue = lambda x:np.log(1 + np.exp(x))\n",
    "\n",
    "train_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for i,(text,index,target,source) in enumerate(from_shepherd(dataset)):\n",
    "        target_topics = source * torch.ones(target.shape[1],dtype=torch.long,device='cuda') # l\n",
    "        hidden_states = base_model(text)[0][:,index].float() # b,l,d\n",
    "        logits = model.score(hidden_states)[:,:,0] # b,l\n",
    "        loss = loss_fn(logits,target)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # clip_grad_value_(trainable_params,clip)\n",
    "            clip_grad_value_(base_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print(f\"iter: {i}, \\n train loss: {train_loss/verbose}\")\n",
    "            train_loss = 0\n",
    "            \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/envs/vllm/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../Model/PRM - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.score.state_dict(), '../Model/model_score.pth')\n",
    "peft_model_id = \"../Model/PRM_LORA\"\n",
    "base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
