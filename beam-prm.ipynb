{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8077274,"sourceType":"datasetVersion","datasetId":4746046},{"sourceId":8099570,"sourceType":"datasetVersion","datasetId":4782935},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900},{"sourceId":5994,"sourceType":"modelInstanceVersion","modelInstanceId":4761},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318},{"sourceId":11394,"sourceType":"modelInstanceVersion","modelInstanceId":8332}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# credits:\n# https://www.kaggle.com/code/bsmit1659/aimo-vllm-accelerated-tot-sc-deepseekmath","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:42:44.072564Z","iopub.execute_input":"2024-04-07T10:42:44.073313Z","iopub.status.idle":"2024-04-07T10:42:44.07799Z","shell.execute_reply.started":"2024-04-07T10:42:44.073282Z","shell.execute_reply":"2024-04-07T10:42:44.077076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall -y torch -q\n!pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n\n# keep data in float16 to avoid OOM\nfile_path = '/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py'\nwith open(file_path, 'r') as file:\n    file_contents = file.readlines()\nfile_contents = [line for line in file_contents if \"logits = logits.float()\" not in line]\nwith open(file_path, 'w') as file:\n    file.writelines(file_contents)","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-27T20:22:11.684928Z","iopub.execute_input":"2024-04-27T20:22:11.685212Z","iopub.status.idle":"2024-04-27T20:24:31.011005Z","shell.execute_reply.started":"2024-04-27T20:22:11.685186Z","shell.execute_reply":"2024-04-27T20:24:31.009944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc\nimport re\nimport sys\nimport subprocess\nfrom collections import defaultdict, Counter\nimport numpy as np\nfrom transformers import (AutoModelForCausalLM,\n    AutoTokenizer,\n    set_seed)\nimport torch\nimport math\n\nllm = LLM(model=\"/kaggle/input/deepseek-math\",\n          dtype='half',\n          enforce_eager=True,\n          gpu_memory_utilization=0.99,\n          swap_space=4,\n          max_model_len=2048,\n          kv_cache_dtype=\"fp8_e5m2\",\n          tensor_parallel_size=1)\n\ntokenizer = llm.get_tokenizer()\n\ngood_token = '+'\nbad_token = '-'\nstep_tag = 'ки'\n\nprm_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm')\nprm_candidate_tokens = prm_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\nstep_tag_id = prm_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\nprm_model = AutoModelForCausalLM.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm',\n                                                 torch_dtype=torch.float16,\n                                                 device_map=\"balanced_low_0\").eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:24:31.012938Z","iopub.execute_input":"2024-04-27T20:24:31.013250Z","iopub.status.idle":"2024-04-27T20:28:27.169734Z","shell.execute_reply.started":"2024-04-27T20:24:31.013220Z","shell.execute_reply":"2024-04-27T20:28:27.168940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import aimo\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_prm(candidates):\n    # Initialize a list to store all the log probabilities\n    all_log_probs = []\n    # Process the candidates in batches\n    for i in range(len(candidates)):\n        # Select a batch of candidates\n        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda:1\")  # Concatenate the padded inputs into a tensor\n\n        with torch.no_grad():\n            logits = prm_model(input_ids).logits[:, :, prm_candidate_tokens] # b,l,C\n            scores = logits.softmax(dim=-1)[:, :, 0].squeeze() # l\n            # Collect the log probabilities from this batch\n            all_log_probs.append(scores[-1].item())\n    return all_log_probs","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:28:27.207787Z","iopub.execute_input":"2024-04-27T20:28:27.208087Z","iopub.status.idle":"2024-04-27T20:28:27.214681Z","shell.execute_reply.started":"2024-04-27T20:28:27.208062Z","shell.execute_reply":"2024-04-27T20:28:27.213764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\nstop_words.append(\"\\n\")\n\nsampling_params = SamplingParams(temperature=1,\n                                 max_tokens=256,\n                                 min_tokens=32,\n                                 stop=stop_words)\n\ncot_instruction = \"\\nYou are an expert at mathematical reasoning. Please reason step by step, and put your final answer within \\\\boxed{}. The answer should be an interger between 0 and 999.\"\n\n\nn = 5 # beams\nsamples = 7\nmax_depth = 24\noverlap_threshold = 0.6\nall_prompts = []\ntotal_paths = []\ntotal_answers = []\n\ndef is_integer(num):\n    if isinstance(num, float):\n        return num.is_integer()\n    elif isinstance(num, int):\n        return True\n    else:\n        return False\n    \ndef is_between_0_and_999(num):\n    return 0 <= num <= 999\n\ndef prm_prompt(text, current_level):\n    return f\"Step {str(current_level)}:\" + text + ' ки'\n\ndef remove_prm_prompt(text):\n    pattern = r\"Step \\d+:\"\n    text = re.sub(pattern, \"\", text)\n    return text.replace(\" ки\",\"\")\n\nimport re\ndef extract_number(text):\n    patterns = [\n        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n        r\"The answer is[:\\s]*([0-9]+)\"\n    ]\n    for pattern in patterns:\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n    return 'parse err'\n\ndef tot_agg(completed_paths):\n    # [(answer,score,len_,current_level),...]\n    if completed_paths:\n        return max(completed_paths,key=lambda x:x[1]+x[3]**2*0.00108)[0]\n    else:\n        return 37 # empty completed_paths\n\ndef get_overlap(nodes_split,node_list):\n    max_overlap = float(\"-inf\")\n    node_len = len(node_list)\n    for previous_split in nodes_split:\n        count = 0\n        len_ = max(len(previous_split),node_len)\n        for i,j in zip(previous_split,node_list):\n            count += (i==j)\n        count /= len_\n        max_overlap = max(max_overlap,count)\n    return max_overlap\n\nfor test, sample_submission in iter_test:\n    problem = test['problem']\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": problem + cot_instruction\n        }\n    ]\n\n    base_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False\n    )\n    current_level = 1\n\n    current_level_nodes = [base_prompt]\n    current_scores = [float('inf')] # for min score\n    completed_paths = []\n    completed_path_splits = []\n    try:\n        while (len(completed_paths) < n) and (current_level < max_depth) and (current_level_nodes):\n            # for generation, remove special tokens for PRM\n            batch_responses = llm.generate([remove_prm_prompt(t) for t in current_level_nodes]*samples, sampling_params)\n            prm_inputs = []\n            cumulative_lens = []\n\n            # Collect candidates for reward model evaluation\n            for candidate,parent in zip(batch_responses,current_level_nodes*samples):\n                prm_input = parent + prm_prompt(candidate.outputs[0].text,current_level)\n                cumulative_tokens = len(candidate.prompt_token_ids) + len(candidate.outputs[0].token_ids)\n                prm_inputs.append(prm_input)\n                cumulative_lens.append(cumulative_tokens)\n            # Get the indices of unique elements in prm_inputs\n            unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n            prm_inputs = [prm_inputs[i] for i in unique_indices]\n            current_scores = [(current_scores*samples)[i] for i in unique_indices]\n            cumulative_lens = [cumulative_lens[i] for i in unique_indices]\n\n            # Batch reward model evaluation\n            prm_scores = eval_prm(prm_inputs)\n            prm_scores = [min(old,new) for old,new in zip(current_scores,prm_scores)]\n            next_level_nodes = []\n            next_scores = []\n            nodes_split = []\n\n            # Prune to keep only the top 'n' candidates based on scores\n            combined = list(zip(prm_inputs,prm_scores,cumulative_lens))\n            combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n            for node,score,len_ in combined:\n                answer = extract_number(remove_prm_prompt(node))\n                if answer == 'parse err': # not finished\n                    if len_ > 2048: continue # max out len_\n                    node_list = node.split(\" ки\")\n                    print(get_overlap(nodes_split,node_list))\n                    if get_overlap(nodes_split,node_list) < overlap_threshold:\n                        next_level_nodes.append(node)\n                        next_scores.append(score)\n                        nodes_split.append(node_list)\n                    else: continue\n                    if len(next_level_nodes) == n: break\n                else: # finished\n                    node_list = node.split(\" ки\")\n                    if get_overlap(completed_path_splits,node_list) < overlap_threshold:\n                        try:\n                            answer = eval(answer)\n                            if is_integer(answer) and is_between_0_and_999(answer):# correct format\n                                completed_paths.append((answer,score,len_,current_level))\n                                completed_path_splits.append(node_list)\n                        except: # bad eval\n                            continue\n            # if current_level_nodes is empty, all max out or err out. exit loop\n            current_scores, current_level_nodes = next_scores, next_level_nodes\n            current_level += 1\n\n    #     print(f'problem {i}, sol {completed_paths}')\n    #     total_paths.append(completed_paths)\n\n        sample_submission['answer'] = tot_agg(completed_paths)\n    except:\n        sample_submission['answer'] = 37\n    env.predict(sample_submission)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-04-27T20:28:27.216205Z","iopub.execute_input":"2024-04-27T20:28:27.216905Z","iopub.status.idle":"2024-04-27T22:25:50.496400Z","shell.execute_reply.started":"2024-04-27T20:28:27.216875Z","shell.execute_reply":"2024-04-27T22:25:50.495424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_paths\n# len(set(current_level_nodes)),len(current_level_nodes),len(set(prm_inputs)),len(prm_inputs)","metadata":{},"execution_count":null,"outputs":[]}]}