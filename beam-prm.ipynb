{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T10:42:44.073313Z","iopub.status.busy":"2024-04-07T10:42:44.072564Z","iopub.status.idle":"2024-04-07T10:42:44.07799Z","shell.execute_reply":"2024-04-07T10:42:44.077076Z","shell.execute_reply.started":"2024-04-07T10:42:44.073282Z"},"trusted":true},"outputs":[],"source":["# credits:\n","# https://www.kaggle.com/code/bsmit1659/aimo-vllm-accelerated-tot-sc-deepseekmath"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T17:14:06.685114Z","iopub.status.busy":"2024-04-27T17:14:06.684820Z","iopub.status.idle":"2024-04-27T17:16:32.425167Z","shell.execute_reply":"2024-04-27T17:16:32.423975Z","shell.execute_reply.started":"2024-04-27T17:14:06.685089Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","dask-cuda 23.8.0 requires pynvml<11.5,>=11.0.0, but you have pynvml 11.5.0 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip uninstall -y torch -q\n","!pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T17:16:32.430992Z","iopub.status.busy":"2024-04-27T17:16:32.430694Z","iopub.status.idle":"2024-04-27T17:20:40.169214Z","shell.execute_reply":"2024-04-27T17:20:40.168154Z","shell.execute_reply.started":"2024-04-27T17:16:32.430961Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-27 17:16:38,060\tINFO util.py:124 -- Outdated packages:\n","  ipywidgets==7.7.1 found, needs ipywidgets>=8\n","Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING 04-27 17:16:39 config.py:767] Casting torch.bfloat16 to torch.float16.\n","INFO 04-27 17:16:39 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n","INFO 04-27 17:16:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/kaggle/input/deepseek-math', tokenizer='/kaggle/input/deepseek-math', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 04-27 17:16:41 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n","INFO 04-27 17:16:41 selector.py:25] Using XFormers backend.\n","INFO 04-27 17:18:14 model_runner.py:104] Loading model weights took 12.8725 GB\n","INFO 04-27 17:18:16 gpu_executor.py:94] # GPU blocks: 177, # CPU blocks: 1092\n"]},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c6c15c58f604067a708d26fd5caf2f8","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["from vllm import LLM, SamplingParams\n","import pandas as pd\n","from tqdm import tqdm\n","import gc\n","import re\n","import sys\n","import subprocess\n","from collections import defaultdict, Counter\n","import numpy as np\n","from transformers import (AutoModelForCausalLM,\n","    AutoTokenizer,\n","    set_seed)\n","import torch\n","import math\n","\n","llm = LLM(model=\"/kaggle/input/deepseek-math\",\n","          dtype='half',\n","          enforce_eager=True,\n","          gpu_memory_utilization=0.99,\n","          swap_space=4,\n","          max_model_len=2048,\n","          kv_cache_dtype=\"fp8_e5m2\",\n","          tensor_parallel_size=1)\n","\n","tokenizer = llm.get_tokenizer()\n","\n","good_token = '+'\n","bad_token = '-'\n","step_tag = 'ки'\n","\n","prm_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm')\n","prm_candidate_tokens = prm_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n","step_tag_id = prm_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n","prm_model = AutoModelForCausalLM.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm',\n","                                                 torch_dtype=torch.float16,\n","                                                 device_map=\"balanced_low_0\").eval()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T17:20:40.172869Z","iopub.status.busy":"2024-04-27T17:20:40.171836Z","iopub.status.idle":"2024-04-27T17:20:40.209636Z","shell.execute_reply":"2024-04-27T17:20:40.208751Z","shell.execute_reply.started":"2024-04-27T17:20:40.172831Z"},"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>problem</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>229ee8</td>\n","      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n","      <td>52</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>246d26</td>\n","      <td>Each of the three-digits numbers $111$ to $999...</td>\n","      <td>250</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2fc4ad</td>\n","      <td>Let the `sparkle' operation on positive intege...</td>\n","      <td>702</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>430b63</td>\n","      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5277ed</td>\n","      <td>There exists a unique increasing geometric seq...</td>\n","      <td>211</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                            problem  answer\n","0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n","1  246d26  Each of the three-digits numbers $111$ to $999...     250\n","2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n","3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n","4  5277ed  There exists a unique increasing geometric seq...     211"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from tqdm import tqdm\n","PRIVATE = True\n","\n","df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T17:20:40.211059Z","iopub.status.busy":"2024-04-27T17:20:40.210791Z","iopub.status.idle":"2024-04-27T17:20:40.217471Z","shell.execute_reply":"2024-04-27T17:20:40.216573Z","shell.execute_reply.started":"2024-04-27T17:20:40.211036Z"},"trusted":true},"outputs":[],"source":["def eval_prm(candidates):\n","    # Initialize a list to store all the log probabilities\n","    all_log_probs = []\n","    # Process the candidates in batches\n","    for i in range(len(candidates)):\n","        # Select a batch of candidates\n","        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda:1\")  # Concatenate the padded inputs into a tensor\n","\n","        with torch.no_grad():\n","            logits = prm_model(input_ids).logits[:, :, prm_candidate_tokens] # b,l,C\n","            scores = logits.softmax(dim=-1)[:, :, 0].squeeze() # l\n","            # Collect the log probabilities from this batch\n","            all_log_probs.append(scores[-1].item())\n","    return all_log_probs"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-27T17:20:40.218957Z","iopub.status.busy":"2024-04-27T17:20:40.218690Z"},"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n","stop_words.append(\"\\n\")\n","\n","sampling_params = SamplingParams(temperature=1,\n","                                 max_tokens=256,\n","                                 min_tokens=32,\n","                                 stop=stop_words)\n","\n","cot_instruction = \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"\n","\n","\n","n = 5 # beams\n","samples = 7\n","max_depth = 24\n","overlap_threshold = 0.6\n","all_prompts = []\n","total_paths = []\n","total_answers = []\n","\n","def is_integer(num):\n","    if isinstance(num, float):\n","        return num.is_integer()\n","    elif isinstance(num, int):\n","        return True\n","    else:\n","        return False\n","    \n","def is_between_0_and_999(num):\n","    return 0 <= num <= 999\n","\n","def prm_prompt(text, current_level):\n","    return f\"Step {str(current_level)}:\" + text + ' ки'\n","\n","def remove_prm_prompt(text):\n","    pattern = r\"Step \\d+:\"\n","    text = re.sub(pattern, \"\", text)\n","    return text.replace(\" ки\",\"\")\n","\n","import re\n","def extract_number(text):\n","    patterns = [\n","        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n","        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n","        r\"The answer is[:\\s]*([0-9]+)\"\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(1)\n","    return 'parse err'\n","\n","def tot_agg(completed_paths):\n","    # [(answer,score,len_,current_level),...]\n","    pass\n","\n","def get_overlap(nodes_split,node_list):\n","    max_overlap = float(\"-inf\")\n","    node_len = len(node_list)\n","    for previous_split in nodes_split:\n","        count = 0\n","        len_ = max(len(previous_split),node_len)\n","        for i,j in zip(previous_split,node_list):\n","            count += (i==j)\n","        count /= len_\n","        max_overlap = max(max_overlap,count)\n","    return max_overlap\n","\n","for i in tqdm(range(len(df))):\n","    id_ = df['id'].loc[i]\n","    problem = df['problem'].loc[i]\n","\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": problem + cot_instruction\n","        }\n","    ]\n","\n","    base_prompt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False\n","    )\n","    current_level = 1\n","\n","    current_level_nodes = [base_prompt]\n","    current_scores = [float('inf')] # for min score\n","    completed_paths = []\n","    completed_path_splits = []\n","    \n","    while (len(completed_paths) < n) and (current_level < max_depth) and (current_level_nodes):\n","        # for generation, remove special tokens for PRM\n","        batch_responses = llm.generate([remove_prm_prompt(t) for t in current_level_nodes]*samples, sampling_params)\n","        prm_inputs = []\n","        cumulative_lens = []\n","\n","        # Collect candidates for reward model evaluation\n","        for candidate,parent in zip(batch_responses,current_level_nodes*samples):\n","            prm_input = parent + prm_prompt(candidate.outputs[0].text,current_level)\n","            cumulative_tokens = len(candidate.prompt_token_ids) + len(candidate.outputs[0].token_ids)\n","            prm_inputs.append(prm_input)\n","            cumulative_lens.append(cumulative_tokens)\n","        # Get the indices of unique elements in prm_inputs\n","        unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n","        prm_inputs = [prm_inputs[i] for i in unique_indices]\n","        current_scores = [(current_scores*samples)[i] for i in unique_indices]\n","        cumulative_lens = [cumulative_lens[i] for i in unique_indices]\n","        \n","        # Batch reward model evaluation\n","        prm_scores = eval_prm(prm_inputs)\n","        prm_scores = [min(old,new) for old,new in zip(current_scores,prm_scores)]\n","        next_level_nodes = []\n","        next_scores = []\n","        nodes_split = []\n","        \n","        # Prune to keep only the top 'n' candidates based on scores\n","        combined = list(zip(prm_inputs,prm_scores,cumulative_lens))\n","        combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n","        for node,score,len_ in combined:\n","            answer = extract_number(remove_prm_prompt(node))\n","            if answer == 'parse err': # not finished\n","                if len_ > 2048: continue # max out len_\n","                node_list = node.split(\" ки\")\n","                if get_overlap(nodes_split,node_list) < overlap_threshold:\n","                    next_level_nodes.append(node)\n","                    next_scores.append(score)\n","                    nodes_split.append(node_list)\n","                else: continue\n","                if len(next_level_nodes) == n: break\n","            else: # finished\n","                node_list = node.split(\" ки\")\n","                if get_overlap(completed_path_splits,node_list) < overlap_threshold:\n","                    try:\n","                        answer = eval(answer)\n","                        if is_integer(answer) and is_between_0_and_999(answer):# correct format\n","                            completed_paths.append((answer,score,len_,current_level))\n","                            completed_path_splits.append(node_list)\n","                    except: # bad eval\n","                        continue\n","        # if current_level_nodes is empty, all max out or err out. exit loop\n","        current_scores, current_level_nodes = next_scores, next_level_nodes\n","        current_level += 1\n","    \n","    print(f'problem {i}, sol {completed_paths}')\n","    total_paths.append(completed_paths)\n","#     total_answers.append(final_answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T15:46:04.868210Z","iopub.status.busy":"2024-04-27T15:46:04.867503Z","iopub.status.idle":"2024-04-27T15:46:04.874731Z","shell.execute_reply":"2024-04-27T15:46:04.873921Z","shell.execute_reply.started":"2024-04-27T15:46:04.868177Z"},"trusted":true},"outputs":[],"source":["completed_paths"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:52:39.021516Z","iopub.status.busy":"2024-04-27T14:52:39.020610Z","iopub.status.idle":"2024-04-27T14:52:39.028321Z","shell.execute_reply":"2024-04-27T14:52:39.027156Z","shell.execute_reply.started":"2024-04-27T14:52:39.021461Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5, 5, 32, 32)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(set(current_level_nodes)),len(current_level_nodes),len(set(prm_inputs)),len(prm_inputs)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["total_paths = [[(28, 0.71240234375, 724, 8),\n","                (52, 0.658203125, 750, 11),\n","                (41, 0.6259765625, 822, 11),\n","                (28, 0.418701171875, 866, 11),\n","                (36, 0.385009765625, 1301, 17)],\n","                [(480, 0.611328125, 497, 4),\n","                (9, 0.59619140625, 447, 4),\n","                (6, 0.67919921875, 415, 5),\n","                (898, 0.20947265625, 687, 5),\n","                (6, 0.67919921875, 451, 6)],\n","                [(17, 0.615234375, 659, 8),\n","                (9, 0.515625, 816, 8),\n","                (5, 0.64404296875, 655, 9),\n","                (2, 0.56982421875, 583, 9),\n","                (146, 0.48046875, 746, 9)],\n","                [(800, 0.16455078125, 1262, 18)],\n","                [(310, 0.82421875, 294, 1),\n","                (310, 0.8154296875, 286, 3),\n","                (310, 0.8154296875, 293, 4),\n","                (310, 0.763671875, 473, 5),\n","                (310, 0.763671875, 617, 6),\n","                (310, 0.57763671875, 363, 6)],\n","                [(199, 0.56591796875, 386, 6),\n","                (199, 0.48046875, 372, 8),\n","                (101, 0.51171875, 490, 9),\n","                (197, 0.449462890625, 753, 9),\n","                (199, 0.42626953125, 453, 9)],\n","                [(194, 0.6689453125, 326, 3),\n","                (97, 0.49609375, 386, 3),\n","                (97, 0.84375, 459, 5)],\n","                [(100, 0.7216796875, 598, 7),\n","                (256, 0.69921875, 595, 8),\n","                (256, 0.6298828125, 660, 8),\n","                (256, 0.615234375, 579, 8),\n","                (100, 0.418701171875, 492, 9)],\n","                [(29, 0.52734375, 493, 5),\n","                (29, 0.484375, 478, 5),\n","                (29, 0.52734375, 491, 6),\n","                (29, 0.484375, 521, 6),\n","                (20, 0.106689453125, 838, 11)],\n","                [(99, 0.56982421875, 285, 4),\n","                (99, 0.51953125, 435, 8),\n","                (99, 0.48828125, 690, 12),\n","                (72, 0.352294921875, 1917, 16),\n","                (100, 0.414794921875, 1061, 18),\n","                (127, 0.3701171875, 973, 18)]]\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import pandas as pd\n","data = pd.read_csv('../Data/ai-mathematical-olympiad-prize/train.csv')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["ys = data.answer.tolist()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["[52, 250, 702, 800, 211, 199, 185, 320, 480, 199]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["ys"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1 2\n","2 2\n","3 2\n","4 2\n","5 2\n","6 2\n","7 2\n","8 2\n","9 2\n","10 2\n","11 2\n","12 2\n","13 2\n","14 2\n","15 2\n","16 2\n","17 2\n","18 2\n","19 2\n","20 2\n","21 2\n","22 2\n","23 2\n","24 2\n","25 2\n","26 2\n","27 2\n","28 2\n","29 2\n","30 2\n","31 2\n","32 2\n","33 2\n","34 2\n","35 2\n","36 2\n","37 2\n","38 2\n","39 2\n","40 2\n","41 2\n","42 2\n","43 2\n","44 2\n","45 2\n","46 2\n","47 2\n","48 2\n","49 2\n","50 2\n","51 2\n","52 2\n","53 2\n","54 2\n","55 2\n","56 2\n","57 2\n","58 2\n","59 2\n","60 2\n","61 2\n","62 2\n","63 2\n","64 2\n","65 2\n","66 2\n","67 2\n","68 2\n","69 2\n","70 2\n","71 2\n","72 2\n","73 2\n","74 2\n","75 2\n","76 2\n","77 2\n","78 2\n","79 2\n","80 2\n","81 2\n","82 2\n","83 2\n","84 2\n","85 2\n","86 2\n","87 2\n","88 2\n","89 2\n","90 2\n","91 2\n","92 2\n","93 2\n","94 2\n","95 2\n","96 3\n","97 3\n","98 3\n","99 3\n","100 3\n","101 3\n","102 3\n","103 3\n","104 3\n","105 3\n","106 3\n","107 3\n","108 3\n","109 3\n","110 3\n","111 3\n","112 3\n","113 3\n","114 3\n","115 3\n","116 3\n","117 3\n","118 3\n","119 3\n","120 3\n","121 2\n","122 2\n","123 2\n","124 2\n","125 2\n","126 2\n","127 2\n","128 2\n","129 2\n","130 2\n","131 2\n","132 2\n","133 2\n","134 2\n","135 2\n","136 2\n","137 2\n","138 2\n","139 2\n","140 2\n","141 2\n","142 2\n","143 2\n","144 2\n","145 2\n","146 2\n","147 2\n","148 2\n","149 2\n","150 2\n","151 2\n","152 2\n","153 2\n","154 2\n","155 2\n","156 2\n","157 2\n","158 2\n","159 2\n","160 2\n","161 2\n","162 2\n","163 1\n","164 1\n","165 1\n","166 1\n","167 1\n","168 1\n","169 1\n","170 1\n","171 1\n","172 1\n","173 1\n","174 1\n","175 1\n","176 1\n","177 1\n","178 1\n","179 1\n","180 1\n","181 1\n","182 1\n","183 1\n","184 1\n","185 1\n","186 1\n","187 1\n","188 1\n","189 1\n","190 1\n","191 1\n","192 1\n","193 1\n","194 1\n","195 1\n","196 1\n","197 1\n","198 1\n","199 1\n"]}],"source":["for i in range(1,200):\n","    correct = 0\n","    for path,y in zip(total_paths,ys):\n","        yhat = max(path,key=lambda x:x[1]+x[3]**2*0.00001*i)[0]\n","        correct += y == yhat\n","    print(i,correct)\n","# max(path,key=lambda x:x[1]+x[3]**2*0.00001*107)[0]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["correct = 0\n","for path,y in zip(total_paths,ys):\n","    # yhat = max(path,key=lambda x:x[1]+x[3]**2*0.0001*i)[0]\n","    # correct += y == yhat\n","    correct += y in [x[0] for x in path]"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["3"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["correct"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T14:13:04.465849Z","iopub.status.busy":"2024-04-12T14:13:04.465061Z","iopub.status.idle":"2024-04-12T14:13:04.472344Z","shell.execute_reply":"2024-04-12T14:13:04.471355Z","shell.execute_reply.started":"2024-04-12T14:13:04.465816Z"},"trusted":true},"outputs":[],"source":["df['answer'] = final_answers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T14:13:06.179528Z","iopub.status.busy":"2024-04-12T14:13:06.1789Z","iopub.status.idle":"2024-04-12T14:13:06.189287Z","shell.execute_reply":"2024-04-12T14:13:06.18826Z","shell.execute_reply.started":"2024-04-12T14:13:06.179499Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T14:13:09.810373Z","iopub.status.busy":"2024-04-12T14:13:09.810019Z","iopub.status.idle":"2024-04-12T14:13:09.83421Z","shell.execute_reply":"2024-04-12T14:13:09.83324Z","shell.execute_reply.started":"2024-04-12T14:13:09.810346Z"},"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["df[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T14:13:12.238056Z","iopub.status.busy":"2024-04-12T14:13:12.237421Z","iopub.status.idle":"2024-04-12T14:13:12.247868Z","shell.execute_reply":"2024-04-12T14:13:12.246991Z","shell.execute_reply.started":"2024-04-12T14:13:12.238024Z"},"papermill":{"duration":0.014339,"end_time":"2024-02-29T09:37:05.594605","exception":false,"start_time":"2024-02-29T09:37:05.580266","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["df[['id','answer']].head()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4746046,"sourceId":8077274,"sourceType":"datasetVersion"},{"datasetId":4782935,"sourceId":8099570,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
