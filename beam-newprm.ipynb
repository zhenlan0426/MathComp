{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a69c63c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:00:27.553180Z",
     "iopub.status.busy": "2024-05-18T17:00:27.552899Z",
     "iopub.status.idle": "2024-05-18T17:00:27.556999Z",
     "shell.execute_reply": "2024-05-18T17:00:27.556287Z"
    },
    "papermill": {
     "duration": 0.012696,
     "end_time": "2024-05-18T17:00:27.558984",
     "exception": false,
     "start_time": "2024-05-18T17:00:27.546288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# credits:\n",
    "# https://www.kaggle.com/code/bsmit1659/aimo-vllm-accelerated-tot-sc-deepseekmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ef62b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:00:27.570960Z",
     "iopub.status.busy": "2024-05-18T17:00:27.570366Z",
     "iopub.status.idle": "2024-05-18T17:03:03.734627Z",
     "shell.execute_reply": "2024-05-18T17:03:03.733614Z"
    },
    "papermill": {
     "duration": 156.172397,
     "end_time": "2024-05-18T17:03:03.736878",
     "exception": false,
     "start_time": "2024-05-18T17:00:27.564481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pynvml<11.5,>=11.0.0, but you have pynvml 11.5.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch -q\n",
    "!pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm -q\n",
    "# keep data in float16 to avoid OOM\n",
    "file_path = '/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py'\n",
    "with open(file_path, 'r') as file:\n",
    "    file_contents = file.readlines()\n",
    "file_contents = [line for line in file_contents if \"logits = logits.float()\" not in line]\n",
    "with open(file_path, 'w') as file:\n",
    "    file.writelines(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5660fa9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:03:03.749818Z",
     "iopub.status.busy": "2024-05-18T17:03:03.749074Z",
     "iopub.status.idle": "2024-05-18T17:08:21.725719Z",
     "shell.execute_reply": "2024-05-18T17:08:21.724832Z"
    },
    "papermill": {
     "duration": 317.985165,
     "end_time": "2024-05-18T17:08:21.727692",
     "exception": false,
     "start_time": "2024-05-18T17:03:03.742527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 17:03:11,246\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-18 17:03:12 config.py:767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 05-18 17:03:12 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n",
      "INFO 05-18 17:03:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/kaggle/input/deepseek-math', tokenizer='/kaggle/input/deepseek-math', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 17:03:13 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 05-18 17:03:13 selector.py:25] Using XFormers backend.\n",
      "INFO 05-18 17:05:03 model_runner.py:104] Loading model weights took 12.8725 GB\n",
      "INFO 05-18 17:05:04 gpu_executor.py:94] # GPU blocks: 177, # CPU blocks: 1092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4167f1d4ccc44aee846ccde523d2cc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/prm-shep and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from transformers import LlamaForSequenceClassification\n",
    "import torch\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "llm = LLM(model=\"/kaggle/input/deepseek-math\",\n",
    "          dtype='half',\n",
    "          enforce_eager=True,\n",
    "          gpu_memory_utilization=0.99,\n",
    "          swap_space=4,\n",
    "          max_model_len=2048,\n",
    "          kv_cache_dtype=\"fp8_e5m2\",\n",
    "          tensor_parallel_size=1)\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "prm_tokenizer = tokenizer\n",
    "prm_model = LlamaForSequenceClassification.from_pretrained('/kaggle/input/prm-shep',\\\n",
    "                                                    num_labels=1,\\\n",
    "                                                    device_map=\"cuda:1\",\n",
    "                                                    torch_dtype=\"auto\",\n",
    "                                                    ).eval()\n",
    "base_model = prm_model.model\n",
    "prm_model.score.load_state_dict(torch.load('/kaggle/input/prm-shep/model_score.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94a7824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:08:21.742846Z",
     "iopub.status.busy": "2024-05-18T17:08:21.742106Z",
     "iopub.status.idle": "2024-05-18T17:08:23.245379Z",
     "shell.execute_reply": "2024-05-18T17:08:23.244596Z"
    },
    "papermill": {
     "duration": 1.513374,
     "end_time": "2024-05-18T17:08:23.247690",
     "exception": false,
     "start_time": "2024-05-18T17:08:21.734316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import aimo\n",
    "env = aimo.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a45bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:08:23.262621Z",
     "iopub.status.busy": "2024-05-18T17:08:23.262138Z",
     "iopub.status.idle": "2024-05-18T17:08:23.268774Z",
     "shell.execute_reply": "2024-05-18T17:08:23.267916Z"
    },
    "papermill": {
     "duration": 0.016196,
     "end_time": "2024-05-18T17:08:23.270775",
     "exception": false,
     "start_time": "2024-05-18T17:08:23.254579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logit2prob = lambda x: 1/(1+np.exp(-x))\n",
    "def eval_prm(candidates):\n",
    "    all_log_probs = []\n",
    "    for i in range(len(candidates)):\n",
    "        input_ids = prm_tokenizer.encode(candidates[i], return_tensors=\"pt\").to(\"cuda:1\")\n",
    "        with torch.no_grad():\n",
    "            hidden_states = base_model(input_ids)[0][:,-1] # 1,l,d -> 1,d\n",
    "            logits = prm_model.score(hidden_states)[0]\n",
    "        all_log_probs.append(logit2prob(logits.item()))\n",
    "    return all_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a27547",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-18T17:08:23.284939Z",
     "iopub.status.busy": "2024-05-18T17:08:23.284652Z",
     "iopub.status.idle": "2024-05-18T17:09:50.210478Z",
     "shell.execute_reply": "2024-05-18T17:09:50.209364Z"
    },
    "papermill": {
     "duration": 86.935742,
     "end_time": "2024-05-18T17:09:50.212958",
     "exception": false,
     "start_time": "2024-05-18T17:08:23.277216",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 21/21 [00:07<00:00,  2.82it/s]\n",
      "Processed prompts: 100%|██████████| 21/21 [00:05<00:00,  3.86it/s]\n",
      "Processed prompts: 100%|██████████| 21/21 [00:07<00:00,  2.88it/s]\n",
      "Processed prompts: 100%|██████████| 21/21 [00:05<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\n",
    "stop_words.append(\"\\n\")\n",
    "\n",
    "sampling_params = SamplingParams(temperature=1,\n",
    "                                 max_tokens=256,\n",
    "                                 min_tokens=32,\n",
    "                                 stop=stop_words)\n",
    "\n",
    "cot_instruction = \"\\nYou are an expert at mathematical reasoning. Please reason step by step, and put your final answer within \\\\boxed{}. The answer should be an interger between 0 and 999.\"\n",
    "\n",
    "\n",
    "n = 1 # beams\n",
    "n_sol = 6\n",
    "samples = 21\n",
    "max_depth = 24\n",
    "max_pct = 0.66\n",
    "\n",
    "all_prompts = []\n",
    "total_paths = []\n",
    "total_answers = []\n",
    "\n",
    "def is_integer(num):\n",
    "    if isinstance(num, float):\n",
    "        return num.is_integer()\n",
    "    elif isinstance(num, int):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_between_0_and_999(num):\n",
    "    return 0 <= num <= 999\n",
    "\n",
    "import re\n",
    "def extract_number(text):\n",
    "    patterns = [\n",
    "        r'The answer is.*\\\\boxed\\{(.*?)\\}',\n",
    "        r\"The answer is[:\\s]*\\$([0-9]+)\\$\",\n",
    "        r\"The answer is[:\\s]*([0-9]+)\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return 'parse err'\n",
    "\n",
    "def group_and_sum(A, B):\n",
    "    '''\n",
    "    A = ['a','b','a']\n",
    "    B = [1,2,3]\n",
    "    -> {'a': 4, 'b': 2}\n",
    "    '''\n",
    "    result_dict = {}\n",
    "    for a, b in zip(A, B):\n",
    "        if a in result_dict:\n",
    "            result_dict[a] += b\n",
    "        else:\n",
    "            result_dict[a] = b\n",
    "    return result_dict\n",
    "\n",
    "def max_dict(d):\n",
    "    return max(d.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "def tot_agg(completed_paths):\n",
    "    answers,scores = zip(*completed_paths)\n",
    "    if answers:\n",
    "        groups = group_and_sum(answers, scores)\n",
    "        return max_dict(groups)\n",
    "    else:\n",
    "        return 37 # empty completed_paths\n",
    "\n",
    "\n",
    "for test, sample_submission in iter_test:\n",
    "    problem = test['problem'].values[0]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": problem + cot_instruction\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    base_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False\n",
    "    )\n",
    "    current_level = 1\n",
    "\n",
    "    current_level_nodes = [base_prompt]\n",
    "    completed_paths = []\n",
    "    completed_path_splits = []\n",
    "    try:\n",
    "        while (len(completed_paths) < n_sol) and (current_level < max_depth) and (current_level_nodes):\n",
    "            # for generation, remove special tokens for PRM\n",
    "            batch_responses = llm.generate(current_level_nodes*samples, sampling_params)\n",
    "            prm_inputs = []\n",
    "\n",
    "            # Collect candidates for reward model evaluation\n",
    "            for candidate,parent in zip(batch_responses,current_level_nodes*samples):\n",
    "                prm_input = parent + candidate.outputs[0].text\n",
    "                prm_inputs.append(prm_input)\n",
    "                \n",
    "            # Get the indices of unique elements in prm_inputs\n",
    "            unique_indices = [i for i, x in enumerate(prm_inputs) if prm_inputs.index(x) == i]\n",
    "            prm_inputs = [prm_inputs[i] for i in unique_indices]\n",
    "\n",
    "            # Batch reward model evaluation\n",
    "            prm_scores = eval_prm(prm_inputs)\n",
    "    #             prm_scores = [min(old,new) for old,new in zip(current_scores,prm_scores)]\n",
    "            next_level_nodes = []\n",
    "            nodes_split = []\n",
    "\n",
    "            # Prune to keep only the top 'n' candidates based on scores\n",
    "            combined = list(zip(prm_inputs,prm_scores))\n",
    "            combined.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their scores\n",
    "            max_score = combined[0][1]\n",
    "            for node,score in combined:\n",
    "                answer = extract_number(node)\n",
    "                if answer == 'parse err': # not finished\n",
    "                    if len(next_level_nodes) < n:\n",
    "                        next_level_nodes.append(node)\n",
    "                else: # finished\n",
    "                    if score > max_score * max_pct:\n",
    "                        try:\n",
    "                            answer = eval(answer)\n",
    "                            if is_integer(answer) and is_between_0_and_999(answer):# correct format\n",
    "                                completed_paths.append((answer,score))\n",
    "                        except: # bad eval\n",
    "                            continue\n",
    "            # if current_level_nodes is empty, all max out or err out. exit loop\n",
    "            current_level_nodes =  next_level_nodes\n",
    "            current_level += 1\n",
    "\n",
    "        #     print(f'problem {i}, sol {completed_paths}')\n",
    "        #     total_paths.append(completed_paths)\n",
    "\n",
    "        sample_submission['answer'] = tot_agg(completed_paths)\n",
    "    except:\n",
    "        sample_submission['answer'] = 37\n",
    "    env.predict(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19fb566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:09:50.236195Z",
     "iopub.status.busy": "2024-05-18T17:09:50.235876Z",
     "iopub.status.idle": "2024-05-18T17:09:50.239892Z",
     "shell.execute_reply": "2024-05-18T17:09:50.239101Z"
    },
    "papermill": {
     "duration": 0.017418,
     "end_time": "2024-05-18T17:09:50.241759",
     "exception": false,
     "start_time": "2024-05-18T17:09:50.224341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total_paths\n",
    "# len(set(current_level_nodes)),len(current_level_nodes),len(set(prm_inputs)),len(prm_inputs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8300737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5036020,
     "sourceId": 8450555,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11394,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 571.271793,
   "end_time": "2024-05-18T17:09:54.761042",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T17:00:23.489249",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2770dfd3b1aa4c489056caf28ff0eb19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d51b8465cf24584bcefbd8de22eb8b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2770dfd3b1aa4c489056caf28ff0eb19",
       "placeholder": "​",
       "style": "IPY_MODEL_71347b6fde884c478ad4c29e00602bd8",
       "value": " 3/3 [03:12&lt;00:00, 60.64s/it]"
      }
     },
     "39821c375ddf49ea9e6efb9c09fc0542": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "412a13122cbc4884a7ba1e4a7bf9c6cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4167f1d4ccc44aee846ccde523d2cc9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c7cd8d4fb35847f08de65bd2e42ceffe",
        "IPY_MODEL_7459d3b6efa74b03ac047eac61c9487f",
        "IPY_MODEL_2d51b8465cf24584bcefbd8de22eb8b1"
       ],
       "layout": "IPY_MODEL_412a13122cbc4884a7ba1e4a7bf9c6cf"
      }
     },
     "71347b6fde884c478ad4c29e00602bd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7459d3b6efa74b03ac047eac61c9487f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78f5fe4bfeb44f1e84c533bc8b8f2da8",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_39821c375ddf49ea9e6efb9c09fc0542",
       "value": 3.0
      }
     },
     "78f5fe4bfeb44f1e84c533bc8b8f2da8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7cd8d4fb35847f08de65bd2e42ceffe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e209cca0c4874c2bb25ebb79bd84ae28",
       "placeholder": "​",
       "style": "IPY_MODEL_ce7ea779817a4237af29446e91448882",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "ce7ea779817a4237af29446e91448882": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e209cca0c4874c2bb25ebb79bd84ae28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
